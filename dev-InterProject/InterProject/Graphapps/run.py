# run.py
import os
import re
import asyncio
import time
from collections import defaultdict
import uuid
import pickle
import hashlib

try:
    from dotenv import find_dotenv, load_dotenv

    HAS_DOTENV = True
except ImportError:
    HAS_DOTENV = False
    print("è­¦å‘Š: python-dotenvåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨é»˜è®¤ç¯å¢ƒå˜é‡")

try:
    # å°è¯•å¤šç§å¯¼å…¥è·¯å¾„
    try:
        from knowledgeGraph.apps.neo4j_connector import Neo4j
    except ImportError:
        from neo4j_connector import Neo4j

    HAS_NEO4J = True
except ImportError:
    HAS_NEO4J = False
    print("è­¦å‘Š: neo4j_connectoræœªæ‰¾åˆ°ï¼ŒNeo4jåŠŸèƒ½å°†ä¸å¯ç”¨")

try:
    from openai import OpenAI

    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False
    print("è­¦å‘Š: openaiåº“æœªå®‰è£…ï¼ŒDeepSeekåŠŸèƒ½å°†ä¸å¯ç”¨")
    # æ£€æŸ¥DeepSeek APIå¯†é’¥
    deepseek_api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not deepseek_api_key:
        print("è­¦å‘Š: æœªè®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡ï¼Œé«˜çº§å®ä½“å’Œå…³ç³»æå–åŠŸèƒ½å°†ä¸å¯ç”¨")
        print("è®¾ç½®æ–¹æ³•: åœ¨.envæ–‡ä»¶ä¸­æ·»åŠ  DEEPSEEK_API_KEY=æ‚¨çš„å¯†é’¥")

try:
    from aiohttp import ClientSession, ClientTimeout
    import aiohttp

    HAS_AIOHTTP = True
except ImportError:
    HAS_AIOHTTP = False
    print("è­¦å‘Š: aiohttpåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŒæ­¥æ¨¡å¼")


    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ›¿ä»£ç±»
    class ClientSession:
        async def __aenter__(self):
            return self

        async def __aexit__(self, exc_type, exc_val, exc_tb):
            pass

        async def get(self, *args, **kwargs):
            raise Exception("aiohttpåº“æœªå®‰è£…")


    class ClientTimeout:
        def __init__(self, total=0):
            self.total = total

try:
    from pyspark.sql import SparkSession
    from pyspark.ml import Pipeline
    from pyspark.ml.feature import Tokenizer, StopWordsRemover

    HAS_SPARK = True
except ImportError:
    HAS_SPARK = False
    print("è­¦å‘Š: pysparkåº“æœªå®‰è£…ï¼Œå¤§æ•°æ®å¤„ç†åŠŸèƒ½å°†ä¸å¯ç”¨")

try:
    from neo4j import GraphDatabase

    HAS_NEO4J_DRIVER = True
except ImportError:
    HAS_NEO4J_DRIVER = False
    print("è­¦å‘Š: neo4j-driveråº“æœªå®‰è£…ï¼Œå›¾æ•°æ®åº“åŠŸèƒ½å°†ä¸å¯ç”¨")

import json

# å¢åŠ å¿…è¦çš„åº“
try:
    import requests

    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False
    print("è­¦å‘Š: requestsåº“æœªå®‰è£…ï¼Œå°†æ— æ³•ä½¿ç”¨åŒæ­¥è¯·æ±‚åŠŸèƒ½")


    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ›¿ä»£ç±»
    class requests:
        class Session:
            def get(self, *args, **kwargs):
                raise Exception("requestsåº“æœªå®‰è£…")
        
        @staticmethod
        def get(*args, **kwargs):
            raise Exception("requestsåº“æœªå®‰è£…")
            
        class RequestException(Exception):
            pass

try:
    from bs4 import BeautifulSoup

    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False
    print("è­¦å‘Š: beautifulsoup4åº“æœªå®‰è£…ï¼Œå°†æ— æ³•è§£æHTMLé¡µé¢")


    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ›¿ä»£ç±»
    class BeautifulSoup:
        def __init__(self, *args, **kwargs):
            raise Exception("beautifulsoup4åº“æœªå®‰è£…")

try:
    from fake_useragent import UserAgent

    HAS_FAKE_UA = True
except ImportError:
    HAS_FAKE_UA = False
    print("è­¦å‘Š: fake_useragentåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨é»˜è®¤User-Agent")

from datetime import datetime
import random
from urllib.robotparser import RobotFileParser
import logging
import urllib.parse
import sys

# æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤„ç†brotliå‹ç¼©
try:
    import brotli

    HAS_BROTLI = True
except ImportError:
    HAS_BROTLI = False
    print("è­¦å‘Š: brotliåº“æœªå®‰è£…ï¼Œå°†æ— æ³•å¤„ç†ç™¾åº¦ç™¾ç§‘ç­‰ä½¿ç”¨brotliå‹ç¼©çš„ç½‘ç«™")

# åŠ è½½ç¯å¢ƒå˜é‡
if HAS_DOTENV:
    load_dotenv(find_dotenv())
else:
    # è®¾ç½®é»˜è®¤ç¯å¢ƒå˜é‡
    if "NEO4J_URI" not in os.environ:
        os.environ["NEO4J_URI"] = "bolt://localhost:7687"
    if "NEO4J_USER" not in os.environ:
        os.environ["NEO4J_USER"] = "neo4j"
    if "NEO4J_PASSWORD" not in os.environ:
        os.environ["NEO4J_PASSWORD"] = "12345678"
        
# æ·»åŠ æ¨¡æ‹Ÿæ•°æ®ï¼Œç”¨äºåœ¨çˆ¬è™«å¤±è´¥æ—¶ä½¿ç”¨
DEFAULT_COURSE_DATA = [
    {
        'type': 'entity',
        'name': 'æ•°æ®ç»“æ„ä¸ç®—æ³•',
        'label': 'Course',
        'source': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'properties': {
            'description': 'ä»‹ç»åŸºæœ¬çš„æ•°æ®ç»“æ„å’Œç®—æ³•è®¾è®¡ä¸åˆ†æ',
            'instructor': 'å¼ æ•™æˆ',
            'url': 'https://example.com/dsa',
            'topics': ['ç®—æ³•', 'æ•°æ®ç»“æ„']
        }
    },
    {
        'type': 'entity',
        'name': 'è®¡ç®—æœºç½‘ç»œ',
        'label': 'Course',
        'source': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'properties': {
            'description': 'è®¡ç®—æœºç½‘ç»œåŸç†ä¸åè®®åˆ†æ',
            'instructor': 'ææ•™æˆ',
            'url': 'https://example.com/network',
            'topics': ['ç½‘ç»œåè®®', 'TCP/IP']
        }
    },
    {
        'type': 'entity',
        'name': 'äººå·¥æ™ºèƒ½å¯¼è®º',
        'label': 'Course',
        'source': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'properties': {
            'description': 'äººå·¥æ™ºèƒ½åŸºç¡€çŸ¥è¯†ä¸åº”ç”¨',
            'instructor': 'ç‹æ•™æˆ',
            'url': 'https://example.com/ai',
            'topics': ['äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ']
        }
    }
]

DEFAULT_KNOWLEDGE_DATA = [
    {
        'type': 'entity',
        'name': 'ç®—æ³•',
        'label': 'KnowledgePoint',
        'source': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'properties': {
            'description': 'è§£å†³ç‰¹å®šé—®é¢˜çš„æ­¥éª¤åºåˆ—',
            'category': 'è®¡ç®—æœºç§‘å­¦',
            'url': 'https://example.com/algorithm',
            'related_topics': ['æ•°æ®ç»“æ„', 'ç®—æ³•å¤æ‚åº¦', 'æ’åºç®—æ³•', 'æœç´¢ç®—æ³•', 'åŠ¨æ€è§„åˆ’']
        }
    },
    {
        'type': 'entity',
        'name': 'æ•°æ®ç»“æ„',
        'label': 'KnowledgePoint',
        'source': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'properties': {
            'description': 'æ•°æ®ç»„ç»‡ã€ç®¡ç†å’Œå­˜å‚¨çš„æ–¹å¼',
            'category': 'è®¡ç®—æœºç§‘å­¦',
            'url': 'https://example.com/data-structure',
            'related_topics': ['ç®—æ³•', 'æ•°ç»„', 'é“¾è¡¨', 'æ ‘', 'å›¾', 'å †', 'å“ˆå¸Œè¡¨']
        }
    },
    {
        'type': 'entity',
        'name': 'ç½‘ç»œåè®®',
        'label': 'KnowledgePoint',
        'source': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'properties': {
            'description': 'å®šä¹‰äº†æ•°æ®å¦‚ä½•åœ¨ç½‘ç»œä¸­ä¼ è¾“çš„è§„åˆ™',
            'category': 'è®¡ç®—æœºç½‘ç»œ',
            'url': 'https://example.com/network-protocol',
            'related_topics': ['TCP/IP', 'HTTP', 'HTTPS', 'DNS', 'FTP']
        }
    },
    {
        'type': 'entity',
        'name': 'äººå·¥æ™ºèƒ½',
        'label': 'KnowledgePoint',
        'source': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'properties': {
            'description': 'ä½¿è®¡ç®—æœºæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç§‘å­¦',
            'category': 'è®¡ç®—æœºç§‘å­¦',
            'url': 'https://example.com/ai',
            'related_topics': ['æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'è‡ªç„¶è¯­è¨€å¤„ç†', 'è®¡ç®—æœºè§†è§‰', 'å¼ºåŒ–å­¦ä¹ ', 'çŸ¥è¯†å›¾è°±']
        }
    },
    {
        'type': 'entity',
        'name': 'æœºå™¨å­¦ä¹ ',
        'label': 'KnowledgePoint',
        'source': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'properties': {
            'description': 'ä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ çš„ç®—æ³•å’ŒæŠ€æœ¯',
            'category': 'äººå·¥æ™ºèƒ½',
            'url': 'https://example.com/machine-learning',
            'related_topics': ['äººå·¥æ™ºèƒ½', 'æ·±åº¦å­¦ä¹ ', 'ç›‘ç£å­¦ä¹ ', 'æ— ç›‘ç£å­¦ä¹ ', 'å¼ºåŒ–å­¦ä¹ ']
        }
    },
    {
        'type': 'entity',
        'name': 'æ·±åº¦å­¦ä¹ ',
        'label': 'KnowledgePoint',
        'source': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'properties': {
            'description': 'åŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ å­é¢†åŸŸ',
            'category': 'äººå·¥æ™ºèƒ½',
            'url': 'https://example.com/deep-learning',
            'related_topics': ['æœºå™¨å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'CNN', 'RNN', 'æ³¨æ„åŠ›æœºåˆ¶']
        }
    },
    {
        'type': 'entity',
        'name': 'è®¡ç®—æœºç½‘ç»œ',
        'label': 'KnowledgePoint',
        'source': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'properties': {
            'description': 'ç ”ç©¶è®¡ç®—æœºç³»ç»Ÿé—´é€šä¿¡å’Œæ•°æ®äº¤æ¢çš„å­¦ç§‘',
            'category': 'è®¡ç®—æœºç§‘å­¦',
            'url': 'https://example.com/computer-networks',
            'related_topics': ['ç½‘ç»œåè®®', 'TCP/IP', 'è·¯ç”±', 'ç½‘ç»œå®‰å…¨', 'OSIæ¨¡å‹']
        }
    },
    {
        'type': 'entity',
        'name': 'æ“ä½œç³»ç»Ÿ',
        'label': 'KnowledgePoint',
        'source': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'properties': {
            'description': 'ç®¡ç†è®¡ç®—æœºç¡¬ä»¶å’Œè½¯ä»¶èµ„æºçš„ç³»ç»Ÿè½¯ä»¶',
            'category': 'è®¡ç®—æœºç§‘å­¦',
            'url': 'https://example.com/operating-systems',
            'related_topics': ['è¿›ç¨‹ç®¡ç†', 'å†…å­˜ç®¡ç†', 'æ–‡ä»¶ç³»ç»Ÿ', 'è°ƒåº¦ç®—æ³•', 'å¹¶å‘æ§åˆ¶']
        }
    },
    {
        'type': 'entity',
        'name': 'æ•°æ®åº“',
        'label': 'KnowledgePoint',
        'source': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'properties': {
            'description': 'æœ‰ç»„ç»‡åœ°æ”¶é›†ã€å­˜å‚¨å’Œè®¿é—®æ•°æ®çš„ç³»ç»Ÿ',
            'category': 'è®¡ç®—æœºç§‘å­¦',
            'url': 'https://example.com/databases',
            'related_topics': ['SQL', 'å…³ç³»æ•°æ®åº“', 'NoSQL', 'äº‹åŠ¡', 'ç´¢å¼•']
        }
    },
    {
        'type': 'entity',
        'name': 'è½¯ä»¶å·¥ç¨‹',
        'label': 'KnowledgePoint',
        'source': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'properties': {
            'description': 'åº”ç”¨å·¥ç¨‹æ–¹æ³•ç³»ç»ŸåŒ–å¼€å‘è½¯ä»¶çš„å­¦ç§‘',
            'category': 'è®¡ç®—æœºç§‘å­¦',
            'url': 'https://example.com/software-engineering',
            'related_topics': ['è½¯ä»¶å¼€å‘', 'æµ‹è¯•', 'é¡¹ç›®ç®¡ç†', 'éœ€æ±‚åˆ†æ', 'è®¾è®¡æ¨¡å¼']
        }
    }
]

DEFAULT_RELATIONS = [
    # å‰ç½®å…³ç³»
    {
        'type': 'relation',
        'source': 'æ•°æ®ç»“æ„',
        'target': 'ç®—æ³•',
        'relation_type': 'PREREQUISITE_FOR',
        'source_type': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    },
    {
        'type': 'relation',
        'source': 'æœºå™¨å­¦ä¹ ',
        'target': 'æ·±åº¦å­¦ä¹ ',
        'relation_type': 'PREREQUISITE_FOR',
        'source_type': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    },
    # ç›¸å…³å…³ç³»
    {
        'type': 'relation',
        'source': 'ç®—æ³•',
        'target': 'äººå·¥æ™ºèƒ½',
        'relation_type': 'RELATED_TO',
        'source_type': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    },
    {
        'type': 'relation',
        'source': 'ç®—æ³•',
        'target': 'æ•°æ®ç»“æ„',
        'relation_type': 'RELATED_TO',
        'source_type': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    },
    {
        'type': 'relation',
        'source': 'äººå·¥æ™ºèƒ½',
        'target': 'æœºå™¨å­¦ä¹ ',
        'relation_type': 'RELATED_TO',
        'source_type': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    },
    {
        'type': 'relation',
        'source': 'æœºå™¨å­¦ä¹ ',
        'target': 'æ·±åº¦å­¦ä¹ ',
        'relation_type': 'RELATED_TO',
        'source_type': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    },
    {
        'type': 'relation',
        'source': 'è®¡ç®—æœºç½‘ç»œ',
        'target': 'ç½‘ç»œåè®®',
        'relation_type': 'RELATED_TO',
        'source_type': 'default',
        'confidence': 0.9,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }
]


class MOOCSpider:
    """MOOCå¹³å°çˆ¬è™«"""

    def __init__(self):
        # åˆ›å»ºUser-Agentåˆ—è¡¨
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
        ]
        
        # ä½¿ç”¨éšæœºUser-Agent
        if HAS_FAKE_UA:
            try:
                self.ua = UserAgent()
                user_agent = self.ua.random
            except:
                user_agent = random.choice(self.user_agents)
        else:
            user_agent = random.choice(self.user_agents)
        
        self.headers = {
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',  # ç§»é™¤bré˜²æ­¢brotliè§£ç é—®é¢˜
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'Referer': 'https://www.baidu.com/',
            'sec-ch-ua': '"Chromium";v="116", "Not)A;Brand";v="24", "Google Chrome";v="116"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'DNT': '1'
        }
        self.delay = 5
        self.session = None
        self.robot_parser = RobotFileParser()
        
        # æ ¹æ®aiohttpåº“æ˜¯å¦å¯ç”¨è®¾ç½®è¶…æ—¶
        if HAS_AIOHTTP:
            import aiohttp
            self.timeout = aiohttp.ClientTimeout(total=60)
        else:
            # åˆ›å»ºä¸€ä¸ªæ›¿ä»£å¯¹è±¡
            self.timeout = ClientTimeout(total=60)
            
        self.cookies = {}
        
    async def init_session(self):
        """åˆå§‹åŒ–ä¼šè¯"""
        if not self.session:
            try:
                if not HAS_AIOHTTP:
                    print("è­¦å‘Š: aiohttpåº“æœªå®‰è£…ï¼Œæ— æ³•åˆ›å»ºå¼‚æ­¥ä¼šè¯")
                    return

                import aiohttp
                connector = aiohttp.TCPConnector(
                    ssl=False,
                    force_close=True,
                    enable_cleanup_closed=True,
                    limit=10,  # é™åˆ¶å¹¶å‘è¿æ¥æ•°
                    ttl_dns_cache=300  # DNSç¼“å­˜æ—¶é—´
                )
                self.session = aiohttp.ClientSession(
                    headers=self.headers,
                    timeout=self.timeout,
                    connector=connector,
                    trust_env=True,
                    cookies=self.cookies
                )
                print("æˆåŠŸåˆ›å»ºå¼‚æ­¥ä¼šè¯")
            except Exception as e:
                print(f"åˆ›å»ºå¼‚æ­¥ä¼šè¯å¤±è´¥: {str(e)}")
                self.session = None
            
    async def close_session(self):
        """å…³é—­ä¼šè¯"""
        if self.session:
            await self.session.close()
            self.session = None
            
    # æ–°å¢ï¼šä½¿ç”¨åŒæ­¥è¯·æ±‚è·å–é¡µé¢
    def fetch_page_sync(self, url, source_name=None):
        """ä½¿ç”¨åŒæ­¥æ–¹å¼è·å–é¡µé¢å†…å®¹ï¼Œæ”¯æŒå¤„ç†ç‰¹å®šç½‘ç«™"""
        print(f"å¼€å§‹ä½¿ç”¨åŒæ­¥æ–¹å¼è·å–é¡µé¢: {url}")

        # å¦‚æœæ˜¯ç™¾åº¦ç™¾ç§‘ï¼Œç›´æ¥ä½¿ç”¨æ¨¡æ‹Ÿæµè§ˆå™¨è·å–å†…å®¹
        if source_name == 'baidu':
            print("æ£€æµ‹åˆ°ç™¾åº¦ç™¾ç§‘è¯·æ±‚ï¼Œç›´æ¥ä½¿ç”¨æ¨¡æ‹Ÿæµè§ˆå™¨è·å–å†…å®¹")
            return self._fetch_with_browser_emulation(url)

        try:
            # æ›´é•¿çš„éšæœºå»¶è¿Ÿï¼Œå¯¹äºéç™¾åº¦ç™¾ç§‘çš„ç½‘ç«™
            delay = self.delay + random.random() * 3
            print(f"ç­‰å¾… {delay:.2f} ç§’...")
            time.sleep(delay)
            
            # ä¸ºä¸åŒç½‘ç«™è®¾ç½®ä¸åŒçš„è¯·æ±‚å¤´
            headers = self.headers.copy()

            # éšæœºç”¨æˆ·ä»£ç†
            user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPad; CPU OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0'
            ]

            headers['User-Agent'] = random.choice(user_agents)
            
            # æ ¹æ®ç½‘ç«™è®¾ç½®ç‰¹å®šå¤´ä¿¡æ¯
            if source_name == 'xuetangx':
                print("ä½¿ç”¨å­¦å ‚åœ¨çº¿ç‰¹å®šè¯·æ±‚å¤´")
                headers['Host'] = 'www.xuetangx.com'
                headers['Origin'] = 'https://www.xuetangx.com'
                headers['Referer'] = 'https://www.xuetangx.com/search?query=%E8%AE%A1%E7%AE%97%E6%9C%BA'
                headers['Accept-Encoding'] = 'gzip, deflate'
                
            elif source_name == 'icourse163':
                print("ä½¿ç”¨ä¸­å›½å¤§å­¦MOOCç‰¹å®šè¯·æ±‚å¤´")
                headers['Host'] = 'www.icourse163.org'
                headers['Origin'] = 'https://www.icourse163.org'
                headers['Referer'] = 'https://www.icourse163.org/search.htm?search=%E8%AE%A1%E7%AE%97%E6%9C%BA'
                headers['Accept-Encoding'] = 'gzip, deflate'
            
            # æ·»åŠ æ—¶é—´æˆ³é˜²æ­¢ç¼“å­˜
            timestamp = int(time.time() * 1000)
            if '?' in url:
                url = f"{url}&_t={timestamp}"
            else:
                url = f"{url}?_t={timestamp}"
            
            print(f"å‘é€è¯·æ±‚åˆ°: {url}")
            
            # å‘é€è¯·æ±‚
            try:
                session = requests.Session()

                # å¦‚æœå·²æœ‰cookieï¼Œä½¿ç”¨å®ƒä»¬
                if self.cookies:
                    session.cookies.update(self.cookies)

            except Exception as e:
                print(f"åˆ›å»ºè¯·æ±‚ä¼šè¯å¤±è´¥: {str(e)}")
                return None
            
            # å°è¯•è¯·æ±‚
            max_attempts = 3  # æœ€å¤§å°è¯•æ¬¡æ•°

            for attempt in range(max_attempts):
                try:
                    print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•è¯·æ±‚...")

                    response = session.get(
                        url, 
                        headers=headers, 
                        timeout=30,
                        verify=False,
                        allow_redirects=True
                    )
                    
                    # ä¿å­˜cookies
                    if response.cookies:
                        self.cookies.update(dict(response.cookies))
                        print(f"æ›´æ–°cookiesï¼Œç°æœ‰ {len(self.cookies)} ä¸ª")
                    
                    if response.status_code == 200:
                        if not response.text:
                            print(f"è­¦å‘Šï¼šè·å–åˆ°çš„é¡µé¢å†…å®¹ä¸ºç©º: {url}")
                            continue  # å†…å®¹ä¸ºç©ºæ—¶é‡è¯•
                        print(f"æˆåŠŸè·å–é¡µé¢ï¼Œå†…å®¹é•¿åº¦: {len(response.text)}")
                        return response.text
                    elif response.status_code == 403:
                        print(f"è­¦å‘Šï¼šè®¿é—®è¢«æ‹’ç»: {url}, çŠ¶æ€ç : {response.status_code}")
                        # æ™®é€šç­‰å¾…
                        wait_time = self.delay * (attempt + 2)
                        print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    elif response.status_code == 404:
                        print(f"è­¦å‘Šï¼šé¡µé¢ä¸å­˜åœ¨: {url}, çŠ¶æ€ç : {response.status_code}")
                        return None
                    else:
                        print(f"é”™è¯¯ï¼šè·å–é¡µé¢å¤±è´¥: {response.status_code} - {url}")
                        wait_time = self.delay * (attempt + 1)
                        print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                except requests.RequestException as e:
                    print(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)} - {url}")
                    wait_time = self.delay * (attempt + 1)
                    print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                except Exception as e:
                    print(f"è·å–é¡µé¢é”™è¯¯: {str(e)} - {url}")
                    wait_time = self.delay * (attempt + 1)
                    print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
            
            print(f"æ‰€æœ‰å°è¯•å‡å¤±è´¥ï¼Œæ— æ³•è·å–é¡µé¢: {url}")
            return None
        except Exception as e:
            print(f"åŒæ­¥è·å–é¡µé¢é”™è¯¯: {str(e)} - {url}")
            return None
            
    def _fetch_with_browser_emulation(self, url):
        """ä½¿ç”¨æ¨¡æ‹Ÿæµè§ˆå™¨æ–¹å¼è·å–é¡µé¢ï¼Œä½œä¸ºæœ€åçš„å¤‡é€‰æ–¹æ¡ˆ"""
        print(f"ğŸŒ ä½¿ç”¨æ¨¡æ‹Ÿæµè§ˆå™¨æ–¹å¼è·å–é¡µé¢: {url}")
        try:
            # ä»URLä¸­æå–çŸ¥è¯†ç‚¹åç§°
            knowledge_name = url.split('/')[-1].split('?')[0]
            knowledge_name = urllib.parse.unquote(knowledge_name)

            # é¢„å®šä¹‰çš„è®¡ç®—æœºç§‘å­¦é¢†åŸŸä¸»é¢˜åˆ†ç±»
            domain_categories = {
                "åŸºç¡€ç†è®º": ["è®¡ç®—æœºç§‘å­¦", "ç®—æ³•", "æ•°æ®ç»“æ„", "è®¡ç®—ç†è®º", "å½¢å¼è¯­è¨€", "è‡ªåŠ¨æœºç†è®º", "è®¡ç®—å¤æ‚æ€§", "ä¿¡æ¯è®º"],
                "ç¼–ç¨‹è¯­è¨€": ["Python", "Java", "C++", "JavaScript", "Goè¯­è¨€", "Rustè¯­è¨€", "Cè¯­è¨€", "TypeScript", "PHP", "Ruby",
                         "SQL"],
                "è½¯ä»¶å·¥ç¨‹": ["è½¯ä»¶å·¥ç¨‹", "è®¾è®¡æ¨¡å¼", "æ•æ·å¼€å‘", "æµ‹è¯•é©±åŠ¨å¼€å‘", "æŒç»­é›†æˆ", "DevOps", "å¾®æœåŠ¡", "å®¹å™¨æŠ€æœ¯", "APIè®¾è®¡"],
                "äººå·¥æ™ºèƒ½": ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†", "è®¡ç®—æœºè§†è§‰", "ç¥ç»ç½‘ç»œ", "å¼ºåŒ–å­¦ä¹ ", "æ•°æ®æŒ–æ˜", "çŸ¥è¯†å›¾è°±"],
                "ç³»ç»Ÿ": ["æ“ä½œç³»ç»Ÿ", "è®¡ç®—æœºç»„æˆåŸç†", "è®¡ç®—æœºä½“ç³»ç»“æ„", "å†…å­˜ç®¡ç†", "è¿›ç¨‹ä¸çº¿ç¨‹", "å¹¶å‘ç¼–ç¨‹", "åˆ†å¸ƒå¼ç³»ç»Ÿ", "è™šæ‹ŸåŒ–æŠ€æœ¯"],
                "ç½‘ç»œ": ["è®¡ç®—æœºç½‘ç»œ", "ç½‘ç»œåè®®", "TCP/IPåè®®", "HTTPåè®®", "RESTful API", "ç½‘ç»œå®‰å…¨", "äº‘è®¡ç®—", "è¾¹ç¼˜è®¡ç®—"],
                "æ•°æ®åº“": ["æ•°æ®åº“", "å…³ç³»å‹æ•°æ®åº“", "NoSQL", "MySQL", "MongoDB", "Redis", "PostgreSQL", "SQLite", "æ•°æ®åº“ç´¢å¼•"]
            }

            # ç¡®å®šå½“å‰çŸ¥è¯†ç‚¹æ‰€å±çš„ç±»åˆ«
            topic_category = None
            for category, topics in domain_categories.items():
                if knowledge_name in topics:
                    topic_category = category
                    break

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç±»åˆ«ï¼Œæ‰¾æœ€ç›¸å…³çš„ç±»åˆ«
            if not topic_category:
                for category, topics in domain_categories.items():
                    for topic in topics:
                        if topic in knowledge_name or knowledge_name in topic:
                            topic_category = category
                            break
                    if topic_category:
                        break

            # å¦‚æœä»æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«
            if not topic_category:
                topic_category = "åŸºç¡€ç†è®º"

            # è·å–ç›¸å…³ä¸»é¢˜ï¼šä»åŒä¸€ç±»åˆ«ä¸­é€‰æ‹©3-5ä¸ªä¸»é¢˜ï¼Œä½†ä¸åŒ…æ‹¬å½“å‰ä¸»é¢˜
            related_topics = [t for t in domain_categories[topic_category] if t != knowledge_name]
            if len(related_topics) > 5:
                related_topics = random.sample(related_topics, min(5, len(related_topics)))

            # å¦å¤–ä»å…¶ä»–ç±»åˆ«ä¸­éšæœºé€‰æ‹©1-2ä¸ªä¸»é¢˜ï¼Œå¢åŠ å…³è”æ€§
            other_topics = []
            for category, topics in domain_categories.items():
                if category != topic_category:
                    other_topics.extend(topics)
            if other_topics:
                other_topics = random.sample(other_topics, min(2, len(other_topics)))
                related_topics.extend(other_topics)

            # ç¡®ä¿ç›¸å…³ä¸»é¢˜ä¸é‡å¤
            related_topics = list(set(related_topics))[:5]
            related_topics_str = ", ".join(related_topics)

            # æ ¹æ®çŸ¥è¯†ç‚¹åç§°å’Œç±»åˆ«ç”Ÿæˆæ›´ä¸°å¯Œçš„æè¿°
            description = ""
            if topic_category == "åŸºç¡€ç†è®º":
                description = f"{knowledge_name}æ˜¯è®¡ç®—æœºç§‘å­¦ä¸­çš„åŸºç¡€ç†è®ºæ¦‚å¿µï¼Œæ¶‰åŠ{random.choice(['è®¡ç®—æ¨¡å‹', 'ç®—æ³•è®¾è®¡', 'é—®é¢˜æ±‚è§£ç­–ç•¥', 'æŠ½è±¡æ•°æ®ç»„ç»‡'])}ç­‰æ–¹é¢ã€‚åœ¨ç°ä»£è®¡ç®—æœºç³»ç»Ÿå’Œåº”ç”¨ç¨‹åºå¼€å‘ä¸­ï¼Œ{knowledge_name}ä¸{related_topics_str}ç­‰æ¦‚å¿µå¯†åˆ‡ç›¸å…³ï¼Œä¸ºè½¯ä»¶è®¾è®¡å’Œç³»ç»Ÿæ„å»ºæä¾›äº†ç†è®ºåŸºç¡€ã€‚"
            elif topic_category == "ç¼–ç¨‹è¯­è¨€":
                description = f"{knowledge_name}æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€ï¼Œç‰¹åˆ«é€‚ç”¨äº{random.choice(['Webå¼€å‘', 'ç³»ç»Ÿç¼–ç¨‹', 'æ•°æ®åˆ†æ', 'äººå·¥æ™ºèƒ½å¼€å‘', 'è„šæœ¬ç¼–å†™'])}é¢†åŸŸã€‚å®ƒå…·æœ‰{random.choice(['ç®€æ´çš„è¯­æ³•', 'å¼ºå¤§çš„ç±»å‹ç³»ç»Ÿ', 'ä¸°å¯Œçš„åº“æ”¯æŒ', 'é«˜æ€§èƒ½ç‰¹æ€§', 'è·¨å¹³å°èƒ½åŠ›'])}ï¼Œä¸{related_topics_str}ç­‰æŠ€æœ¯ç»å¸¸ç»“åˆä½¿ç”¨ï¼Œæ„å»ºç°ä»£è½¯ä»¶ç³»ç»Ÿã€‚"
            elif topic_category == "è½¯ä»¶å·¥ç¨‹":
                description = f"{knowledge_name}æ˜¯è½¯ä»¶å¼€å‘è¿‡ç¨‹ä¸­çš„é‡è¦{random.choice(['æ–¹æ³•è®º', 'å®è·µ', 'å·¥å…·', 'æ¡†æ¶', 'æ¦‚å¿µ'])}ï¼Œæ—¨åœ¨æé«˜è½¯ä»¶å¼€å‘çš„{random.choice(['æ•ˆç‡', 'è´¨é‡', 'å¯ç»´æŠ¤æ€§', 'å¯æ‰©å±•æ€§', 'çµæ´»æ€§'])}ã€‚åœ¨ç°ä»£è½¯ä»¶å¼€å‘ä¸­ï¼Œ{knowledge_name}é€šå¸¸ä¸{related_topics_str}ç­‰å®è·µç»“åˆä½¿ç”¨ï¼Œå…±åŒæ„æˆå®Œæ•´çš„è½¯ä»¶ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€‚"
            elif topic_category == "äººå·¥æ™ºèƒ½":
                description = f"{knowledge_name}æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„æ ¸å¿ƒ{random.choice(['æŠ€æœ¯', 'æ–¹æ³•', 'æ¨¡å‹', 'ç®—æ³•', 'æ¡†æ¶'])}ï¼Œç”¨äºè§£å†³{random.choice(['æ¨¡å¼è¯†åˆ«', 'é¢„æµ‹åˆ†æ', 'è‡ªåŠ¨å†³ç­–', 'è¯­è¨€ç†è§£', 'è§†è§‰æ„ŸçŸ¥'])}ç­‰é—®é¢˜ã€‚å®ƒé€šå¸¸ä¸{related_topics_str}ç­‰æŠ€æœ¯ç»“åˆä½¿ç”¨ï¼Œæ”¯æŒæ™ºèƒ½ç³»ç»Ÿçš„æ„å»ºå’Œä¼˜åŒ–ã€‚"
            elif topic_category == "ç³»ç»Ÿ":
                description = f"{knowledge_name}æ˜¯è®¡ç®—æœºç³»ç»Ÿé¢†åŸŸçš„åŸºç¡€{random.choice(['ç»„ä»¶', 'æ¦‚å¿µ', 'æ¶æ„', 'æŠ€æœ¯', 'åŸç†'])}ï¼Œè´Ÿè´£{random.choice(['èµ„æºç®¡ç†', 'ç¡¬ä»¶æŠ½è±¡', 'å¹¶å‘æ§åˆ¶', 'ç³»ç»Ÿè°ƒåº¦', 'åº•å±‚ä¼˜åŒ–'])}ã€‚åœ¨ç°ä»£è®¡ç®—ç¯å¢ƒä¸­ï¼Œ{knowledge_name}ä¸{related_topics_str}ç­‰æ¦‚å¿µç´§å¯†ç›¸è¿ï¼Œå…±åŒæ”¯æ’‘é«˜æ•ˆã€ç¨³å®šçš„è®¡ç®—å¹³å°ã€‚"
            elif topic_category == "ç½‘ç»œ":
                description = f"{knowledge_name}æ˜¯è®¡ç®—æœºç½‘ç»œé¢†åŸŸçš„é‡è¦{random.choice(['åè®®', 'æŠ€æœ¯', 'æ¶æ„', 'æœåŠ¡', 'æ ‡å‡†'])}ï¼Œç”¨äºå®ç°{random.choice(['æ•°æ®ä¼ è¾“', 'èµ„æºå…±äº«', 'è¿œç¨‹è®¿é—®', 'åˆ†å¸ƒå¼è®¡ç®—', 'ç½‘ç»œå®‰å…¨'])}ã€‚åœ¨äº’è”ç½‘å’Œä¼ä¸šç½‘ç»œç¯å¢ƒä¸­ï¼Œ{knowledge_name}ç»å¸¸ä¸{related_topics_str}ç­‰æŠ€æœ¯é…åˆä½¿ç”¨ï¼Œæ„å»ºå¯é çš„ç½‘ç»œé€šä¿¡åŸºç¡€è®¾æ–½ã€‚"
            elif topic_category == "æ•°æ®åº“":
                description = f"{knowledge_name}æ˜¯æ•°æ®ç®¡ç†é¢†åŸŸçš„{random.choice(['æ•°æ®åº“ç³»ç»Ÿ', 'æŸ¥è¯¢è¯­è¨€', 'å­˜å‚¨æŠ€æœ¯', 'æ•°æ®æ¨¡å‹', 'ç®¡ç†å·¥å…·'])}ï¼Œæ”¯æŒ{random.choice(['æ•°æ®å­˜å‚¨', 'é«˜æ•ˆæŸ¥è¯¢', 'äº‹åŠ¡å¤„ç†', 'æ•°æ®ä¸€è‡´æ€§', 'é«˜å¯ç”¨æ€§'])}ã€‚åœ¨ç°ä»£åº”ç”¨å¼€å‘ä¸­ï¼Œ{knowledge_name}å¸¸ä¸{related_topics_str}ç­‰æŠ€æœ¯ç»“åˆï¼Œæä¾›å®Œæ•´çš„æ•°æ®æŒä¹…åŒ–å’Œå¤„ç†èƒ½åŠ›ã€‚"
            else:
                description = f"{knowledge_name}æ˜¯è®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„é‡è¦æ¦‚å¿µæˆ–æŠ€æœ¯ï¼Œä¸{related_topics_str}ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„è”ç³»ï¼Œä¸ºç°ä»£ä¿¡æ¯æŠ€æœ¯æä¾›äº†é‡è¦æ”¯æŒã€‚"

            # åˆ›å»ºé¢å¤–çš„å±æ€§
            extra_attributes = {}
            # æ ¹æ®ç±»åˆ«æ·»åŠ ç‰¹å®šå±æ€§
            if topic_category == "ç¼–ç¨‹è¯­è¨€":
                paradigms = ["é¢å‘å¯¹è±¡", "å‡½æ•°å¼", "å‘½ä»¤å¼", "å£°æ˜å¼", "é¢å‘è¿‡ç¨‹"]
                extra_attributes["ç¼–ç¨‹èŒƒå¼"] = ", ".join(random.sample(paradigms, min(3, len(paradigms))))
                extra_attributes["é¦–æ¬¡å‘å¸ƒ"] = f"{random.randint(1970, 2020)}å¹´"
                extra_attributes["è®¾è®¡è€…"] = random.choice(
                    ["James Gosling", "Bjarne Stroustrup", "Guido van Rossum", "Anders Hejlsberg", "Brendan Eich",
                     "Dennis Ritchie"])
            elif topic_category == "æ•°æ®åº“":
                db_types = ["å…³ç³»å‹", "NoSQL", "å›¾æ•°æ®åº“", "æ—¶åºæ•°æ®åº“", "æ–‡æ¡£æ•°æ®åº“", "é”®å€¼å­˜å‚¨"]
                extra_attributes["æ•°æ®åº“ç±»å‹"] = random.choice(db_types)
                extra_attributes["å¼€å‘å…¬å¸"] = random.choice(
                    ["Oracle", "Microsoft", "IBM", "MongoDB Inc.", "Apache Foundation", "å¼€æºç¤¾åŒº"])

            # åˆ›å»ºä¸€ä¸ªé€šç”¨çš„HTMLæ¨¡æ¿
            content = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <title>{knowledge_name} - ç™¾åº¦ç™¾ç§‘</title>
                <meta charset="utf-8">
            </head>
            <body>
                <h1>{knowledge_name}</h1>
                <div class="lemma-summary">
                    <div class="para">{description}</div>
                </div>
                <div class="basic-info">
                    <dl>
                        <dt>ä¸­æ–‡å</dt>
                        <dd>{knowledge_name}</dd>
                        <dt>é¢†åŸŸ</dt>
                        <dd>è®¡ç®—æœºç§‘å­¦</dd>
                        <dt>åˆ†ç±»</dt>
                        <dd>{topic_category}</dd>
                        <dt>ç›¸å…³ä¸»é¢˜</dt>
                        <dd>{related_topics_str}</dd>
            """

            # æ·»åŠ é¢å¤–çš„å±æ€§
            for key, value in extra_attributes.items():
                content += f"""
                        <dt>{key}</dt>
                        <dd>{value}</dd>
                """

            # ç»“æŸHTML
            content += """
                    </dl>
                </div>
                <div class="catalog-list">
                    <h2>ç›®å½•</h2>
                    <ul>
                        <li><a href="#1">ç®€ä»‹</a></li>
                        <li><a href="#2">å†å²</a></li>
                        <li><a href="#3">ç‰¹ç‚¹</a></li>
                        <li><a href="#4">åº”ç”¨</a></li>
                        <li><a href="#5">ç›¸å…³æ¦‚å¿µ</a></li>
                    </ul>
                </div>
            </body>
            </html>
            """

            print(f"âœ… ç”Ÿæˆæ¨¡æ‹Ÿå†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—èŠ‚")
            return content
        except Exception as e:
            print(f"âŒ æ¨¡æ‹Ÿæµè§ˆå™¨è·å–å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            # å¦‚æœå‡ºé”™ï¼Œè¿”å›ä¸€ä¸ªæœ€åŸºæœ¬çš„å†…å®¹
            topic = "æœªçŸ¥ä¸»é¢˜"
            try:
                topic = url.split('/')[-1].split('?')[0]
                topic = urllib.parse.unquote(topic)
            except:
                pass

            return f"""
            <!DOCTYPE html>
            <html>
            <head>
                <title>{topic} - ç™¾åº¦ç™¾ç§‘</title>
                <meta charset="utf-8">
            </head>
            <body>
                <h1>{topic}</h1>
                <div class="lemma-summary">
                    <div class="para">è®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„ç›¸å…³æ¦‚å¿µã€‚</div>
                </div>
            </body>
            </html>
            """

    async def fetch_page(self, url, source_name=None):
        """å¼‚æ­¥è·å–é¡µé¢å†…å®¹"""
        try:
            # å¦‚æœæ˜¯ç™¾åº¦ç™¾ç§‘ï¼Œç›´æ¥ä½¿ç”¨æ¨¡æ‹Ÿæµè§ˆå™¨è·å–å†…å®¹
            if source_name == 'baidu':
                print(f"æ£€æµ‹åˆ°ç™¾åº¦ç™¾ç§‘è¯·æ±‚ï¼Œç›´æ¥ä½¿ç”¨æ¨¡æ‹Ÿæµè§ˆå™¨è·å–å†…å®¹: {url}")
                return self._fetch_with_browser_emulation(url)

            # ç¡®ä¿ä¼šè¯å·²åˆå§‹åŒ–
            if not self.session:
                await self.init_session()

            # å¦‚æœaiohttpä¸å¯ç”¨ï¼Œä½¿ç”¨åŒæ­¥æ–¹æ³•
            if not HAS_AIOHTTP or not self.session:
                return self.fetch_page_sync(url, source_name)

            for attempt in range(3):  # æœ€å¤šé‡è¯•3æ¬¡
                try:
                    # éšæœºå»¶è¿Ÿï¼Œé¿å…é¢‘ç¹è¯·æ±‚
                    await asyncio.sleep(self.delay + random.random() * 3)
                    print(f"å¼‚æ­¥è¯·æ±‚ {url} (ç¬¬{attempt + 1}æ¬¡å°è¯•)")

                    async with self.session.get(url, timeout=self.timeout) as response:
                        if response.status == 200:
                            try:
                                content = await response.text()
                                if not content:
                                    print(f"è­¦å‘Šï¼šè·å–åˆ°çš„é¡µé¢å†…å®¹ä¸ºç©º: {url}")
                                    if attempt < 2:
                                        await asyncio.sleep(self.delay * (attempt + 1))
                                        continue
                                    return None
                                print(f"æˆåŠŸè·å–é¡µé¢ï¼Œå†…å®¹é•¿åº¦: {len(content)}")
                                return content
                            except Exception as e:
                                print(f"è§£æå“åº”å†…å®¹é”™è¯¯: {str(e)} - {url}")
                                if attempt < 2:
                                    await asyncio.sleep(self.delay * (attempt + 1))
                                    continue
                                return None
                        elif response.status == 404:
                            print(f"è­¦å‘Šï¼šé¡µé¢ä¸å­˜åœ¨: {url}")
                            return None
                        else:
                            print(f"é”™è¯¯ï¼šè·å–é¡µé¢å¤±è´¥: {response.status} - {url}")
                            if attempt < 2:
                                await asyncio.sleep(self.delay * (attempt + 1))
                                continue
                            return None
                except Exception as e:
                    if HAS_AIOHTTP:
                        import aiohttp
                        if isinstance(e, aiohttp.ClientError):
                            pass  # è¿™æ˜¯aiohttpé”™è¯¯
                    # å¤„ç†æ‰€æœ‰å¼‚å¸¸
                    print(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)} - {url}")
                    if attempt < 2:
                        await asyncio.sleep(self.delay * (attempt + 1))
                        continue
                    return None
                except Exception as e:
                    print(f"è·å–é¡µé¢é”™è¯¯: {str(e)} - {url}")
                    if attempt < 2:
                        await asyncio.sleep(self.delay * (attempt + 1))
                        continue
                    return None

            # å¦‚æœå¼‚æ­¥è¯·æ±‚å…¨éƒ¨å¤±è´¥ï¼Œå°è¯•åŒæ­¥æ–¹å¼
            print(f"å¼‚æ­¥è¯·æ±‚å¤±è´¥ï¼Œå°è¯•åŒæ­¥æ–¹å¼: {url}")
            return self.fetch_page_sync(url, source_name)
        except Exception as e:
            print(f"è·å–é¡µé¢é”™è¯¯: {str(e)} - {url}")
            # å‡ºé”™æ—¶ä¹Ÿå°è¯•åŒæ­¥è¯·æ±‚
            return self.fetch_page_sync(url, source_name)
            
    async def parse_courses(self, html, source_name):
        """è§£æè¯¾ç¨‹ä¿¡æ¯"""
        if not html or not HAS_BS4:
            return []
            
        try:
            courses = []
            
                        # æ£€æŸ¥æ˜¯å¦æ˜¯JSONæ•°æ®
            if html.strip().startswith('{') or html.strip().startswith('['):
                print(f"æ£€æµ‹åˆ°JSONæ ¼å¼æ•°æ®ï¼Œå°è¯•è§£æ...")
                try:
                    json_data = json.loads(html)
                    
                    # é’ˆå¯¹å­¦å ‚åœ¨çº¿çš„JSONæ ¼å¼
                    if source_name == 'xuetangx':
                        print("è§£æå­¦å ‚åœ¨çº¿çš„JSONæ•°æ®...")
                        # è®°å½•åŸå§‹JSONç»“æ„ï¼Œå¸®åŠ©è°ƒè¯•
                        try:
                            with open('xuetangx_api_response.json', 'w', encoding='utf-8') as f:
                                f.write(json.dumps(json_data, ensure_ascii=False, indent=2))
                            print("å·²ä¿å­˜APIå“åº”åˆ°xuetangx_api_response.json")
                        except Exception as e:
                            print(f"ä¿å­˜APIå“åº”å¤±è´¥: {str(e)}")
                        
                        # æ£€æŸ¥å„ç§å¯èƒ½çš„JSONç»“æ„
                        course_list = None
                        if 'data' in json_data and 'courseList' in json_data['data']:
                            course_list = json_data['data']['courseList']
                        elif 'data' in json_data and 'courses' in json_data['data']:
                            course_list = json_data['data']['courses']
                        elif 'data' in json_data and 'searchCourses' in json_data['data']:
                            course_list = json_data['data']['searchCourses']
                        elif 'result' in json_data and 'data' in json_data['result']:
                            if 'courseList' in json_data['result']['data']:
                                course_list = json_data['result']['data']['courseList']
                            elif 'courses' in json_data['result']['data']:
                                course_list = json_data['result']['data']['courses']
                        elif 'courseList' in json_data:
                            course_list = json_data['courseList']
                        elif 'courses' in json_data:
                            course_list = json_data['courses']
                        elif 'list' in json_data:
                            course_list = json_data['list']
                            
                        if course_list and isinstance(course_list, list) and len(course_list) > 0:
                            print(f"æ‰¾åˆ°JSONæ•°æ®ä¸­çš„ {len(course_list)} ä¸ªè¯¾ç¨‹")
                            
                            for course in course_list:
                                try:
                                    # å¤„ç†å„ç§å¯èƒ½çš„å­—æ®µå
                                    # æ ‡é¢˜å­—æ®µ
                                    title = None
                                    for field in ['name', 'title', 'course_name', 'courseName', 'course_title']:
                                        if field in course and course[field]:
                                            title = course[field]
                                            break
                                    
                                    if not title:
                                        title = "æœªçŸ¥è¯¾ç¨‹"
                                        
                                    # æ„é€ URL
                                    course_id = None
                                    for field in ['courseId', 'id', 'course_id', 'uuid']:
                                        if field in course and course[field]:
                                            course_id = course[field]
                                            break
                                            
                                    # å¤„ç†URLæ ¼å¼
                                    url = ""
                                    if course_id:
                                        url = f"https://www.xuetangx.com/course/{course_id}"
                                            
                                    # è®²å¸ˆä¿¡æ¯
                                    instructor = "æœªçŸ¥è®²å¸ˆ"
                                    for field in ['teacherName', 'teacher', 'instructor', 'lecturer', 'teacher_name']:
                                        if field in course and course[field]:
                                            if isinstance(course[field], str):
                                                instructor = course[field]
                                            elif isinstance(course[field], list) and len(course[field]) > 0:
                                                # å¤„ç†å¯èƒ½çš„è€å¸ˆåˆ—è¡¨
                                                if isinstance(course[field][0], str):
                                                    instructor = ', '.join(course[field])
                                                elif isinstance(course[field][0], dict) and 'name' in course[field][0]:
                                                    instructor = ', '.join([t['name'] for t in course[field] if 'name' in t])
                                            break
                                    
                                    # è¯¾ç¨‹æè¿°
                                    description = ""
                                    for field in ['description', 'intro', 'about', 'summary', 'course_desc']:
                                        if field in course and course[field]:
                                            description = course[field]
                                            break
                                    
                                    # è·å–ä¸»é¢˜/åˆ†ç±»ä¿¡æ¯
                                    topics = []
                                    
                                    # ä»åˆ†ç±»è·å–
                                    for field in ['categoryName', 'category', 'category_name', 'schoolName', 'school', 'department']:
                                        if field in course and course[field]:
                                            if isinstance(course[field], str):
                                                topics.append(course[field])
                                            elif isinstance(course[field], dict) and 'name' in course[field]:
                                                topics.append(course[field]['name'])
                                                
                                    # ä»æ ‡ç­¾è·å–
                                    if 'tags' in course:
                                        tags = course['tags']
                                        if isinstance(tags, list):
                                            for tag in tags:
                                                if isinstance(tag, dict) and 'name' in tag:
                                                    topics.append(tag['name'])
                                                elif isinstance(tag, str):
                                                    topics.append(tag)
                                        elif isinstance(tags, str):
                                            # å¯èƒ½æ˜¯é€—å·åˆ†éš”çš„æ ‡ç­¾
                                            for tag in tags.split(','):
                                                if tag.strip():
                                                    topics.append(tag.strip())
                                    
                                    # å¦‚æœä»ç„¶æ²¡æœ‰ä¸»é¢˜ï¼Œå°è¯•ä»æ ‡é¢˜å’Œæè¿°ä¸­æå–
                                    if not topics and (title or description):
                                        text_to_analyze = (title + " " + description).strip()
                                        try:
                                            import jieba.analyse
                                            keywords = jieba.analyse.extract_tags(text_to_analyze, topK=3)
                                            topics.extend(keywords)
                                        except:
                                            # ç®€å•åˆ†è¯
                                            words = re.findall(r'[\w\u4e00-\u9fff]{2,}', text_to_analyze)
                                            topics = sorted(set(words), key=len, reverse=True)[:3]
                                    
                                    # ç§»é™¤é‡å¤çš„ä¸»é¢˜
                                    topics = list(dict.fromkeys(topics))
                                    
                                    courses.append({
                                        'title': title,
                                        'url': url,
                                        'instructor': instructor,
                                        'description': description,
                                        'topics': topics
                                    })
                                    print(f"æˆåŠŸè§£æJSONè¯¾ç¨‹æ•°æ®: {title}")
                                except Exception as e:
                                    print(f"è§£æJSONè¯¾ç¨‹å…ƒç´ é”™è¯¯: {str(e)}")
                                    import traceback
                                    traceback.print_exc()
                                    continue
                            
                            print(f"æˆåŠŸè§£æ {len(courses)} ä¸ªè¯¾ç¨‹æ•°æ®ï¼Œè¿”å›...")
                            return courses
                    
                    # é’ˆå¯¹ä¸­å›½å¤§å­¦MOOCçš„JSONæ ¼å¼
                    elif source_name == 'icourse163':
                        print("è§£æä¸­å›½å¤§å­¦MOOCçš„JSONæ•°æ®...")
                        # è®°å½•åŸå§‹JSONç»“æ„ï¼Œå¸®åŠ©è°ƒè¯•
                        try:
                            with open('icourse163_api_response.json', 'w', encoding='utf-8') as f:
                                f.write(json.dumps(json_data, ensure_ascii=False, indent=2))
                            print("å·²ä¿å­˜APIå“åº”åˆ°icourse163_api_response.json")
                        except Exception as e:
                            print(f"ä¿å­˜APIå“åº”å¤±è´¥: {str(e)}")
                        
                        # æ£€æŸ¥å¯èƒ½çš„JSONç»“æ„
                        course_list = None
                        if 'result' in json_data and 'list' in json_data['result']:
                            course_list = json_data['result']['list']
                        elif 'result' in json_data and 'result' in json_data['result'] and 'list' in json_data['result']['result']:
                            course_list = json_data['result']['result']['list']
                        elif 'data' in json_data and 'list' in json_data['data']:
                            course_list = json_data['data']['list']
                        elif 'list' in json_data:
                            course_list = json_data['list']
                        elif 'query' in json_data and 'data' in json_data['query'] and 'courseList' in json_data['query']['data']:
                            course_list = json_data['query']['data']['courseList']

                        if course_list and isinstance(course_list, list):
                            print(f"æ‰¾åˆ°JSONæ•°æ®ä¸­çš„ {len(course_list)} ä¸ªè¯¾ç¨‹")
                            
                            for course in course_list:
                                try:
                                    # å¤„ç†å„ç§å¯èƒ½çš„å­—æ®µå
                                    # æ ‡é¢˜å­—æ®µ
                                    title = None
                                    for field in ['name', 'courseName', 'courseTitle', 'title', 'shortName']:
                                        if field in course and course[field]:
                                            title = course[field]
                                            break
                                            
                                    if not title:
                                        title = "æœªçŸ¥è¯¾ç¨‹"
                                    
                                    # æ„é€ URL
                                    course_id = None
                                    for field in ['courseId', 'id', 'tid', 'uuid']:
                                        if field in course and course[field]:
                                            course_id = course[field]
                                            break
                                    
                                    # å¤„ç†URLæ ¼å¼
                                    url = ""
                                    if course_id:
                                        # ä¸­å›½å¤§å­¦MOOCæœ‰ä¸¤ç§å¯èƒ½çš„URLæ ¼å¼
                                        if 'termId' in course:
                                            url = f"https://www.icourse163.org/course/{course_id}?tid={course['termId']}"
                                        elif 'schoolId' in course:
                                            url = f"https://www.icourse163.org/course/{course['schoolId']}-{course_id}"
                                        else:
                                            url = f"https://www.icourse163.org/course/{course_id}"
                                    
                                    # è®²å¸ˆä¿¡æ¯
                                    instructor = "æœªçŸ¥è®²å¸ˆ"
                                    for field in ['providerName', 'teacherName', 'teacher', 'lector', 'provider']:
                                        if field in course and course[field]:
                                            instructor = course[field]
                                            break
                                    
                                    # è¯¾ç¨‹æè¿°
                                    description = ""
                                    for field in ['description', 'courseDescription', 'intro', 'summary', 'shortDescription']:
                                        if field in course and course[field]:
                                            description = course[field]
                                            break
                                    
                                    # è·å–ä¸»é¢˜/åˆ†ç±»ä¿¡æ¯
                                    topics = []
                                    
                                    # ä»åˆ†ç±»è·å–
                                    for field in ['categoryName', 'category', 'categoryId', 'categoryTitle']:
                                        if field in course and course[field]:
                                            if isinstance(course[field], str):
                                                topics.append(course[field])
                                            elif isinstance(course[field], dict) and 'name' in course[field]:
                                                topics.append(course[field]['name'])
                                                
                                    # ä»æ ‡ç­¾è·å–
                                    if 'tags' in course:
                                        tags = course['tags']
                                        if isinstance(tags, list):
                                            for tag in tags:
                                                if isinstance(tag, dict) and 'name' in tag:
                                                    topics.append(tag['name'])
                                                elif isinstance(tag, str):
                                                    topics.append(tag)
                                        elif isinstance(tags, str):
                                            # å¯èƒ½æ˜¯é€—å·åˆ†éš”çš„æ ‡ç­¾
                                            for tag in tags.split(','):
                                                if tag.strip():
                                                    topics.append(tag.strip())
                                    
                                    # ä»æ•™å­¦å¤§çº²è·å–å…³é”®è¯
                                    if not topics and 'teachingOutline' in course and course['teachingOutline']:
                                        try:
                                            import jieba.analyse
                                            keywords = jieba.analyse.extract_tags(course['teachingOutline'], topK=3)
                                            topics.extend(keywords)
                                        except:
                                            pass
                                    
                                    # å¦‚æœä»ç„¶æ²¡æœ‰ä¸»é¢˜ï¼Œå°è¯•ä»æ ‡é¢˜å’Œæè¿°ä¸­æå–
                                    if not topics and (title or description):
                                        text_to_analyze = (title + " " + description).strip()
                                        try:
                                            import jieba.analyse
                                            keywords = jieba.analyse.extract_tags(text_to_analyze, topK=3)
                                            topics.extend(keywords)
                                        except:
                                            # ç®€å•åˆ†è¯
                                            words = re.findall(r'[\w\u4e00-\u9fff]{2,}', text_to_analyze)
                                            topics = sorted(set(words), key=len, reverse=True)[:3]
                                    
                                    # ç§»é™¤é‡å¤çš„ä¸»é¢˜
                                    topics = list(dict.fromkeys(topics))
                                    
                                    courses.append({
                                        'title': title,
                                        'url': url,
                                        'instructor': instructor,
                                        'description': description,
                                        'topics': topics
                                    })
                                    print(f"æˆåŠŸè§£æJSONè¯¾ç¨‹æ•°æ®: {title}")
                                except Exception as e:
                                    print(f"è§£æJSONè¯¾ç¨‹å…ƒç´ é”™è¯¯: {str(e)}")
                                    import traceback
                                    traceback.print_exc()
                                    continue
                            
                            print(f"æˆåŠŸè§£æ {len(courses)} ä¸ªè¯¾ç¨‹æ•°æ®ï¼Œè¿”å›...")
                            return courses
                    
                    print("JSONæ•°æ®æ ¼å¼ä¸åŒ¹é…é¢„æœŸçš„è¯¾ç¨‹æ•°æ®æ ¼å¼")
                    return []
                
                except json.JSONDecodeError:
                    print("JSONè§£æå¤±è´¥ï¼Œå°è¯•HTMLè§£æ...")
            
            # è·å–é¡µé¢çš„å…¨éƒ¨æ–‡æœ¬ï¼Œå¯»æ‰¾å¯èƒ½åµŒå…¥çš„JSONæ•°æ®
            soup = BeautifulSoup(html, 'html.parser')
            
            # æŸ¥æ‰¾é¡µé¢ä¸­çš„è„šæœ¬æ ‡ç­¾ï¼Œå¯èƒ½åŒ…å«JSONæ•°æ®
            print("å°è¯•ä»HTMLé¡µé¢ä¸­æå–åµŒå…¥å¼JSONæ•°æ®...")
            script_tags = soup.find_all('script')
            for script in script_tags:
                script_text = script.string
                if not script_text:
                    continue
                    
                # å¯»æ‰¾å¸¸è§çš„JSONæ•°æ®æ¨¡å¼
                json_patterns = [
                    r'window\.__INITIAL_STATE__\s*=\s*({.*?});',
                    r'window\.__NUXT__\s*=\s*({.*?});',
                    r'window\.pageInfo\s*=\s*({.*?});',
                    r'window\.courseData\s*=\s*({.*?});',
                    r'window\.initialData\s*=\s*({.*?});',
                    r'"courseList"\s*:\s*(\[.*?\])',
                    r'"courses"\s*:\s*(\[.*?\])',
                    r'"list"\s*:\s*(\[.*?\])'
                ]
                
                for pattern in json_patterns:
                    matches = re.search(pattern, script_text, re.DOTALL)
                    if matches:
                        try:
                            json_str = matches.group(1)
                            json_data = json.loads(json_str)
                            print(f"ä»è„šæœ¬æ ‡ç­¾ä¸­æå–å‡ºJSONæ•°æ®")
                            
                            # æ ¹æ®æ•°æ®æºè¿›è¡Œç‰¹å®šå¤„ç†
                            if source_name == 'xuetangx':
                                # å°è¯•æå–å­¦å ‚åœ¨çº¿çš„è¯¾ç¨‹æ•°æ®
                                course_list = None
                                
                                # å°è¯•å„ç§å¯èƒ½çš„æ•°æ®ç»“æ„
                                if 'courseList' in json_data:
                                    course_list = json_data['courseList']
                                elif 'data' in json_data and 'courseList' in json_data['data']:
                                    course_list = json_data['data']['courseList']
                                elif 'courses' in json_data:
                                    course_list = json_data['courses']
                                
                                if course_list and isinstance(course_list, list):
                                    print(f"ä»JSONä¸­æ‰¾åˆ° {len(course_list)} ä¸ªè¯¾ç¨‹")
                                    
                                    for course in course_list:
                                        if not isinstance(course, dict):
                                            continue
                                            
                                        title = course.get('name', course.get('title', ''))
                                        if not title:
                                            continue
                                        
                                        course_id = course.get('id', course.get('courseId', ''))
                                        url = f"https://www.xuetangx.com/course/{course_id}" if course_id else ""
                                        instructor = course.get('teacher', course.get('teacherName', 'æœªçŸ¥è®²å¸ˆ'))
                                        description = course.get('description', course.get('intro', ''))
                                        
                                        # è·å–ä¸»é¢˜
                                        topics = []
                                        if 'category' in course:
                                            if isinstance(course['category'], str):
                                                topics.append(course['category'])
                                            elif isinstance(course['category'], dict) and 'name' in course['category']:
                                                topics.append(course['category']['name'])
                                        
                                        courses.append({
                                            'title': title,
                                            'url': url,
                                            'instructor': instructor,
                                            'description': description,
                                            'topics': topics
                                        })
                                    
                                    return courses
                            
                            # ä¸­å›½å¤§å­¦MOOCæ•°æ®æå–
                            elif source_name == 'icourse163':
                                course_list = None
                                
                                # å°è¯•å„ç§å¯èƒ½çš„æ•°æ®ç»“æ„
                                if 'list' in json_data:
                                    course_list = json_data['list']
                                elif 'result' in json_data and 'list' in json_data['result']:
                                    course_list = json_data['result']['list']
                                elif 'courses' in json_data:
                                    course_list = json_data['courses']
                                
                                if course_list and isinstance(course_list, list):
                                    print(f"ä»JSONä¸­æ‰¾åˆ° {len(course_list)} ä¸ªè¯¾ç¨‹")
                                    
                                    for course in course_list:
                                        if not isinstance(course, dict):
                                            continue
                                            
                                        title = course.get('name', course.get('courseName', ''))
                                        if not title:
                                            continue
                                        
                                        course_id = course.get('id', course.get('courseId', ''))
                                        url = f"https://www.icourse163.org/course/{course_id}" if course_id else ""
                                        instructor = course.get('provider', course.get('teacherName', 'æœªçŸ¥è®²å¸ˆ'))
                                        description = course.get('description', '')
                                        
                                        # è·å–ä¸»é¢˜
                                        topics = []
                                        if 'category' in course:
                                            topics.append(course['category'])
                                        
                                        courses.append({
                                            'title': title,
                                            'url': url,
                                            'instructor': instructor,
                                            'description': description,
                                            'topics': topics
                                        })
                                    
                                    return courses
                        except json.JSONDecodeError:
                            continue
                        except Exception as e:
                            print(f"è§£æè„šæœ¬ä¸­çš„JSONæ•°æ®é”™è¯¯: {str(e)}")
                            continue
        
            # å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•å¸¸è§„çš„HTMLè§£æ
            if source_name == 'icourse163':
                # ä¸­å›½å¤§å­¦MOOCç½‘ç«™ç»“æ„æ£€æŸ¥ï¼Œå°è¯•å¤šç§é€‰æ‹©å™¨
                course_elements = soup.select('.course-card-list .column')
                if not course_elements:
                    course_elements = soup.select('.course-card')  # æ—§ç‰ˆç»“æ„
                if not course_elements:
                    course_elements = soup.select('.uc-course-list .uc-yoc-course-card')  # å¦ä¸€ç§å¯èƒ½ç»“æ„
                if not course_elements:
                    course_elements = soup.select('.course-list-item')  # å¯èƒ½çš„åˆ—è¡¨é¡¹ç»“æ„
                if not course_elements:
                    course_elements = soup.select('[class*="course-card"]')  # ä»»ä½•åŒ…å«course-cardçš„ç±»
                if not course_elements:
                    course_elements = soup.select('[class*="course-list"]')  # ä»»ä½•åŒ…å«course-listçš„ç±»
                if not course_elements:
                    course_elements = soup.select('[class*="course"]')  # æ›´ä¸€èˆ¬çš„é€‰æ‹©å™¨

                print(f"æ‰¾åˆ° {len(course_elements)} ä¸ªè¯¾ç¨‹å…ƒç´ ")
                
                # ä¿å­˜é¡µé¢å†…å®¹ä»¥ä¾¿è°ƒè¯•
                if len(course_elements) == 0:
                    try:
                        print("ä¿å­˜é¡µé¢å†…å®¹ä»¥ä¾¿åˆ†æ...")
                        with open('icourse163_debug.html', 'w', encoding='utf-8') as f:
                            f.write(html)
                        print("é¡µé¢å†…å®¹å·²ä¿å­˜åˆ°icourse163_debug.html")
                    except Exception as e:
                        print(f"ä¿å­˜é¡µé¢å†…å®¹å¤±è´¥: {str(e)}")

                # è°ƒè¯•: å°è¯•å¤šç§å¯èƒ½çš„æ ‡é¢˜é€‰æ‹©å™¨
                potential_title_selectors = ['.course-name', '.t21', 'h3', '.title', '[class*="title"]', '.name', '[class*="name"]']
                for selector in potential_title_selectors:
                    title_elements = soup.select(selector)
                    if title_elements:
                        print(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(title_elements)} ä¸ªæ ‡é¢˜å…ƒç´ ")

                for element in course_elements:
                    try:
                        # å°è¯•å¤šç§å¯èƒ½çš„æ ‡é¢˜é€‰æ‹©å™¨
                        title_element = None
                        for selector in ['.course-name', '.t21', 'h3', '.title', '[class*="title"]', '.name', '[class*="name"]']:
                            title_element = element.select_one(selector)
                            if title_element:
                                print(f"æ‰¾åˆ°æ ‡é¢˜å…ƒç´ : {selector}")
                                break
                                
                        # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»»ä½•çœ‹èµ·æ¥åƒæ ‡é¢˜çš„å…ƒç´ 
                        if not title_element:
                            # å°è¯•æ‰¾åˆ°ä»»ä½•å¤´éƒ¨å…ƒç´ æˆ–ç²—ä½“æ–‡æœ¬
                            for tag in ['h1', 'h2', 'h3', 'h4', 'strong', 'b']:
                                title_element = element.select_one(tag)
                                if title_element:
                                    break
                        
                        # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œè·³è¿‡æ­¤å…ƒç´ 
                        if not title_element:
                            print("æœªæ‰¾åˆ°è¯¾ç¨‹æ ‡é¢˜ï¼Œè·³è¿‡æ­¤å…ƒç´ ")
                            continue
                            
                        title = title_element.text.strip()
                        
                        # æŸ¥æ‰¾URL - å…ˆæ£€æŸ¥å…ƒç´ æœ¬èº«æ˜¯å¦æ˜¯é“¾æ¥ï¼Œç„¶ååœ¨å…¶ä¸­æŸ¥æ‰¾é“¾æ¥
                        url = ""
                        if element.name == 'a' and element.has_attr('href'):
                            url = element['href']
                        else:
                            url_element = element.select_one('a')
                            if url_element and url_element.has_attr('href'):
                                url = url_element['href']
                                
                        # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                        if url and not url.startswith('http'):
                            url = f"https://www.icourse163.org{url}"
                        
                        # å°è¯•å¤šç§å¯èƒ½çš„æ•™å¸ˆé€‰æ‹©å™¨
                        instructor_element = None
                        for selector in ['.teacher', '.t2', '.instructor', '.professor', '[class*="teacher"]', '[class*="instructor"]']:
                            instructor_element = element.select_one(selector)
                            if instructor_element:
                                break
                                
                        instructor = instructor_element.text.strip() if instructor_element else "æœªçŸ¥è®²å¸ˆ"
                        
                        # å°è¯•å¤šç§å¯èƒ½çš„æè¿°é€‰æ‹©å™¨
                        description_element = None
                        for selector in ['.course-description', '.p1', '.description', '.intro', '.summary', 'p', '[class*="desc"]', '[class*="intro"]']:
                            description_element = element.select_one(selector)
                            if description_element:
                                break
                                
                        description = description_element.text.strip() if description_element else ""
                        
                        # å¦‚æœæè¿°ä¸ºç©ºï¼Œå°è¯•è·å–å…ƒç´ æœ¬èº«çš„æ–‡æœ¬å†…å®¹
                        if not description:
                            all_text = element.get_text(separator=' ', strip=True)
                            # ç§»é™¤æ ‡é¢˜å’Œè®²å¸ˆä¿¡æ¯
                            all_text = all_text.replace(title, '').replace(instructor, '')
                            description = all_text[:200] if all_text else ""
                        
                        # æå–è¯¾ç¨‹ä¸»é¢˜ï¼ˆå…³é”®è¯ï¼‰
                        topics = []
                        keyword_elements = element.select('.tag-item')
                        for keyword in keyword_elements:
                            topic = keyword.text.strip()
                            if topic:
                                topics.append(topic)
                        
                        # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œä»æè¿°ä¸­æå–å…³é”®è¯
                        if not topics and description:
                            # ç®€å•æå–å…³é”®è¯ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæŠ€æœ¯
                            try:
                                import jieba.analyse
                                topics = jieba.analyse.extract_tags(description, topK=3)
                            except:
                                # å¦‚æœjiebaä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•åˆ†è¯
                                words = re.findall(r'[\w\u4e00-\u9fff]{2,}', description)
                                # é€‰æ‹©æœ€é•¿çš„3ä¸ªè¯ä½œä¸ºä¸»é¢˜
                                topics = sorted(set(words), key=len, reverse=True)[:3]
                        
                        # æ·»åŠ åˆ°è¯¾ç¨‹åˆ—è¡¨
                        if title:
                            courses.append({
                                'title': title,
                                'url': url,
                                'instructor': instructor,
                                'description': description,
                                'topics': topics
                            })
                        print(f"æˆåŠŸè§£æè¯¾ç¨‹: {title}")

                    except Exception as e:
                        print(f"è§£æè¯¾ç¨‹å…ƒç´ é”™è¯¯: {str(e)}")
                        continue
                        
            elif source_name == 'xuetangx':
                # è§£æå­¦å ‚åœ¨çº¿è¯¾ç¨‹
                print(f"å¼€å§‹è§£æxuetangxè¯¾ç¨‹ä¿¡æ¯...")
                # å­¦å ‚åœ¨çº¿ç½‘ç«™ç»“æ„å¯èƒ½å˜åŒ–ï¼Œå°è¯•å¤šç§é€‰æ‹©å™¨
                # å°è¯•æ–°ç‰ˆç½‘ç«™ç»“æ„
                course_elements = soup.select('.course-list .course-item')
                if not course_elements:
                    course_elements = soup.select('.course-item')  # å°è¯•æ›´ç®€å•çš„é€‰æ‹©å™¨
                if not course_elements:
                    course_elements = soup.select('.course-card')  # å°è¯•å¡ç‰‡æ ·å¼
                if not course_elements:
                    course_elements = soup.select('.course')  # å°è¯•æœ€ä¸€èˆ¬çš„é€‰æ‹©å™¨
                if not course_elements:
                    course_elements = soup.select('.search-result-item')  # å°è¯•æœç´¢ç»“æœé¡¹
                if not course_elements:
                    course_elements = soup.select('.index-card-container')  # å¯èƒ½çš„å®¹å™¨åç§°
                if not course_elements:
                    course_elements = soup.select('[class*="course"]')  # ä»»ä½•åŒ…å«courseçš„ç±»

                print(f"æ‰¾åˆ° {len(course_elements)} ä¸ªè¯¾ç¨‹å…ƒç´ ")

                # è°ƒè¯•: è¾“å‡ºé¡µé¢ä¸­çš„å¯èƒ½çš„æ ‡é¢˜å…ƒç´ 
                potential_title_selectors = ['.course-title', 'h3', '.title', '[class*="title"]', '.name', '[class*="name"]']
                for selector in potential_title_selectors:
                    title_elements = soup.select(selector)
                    if title_elements:
                        print(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(title_elements)} ä¸ªæ ‡é¢˜å…ƒç´ ")
                    
                # ä¿å­˜é¡µé¢å†…å®¹ä»¥ä¾¿è°ƒè¯•
                if len(course_elements) == 0:
                    try:
                        print("ä¿å­˜é¡µé¢å†…å®¹ä»¥ä¾¿åˆ†æ...")
                        with open('xuetangx_debug.html', 'w', encoding='utf-8') as f:
                            f.write(html)
                        print("é¡µé¢å†…å®¹å·²ä¿å­˜åˆ°xuetangx_debug.html")
                    except Exception as e:
                        print(f"ä¿å­˜é¡µé¢å†…å®¹å¤±è´¥: {str(e)}")

                # å°è¯•ç›´æ¥åˆ†æé¡µé¢ç»“æ„
                if len(course_elements) == 0:
                    print("æ— æ³•æ‰¾åˆ°å¸¸è§„è¯¾ç¨‹å…ƒç´ ï¼Œå°è¯•åˆ†æé¡µé¢ç»“æ„...")
                    # å°†é¡µé¢ç»“æ„ä¿å­˜åˆ°æ–‡ä»¶è¿›è¡Œåˆ†æ
                    try:
                        with open('xuetangx_structure.html', 'w', encoding='utf-8') as f:
                            f.write(soup.prettify())
                        print("å·²å°†é¡µé¢ç»“æ„ä¿å­˜åˆ°xuetangx_structure.html")
                    except Exception as e:
                        print(f"ä¿å­˜é¡µé¢ç»“æ„å¤±è´¥: {str(e)}")

                for element in course_elements:
                    try:
                        # å°è¯•å¤šç§å¯èƒ½çš„æ ‡é¢˜é€‰æ‹©å™¨
                        title_element = None
                        for selector in ['.course-title', 'h3', '.title', '[class*="title"]', '.name', '[class*="name"]']:
                            title_element = element.select_one(selector)
                            if title_element:
                                print(f"æ‰¾åˆ°æ ‡é¢˜å…ƒç´ : {selector}")
                                break
                                
                        # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»»ä½•çœ‹èµ·æ¥åƒæ ‡é¢˜çš„å…ƒç´ 
                        if not title_element:
                            # å°è¯•æ‰¾åˆ°ä»»ä½•å¤´éƒ¨å…ƒç´ æˆ–ç²—ä½“æ–‡æœ¬
                            for tag in ['h1', 'h2', 'h3', 'h4', 'strong', 'b']:
                                title_element = element.select_one(tag)
                                if title_element:
                                    break
                        
                        title = title_element.text.strip() if title_element else "æœªçŸ¥è¯¾ç¨‹"
                        
                        # æŸ¥æ‰¾URL - å…ˆæŸ¥æ‰¾å…ƒç´ æœ¬èº«æ˜¯å¦æ˜¯é“¾æ¥ï¼Œç„¶ååœ¨å…¶ä¸­æŸ¥æ‰¾é“¾æ¥
                        url = ""
                        if element.name == 'a' and element.has_attr('href'):
                            url = element['href']
                        else:
                            url_element = element.select_one('a')
                            if url_element and url_element.has_attr('href'):
                                url = url_element['href']
                                
                        # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                        if url and not url.startswith('http'):
                            url = f"https://www.xuetangx.com{url}"
                        
                        # å°è¯•å¤šç§å¯èƒ½çš„æ•™å¸ˆé€‰æ‹©å™¨
                        instructor_element = None
                        for selector in ['.teacher-name', '.teachers', '.instructor', '.professor', '[class*="teacher"]', '[class*="instructor"]']:
                            instructor_element = element.select_one(selector)
                            if instructor_element:
                                break
                                
                        instructor = instructor_element.text.strip() if instructor_element else "æœªçŸ¥è®²å¸ˆ"
                        
                        # å°è¯•å¤šç§å¯èƒ½çš„æè¿°é€‰æ‹©å™¨
                        description_element = None
                        for selector in ['.course-desc', '.description', '.intro', '.summary', 'p', '[class*="desc"]', '[class*="intro"]']:
                            description_element = element.select_one(selector)
                            if description_element:
                                break
                                
                        description = description_element.text.strip() if description_element else ""
                        
                        # å¦‚æœæè¿°ä¸ºç©ºï¼Œå°è¯•è·å–å…ƒç´ æœ¬èº«çš„æ–‡æœ¬å†…å®¹
                        if not description:
                            all_text = element.get_text(separator=' ', strip=True)
                            # ç§»é™¤æ ‡é¢˜å’Œè®²å¸ˆä¿¡æ¯
                            all_text = all_text.replace(title, '').replace(instructor, '')
                            description = all_text[:200] if all_text else ""
                        
                        # æå–è¯¾ç¨‹ä¸»é¢˜ï¼ˆå…³é”®è¯ï¼‰
                        topics = []
                        keyword_elements = element.select('.course-label')
                        for keyword in keyword_elements:
                            topic = keyword.text.strip()
                            if topic:
                                topics.append(topic)
                        
                        # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œä»æè¿°ä¸­æå–å…³é”®è¯
                        if not topics and description:
                            try:
                                import jieba.analyse
                                topics = jieba.analyse.extract_tags(description, topK=3)
                            except:
                                # å¦‚æœjiebaä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•åˆ†è¯
                                words = re.findall(r'[\w\u4e00-\u9fff]{2,}', description)
                                # é€‰æ‹©æœ€é•¿çš„3ä¸ªè¯ä½œä¸ºä¸»é¢˜
                                topics = sorted(set(words), key=len, reverse=True)[:3]
                        
                        courses.append({
                            'title': title,
                            'url': url,
                            'instructor': instructor,
                            'description': description,
                            'topics': topics
                        })
                    except Exception as e:
                        print(f"è§£æè¯¾ç¨‹å…ƒç´ é”™è¯¯: {str(e)}")
                        continue
                        
            return courses
            
        except Exception as e:
            print(f"è§£æè¯¾ç¨‹HTMLé”™è¯¯: {str(e)}")
            return []
            
    async def parse_knowledge_points(self, html, source_name, source_url):
        """è§£æçŸ¥è¯†ç‚¹ä¿¡æ¯"""
        if not html or not HAS_BS4:
            return []
            
        try:
            knowledge_points = []
            soup = BeautifulSoup(html, 'html.parser')
            
            if source_name == 'baidu':
                # è§£æç™¾åº¦ç™¾ç§‘çŸ¥è¯†ç‚¹
                try:
                    # æå–æ ‡é¢˜
                    title_element = soup.select_one('.lemmaWgt-lemmaTitle-title h1')
                    title = title_element.text.strip() if title_element else os.path.basename(source_url)
                    
                    # æå–å†…å®¹æ‘˜è¦
                    content_element = soup.select_one('.lemma-summary')
                    content = content_element.text.strip() if content_element else ""
                    
                    # æå–åˆ†ç±»ä¿¡æ¯
                    category_elements = soup.select('.lemmaWgt-subjectNav a')
                    categories = [element.text.strip() for element in category_elements if element.text.strip()]
                    category = categories[0] if categories else "è®¡ç®—æœº"
                    
                    # æå–ç›¸å…³ä¸»é¢˜
                    related_topics = []
                    
                    # æ–¹æ³•1ï¼šä»"ç›¸å…³çŸ¥è¯†"æ¨¡å—æå–
                    related_elements = soup.select('.lemma-reference a')
                    for element in related_elements:
                        topic = element.text.strip()
                        if topic and topic != title:
                            related_topics.append(topic)
                            
                    # æ–¹æ³•2ï¼šä»æ­£æ–‡ä¸­çš„é“¾æ¥æå–
                    if not related_topics:
                        content_links = soup.select('.lemma-summary a')
                        for link in content_links:
                            topic = link.text.strip()
                            if topic and topic != title and len(topic) > 1:
                                related_topics.append(topic)
                    
                    # æ–¹æ³•3ï¼šä»åˆ†ç±»å¯¼èˆªæå–
                    if not related_topics:
                        for category in categories:
                            if category != title and len(category) > 1:
                                related_topics.append(category)
                    
                    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç›¸å…³ä¸»é¢˜ï¼Œå°è¯•æå–æ­£æ–‡ä¸­çš„å…³é”®è¯
                    if not related_topics and content:
                        try:
                            import jieba.analyse
                            keywords = jieba.analyse.extract_tags(content, topK=3)
                            related_topics.extend(keywords)
                        except:
                            # å¦‚æœjiebaä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•åˆ†è¯
                            words = re.findall(r'[\w\u4e00-\u9fff]{2,}', content)
                            # è¿‡æ»¤æ‰å¸¸è§è¯
                            common_words = {'çŸ¥è¯†', 'ç§‘å­¦', 'æ–¹æ³•', 'æŠ€æœ¯', 'å†…å®¹', 'ç®€ä»‹', 'ç™¾ç§‘', 'åˆ†ç±»'}
                            words = [w for w in words if w not in common_words]
                            # é€‰æ‹©æœ€é•¿çš„3ä¸ªè¯ä½œä¸ºç›¸å…³ä¸»é¢˜
                            related_topics.extend(sorted(set(words), key=len, reverse=True)[:3])
                    
                    # ç¡®ä¿ç›¸å…³ä¸»é¢˜ä¸é‡å¤
                    related_topics = list(set(related_topics))
                    
                    knowledge_points.append({
                        'title': title,
                        'content': content,
                        'category': category,
                        'url': source_url,
                        'related_topics': related_topics
                    })
                    
                except Exception as e:
                    print(f"è§£æç™¾åº¦ç™¾ç§‘å†…å®¹é”™è¯¯: {str(e)}")
                    
            return knowledge_points
            
        except Exception as e:
            print(f"è§£æçŸ¥è¯†ç‚¹HTMLé”™è¯¯: {str(e)}")
            return []


class DataSourceCollector:
    """æ•°æ®æºé‡‡é›†å™¨"""
    
    def __init__(self):
        self.spider = MOOCSpider()
        self.sources = {
            'baidu': 'https://baike.baidu.com/item/'
        }
        
        # æœç´¢å…³é”®è¯
        self.keywords = [
            'è®¡ç®—æœº', 'äººå·¥æ™ºèƒ½', 'æ•°æ®ç§‘å­¦', 'è½¯ä»¶å·¥ç¨‹'
        ]
        
        # çŸ¥è¯†ç‚¹åˆ—è¡¨ - å¢åŠ æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œç¡®ä¿é‡è¦çš„è®¡ç®—æœºä¸“ä¸šçŸ¥è¯†è¢«é‡‡é›†
        self.knowledge_points = [
            'ç®—æ³•', 'äººå·¥æ™ºèƒ½', 'æ•°æ®ç»“æ„', 'è®¡ç®—æœºç§‘å­¦', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ',
            'è®¡ç®—æœºç½‘ç»œ', 'æ“ä½œç³»ç»Ÿ', 'æ•°æ®åº“', 'è½¯ä»¶å·¥ç¨‹', 'ç¼–è¯‘åŸç†'
        ]

        # ä½¿ç”¨å›½å†…æ•™è‚²èµ„æºç½‘ç«™
        self.course_sources = {
            'icourse163': {
                'url': 'https://www.icourse163.org',
                'categories': ['è®¡ç®—æœº', 'äººå·¥æ™ºèƒ½', 'ç¼–ç¨‹', 'æ•°æ®ç§‘å­¦', 'è½¯ä»¶å·¥ç¨‹', 'ç½‘ç»œå®‰å…¨']
            },
            'xuetangx': {
                'url': 'https://www.xuetangx.com',
                'categories': ['è®¡ç®—æœº', 'äººå·¥æ™ºèƒ½', 'ç¼–ç¨‹', 'æ•°æ®ç§‘å­¦', 'è½¯ä»¶å·¥ç¨‹', 'ç½‘ç»œå®‰å…¨']
            }
        }

        # çŸ¥è¯†ç‚¹æ•°æ®æº - åªä½¿ç”¨ç™¾åº¦ç™¾ç§‘å¹¶å¢åŠ è®¡ç®—æœºä¸“ä¸šç›¸å…³çŸ¥è¯†ç‚¹
        self.knowledge_sources = {
            'baidu': {
                'url': 'https://baike.baidu.com/item',
                # ç¡®ä¿æ ¸å¿ƒçš„è®¡ç®—æœºä¸“ä¸šçŸ¥è¯†ç‚¹æ’åœ¨åˆ—è¡¨å‰é¢ï¼Œä¼˜å…ˆè¢«é‡‡é›†
                'subtopics': [
                    # åŸºç¡€è®¡ç®—æœºç§‘å­¦æ ¸å¿ƒçŸ¥è¯†ç‚¹
                    'ç®—æ³•', 'æ•°æ®ç»“æ„', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'è®¡ç®—æœºç½‘ç»œ',
                    'æ“ä½œç³»ç»Ÿ', 'æ•°æ®åº“', 'è½¯ä»¶å·¥ç¨‹', 'ç¼–è¯‘åŸç†', 'è®¡ç®—æœºç»„æˆåŸç†',

                    # ç‰¹å®šç®—æ³•ç±»çŸ¥è¯†ç‚¹
                    'æ’åºç®—æ³•', 'æœç´¢ç®—æ³•', 'åŠ¨æ€è§„åˆ’', 'å›¾ç®—æ³•', 'è´ªå¿ƒç®—æ³•', 'åˆ†æ²»ç®—æ³•',

                    # ç‰¹å®šæ•°æ®ç»“æ„çŸ¥è¯†ç‚¹
                    'æ•°ç»„', 'é“¾è¡¨', 'æ ˆ', 'é˜Ÿåˆ—', 'å“ˆå¸Œè¡¨', 'æ ‘', 'å›¾', 'å †',

                    # ç‰¹å®šäººå·¥æ™ºèƒ½çŸ¥è¯†ç‚¹
                    'è‡ªç„¶è¯­è¨€å¤„ç†', 'è®¡ç®—æœºè§†è§‰', 'çŸ¥è¯†å›¾è°±', 'ç¥ç»ç½‘ç»œ', 'å¼ºåŒ–å­¦ä¹ ',

                    # å…¶ä»–è®¡ç®—æœºç§‘å­¦é¢†åŸŸ
                    'è®¡ç®—æœºç§‘å­¦', 'è®¡ç®—æœºå®‰å…¨', 'è½¯ä»¶æµ‹è¯•', 'åˆ†å¸ƒå¼ç³»ç»Ÿ', 'äº‘è®¡ç®—',
                    'å¤§æ•°æ®', 'å¹¶è¡Œè®¡ç®—', 'Webå¼€å‘', 'ç§»åŠ¨å¼€å‘', 'ç‰©è”ç½‘',

                    # æ‰©å±•ç±»åˆ« - åŸæœ‰å†…å®¹ä¿ç•™åœ¨åé¢
                    'ç¦»æ•£æ•°å­¦', 'è®¡ç®—ç†è®º', 'å½¢å¼è¯­è¨€', 'è‡ªåŠ¨æœºç†è®º', 'è®¡ç®—å¤æ‚æ€§ç†è®º',
                    'ä¿¡æ¯è®º', 'ç¼–ç ç†è®º', 'å¯†ç å­¦', 'è®¡ç®—å‡ ä½•', 'ç»„åˆä¼˜åŒ–',
                    'æ•°å€¼åˆ†æ', 'è¿ç­¹å­¦', 'è®¡ç®—è¯­è¨€å­¦', 'é‡å­è®¡ç®—', 'ç”Ÿç‰©è®¡ç®—',
                    'ç†è®ºè®¡ç®—æœºç§‘å­¦', 'é€»è¾‘å­¦', 'é›†åˆè®º', 'å›¾è®º', 'æ•°è®º',

                    # æ•°æ®ç»“æ„ç»†åˆ†
                    'äºŒå‰æ ‘', 'äºŒå‰æœç´¢æ ‘', 'å¹³è¡¡æ ‘', 'AVLæ ‘', 'çº¢é»‘æ ‘', 'Bæ ‘', 'B+æ ‘',
                    'å›¾ç»“æ„', 'å¹¶æŸ¥é›†', 'å­—å…¸æ ‘', 'çº¿æ®µæ ‘', 'æ ‘çŠ¶æ•°ç»„', 'è·³è¡¨',
                    'æ•£åˆ—å‡½æ•°', 'å¸ƒéš†è¿‡æ»¤å™¨', 'æœ€å°ç”Ÿæˆæ ‘', 'æœ€çŸ­è·¯å¾„', 'æ‹“æ‰‘æ’åº',

                    # ç®—æ³•ç»†åˆ†
                    'å¿«é€Ÿæ’åº', 'å½’å¹¶æ’åº', 'å †æ’åº', 'å†’æ³¡æ’åº', 'æ’å…¥æ’åº',
                    'é€‰æ‹©æ’åº', 'è®¡æ•°æ’åº', 'åŸºæ•°æ’åº', 'æ¡¶æ’åº', 'å¸Œå°”æ’åº',
                    'äºŒåˆ†æŸ¥æ‰¾', 'æ·±åº¦ä¼˜å…ˆæœç´¢', 'å¹¿åº¦ä¼˜å…ˆæœç´¢', 'A*ç®—æ³•',
                    'å›æº¯ç®—æ³•', 'åˆ†æ”¯é™ç•Œæ³•', 'æœ€çŸ­è·¯å¾„ç®—æ³•', 'æœ€å°ç”Ÿæˆæ ‘ç®—æ³•', 'KMPç®—æ³•', 'å¯å‘å¼ç®—æ³•',
                    'è’™ç‰¹å¡æ´›ç®—æ³•', 'é—ä¼ ç®—æ³•', 'æ¨¡æ‹Ÿé€€ç«ç®—æ³•', 'ç²’å­ç¾¤ä¼˜åŒ–',
                    'éšæœºç®—æ³•', 'è¿‘ä¼¼ç®—æ³•', 'åœ¨çº¿ç®—æ³•', 'ç¦»çº¿ç®—æ³•', 'å¹¶è¡Œç®—æ³•',
                    'å­—ç¬¦ä¸²åŒ¹é…', 'ç½‘ç»œæµç®—æ³•', 'FFTç®—æ³•', 'æ•°è®ºç®—æ³•',

                    # ç¼–ç¨‹è¯­è¨€ä¸å¼€å‘
                    'Python', 'Java', 'C++', 'JavaScript', 'Goè¯­è¨€',
                    'Rustè¯­è¨€', 'Cè¯­è¨€', 'PHP', 'SQL', 'TypeScript',
                    'Ruby', 'Swift', 'Kotlin', 'Scala', 'Rè¯­è¨€',
                    'MATLAB', 'Perl', 'Shellè„šæœ¬', 'Lisp', 'Prolog',
                    'Haskell', 'Erlang', 'Clojure', 'F#', 'Dart',
                    'COBOL', 'Fortran', 'Pascal', 'Ada', 'Assembly',
                    'WebAssembly', 'Objective-C', 'Visual Basic', 'Delphi', 'Julia',
                    'Groovy', 'PowerShell', 'Lua', 'Scheme', 'VHDL',

                    # ç¼–ç¨‹è¯­è¨€æ¦‚å¿µ
                    'ç¼–ç¨‹èŒƒå¼', 'å‘½ä»¤å¼ç¼–ç¨‹', 'å‡½æ•°å¼ç¼–ç¨‹', 'é€»è¾‘å¼ç¼–ç¨‹', 'é¢å‘å¯¹è±¡ç¼–ç¨‹',
                    'é¢å‘è¿‡ç¨‹ç¼–ç¨‹', 'äº‹ä»¶é©±åŠ¨ç¼–ç¨‹', 'å£°æ˜å¼ç¼–ç¨‹', 'å¹¶å‘ç¼–ç¨‹', 'å…ƒç¼–ç¨‹',
                    'æ³›å‹ç¼–ç¨‹', 'åå°„æœºåˆ¶', 'é—­åŒ…', 'åç¨‹', 'æ¨¡å—åŒ–',
                    'å¼‚å¸¸å¤„ç†', 'åƒåœ¾å›æ”¶', 'æŒ‡é’ˆ', 'å¼•ç”¨', 'å†…å­˜ç®¡ç†',
                    'ç±»å‹ç³»ç»Ÿ', 'é™æ€ç±»å‹', 'åŠ¨æ€ç±»å‹', 'å¼ºç±»å‹', 'å¼±ç±»å‹',
                    'ç±»å‹æ¨å¯¼', 'ç±»å‹æ£€æŸ¥', 'è¿ç®—ç¬¦é‡è½½', 'æ–¹æ³•é‡è½½', 'æ–¹æ³•é‡å†™',
                    'ç»§æ‰¿', 'å¤šæ€', 'å°è£…', 'æ¥å£', 'æŠ½è±¡ç±»',
                    'è®¾è®¡æ¨¡å¼', 'å•ä¾‹æ¨¡å¼', 'å·¥å‚æ¨¡å¼', 'è§‚å¯Ÿè€…æ¨¡å¼', 'MVCæ¨¡å¼',

                    # è½¯ä»¶å¼€å‘æ–¹æ³•è®º
                    'æ•æ·å¼€å‘', 'è®¾è®¡æ¨¡å¼', 'DevOps', 'æµ‹è¯•é©±åŠ¨å¼€å‘', 'æŒç»­é›†æˆ',
                    'æŒç»­éƒ¨ç½²', 'æŒç»­äº¤ä»˜', 'é¢å‘å¯¹è±¡åˆ†æä¸è®¾è®¡', 'é¢†åŸŸé©±åŠ¨è®¾è®¡', 'æé™ç¼–ç¨‹',
                    'Scrum', 'Kanban', 'ç€‘å¸ƒæ¨¡å‹', 'èºæ—‹æ¨¡å‹', 'è¿­ä»£å¼€å‘',
                    'å¢é‡å¼€å‘', 'å¿«é€ŸåŸå‹å¼€å‘', 'ç²¾ç›Šè½¯ä»¶å¼€å‘', 'é…ç½®ç®¡ç†', 'ç‰ˆæœ¬æ§åˆ¶',
                    'Git', 'SVN', 'CI/CD', 'ä»£ç å®¡æŸ¥', 'ç»“å¯¹ç¼–ç¨‹',
                    'é‡æ„', 'æŠ€æœ¯å€ºåŠ¡', 'ç”¨æˆ·æ•…äº‹', 'éªŒæ”¶æµ‹è¯•', 'å›å½’æµ‹è¯•',
                    'é›†æˆæµ‹è¯•', 'å•å…ƒæµ‹è¯•', 'ç³»ç»Ÿæµ‹è¯•', 'å‹åŠ›æµ‹è¯•', 'æ€§èƒ½æµ‹è¯•',
                    'å®‰å…¨æµ‹è¯•', 'å¯ç”¨æ€§æµ‹è¯•', 'A/Bæµ‹è¯•', 'è‡ªåŠ¨åŒ–æµ‹è¯•', 'æ‰‹åŠ¨æµ‹è¯•',
                    'é»‘ç›’æµ‹è¯•', 'ç™½ç›’æµ‹è¯•', 'ç°ç›’æµ‹è¯•', 'ç”¨ä¾‹æµ‹è¯•', 'å†’çƒŸæµ‹è¯•',
                    'å¾®æœåŠ¡', 'å®¹å™¨æŠ€æœ¯', 'Docker', 'Kubernetes', 'APIè®¾è®¡',
                    'RESTfulæ¶æ„', 'æœåŠ¡ç½‘æ ¼', 'æ— æœåŠ¡å™¨æ¶æ„', 'äº‘åŸç”Ÿ', 'å¯è§‚æµ‹æ€§',

                    # äººå·¥æ™ºèƒ½ä¸æ•°æ®ç§‘å­¦
                    'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'è‡ªç„¶è¯­è¨€å¤„ç†', 'è®¡ç®—æœºè§†è§‰',
                    'ç¥ç»ç½‘ç»œ', 'å¼ºåŒ–å­¦ä¹ ', 'æ•°æ®æŒ–æ˜', 'å¤§æ•°æ®', 'æ•°æ®ç§‘å­¦',
                    'ç›‘ç£å­¦ä¹ ', 'æ— ç›‘ç£å­¦ä¹ ', 'åŠç›‘ç£å­¦ä¹ ', 'è¿ç§»å­¦ä¹ ', 'å¯¹æŠ—å­¦ä¹ ',
                    'ç”Ÿæˆå¼AI', 'å†³ç­–æ ‘', 'éšæœºæ£®æ—', 'æ”¯æŒå‘é‡æœº', 'æœ´ç´ è´å¶æ–¯',
                    'Kè¿‘é‚»ç®—æ³•', 'Kå‡å€¼èšç±»', 'ä¸»æˆåˆ†åˆ†æ', 'çº¿æ€§å›å½’', 'é€»è¾‘å›å½’',
                    'æ¢¯åº¦ä¸‹é™', 'åå‘ä¼ æ’­', 'å·ç§¯ç¥ç»ç½‘ç»œ', 'å¾ªç¯ç¥ç»ç½‘ç»œ', 'é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ',
                    'é—¨æ§å¾ªç¯å•å…ƒ', 'æ³¨æ„åŠ›æœºåˆ¶', 'Transformer', 'BERT', 'GPT',
                    'è‡ªç¼–ç å™¨', 'å˜åˆ†è‡ªç¼–ç å™¨', 'ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ', 'çŸ¥è¯†å›¾è°±', 'è¯­ä¹‰ç½‘',
                    'è¯åµŒå…¥', 'Word2Vec', 'GloVe', 'LSTM', 'æƒ…æ„Ÿåˆ†æ',
                    'å‘½åå®ä½“è¯†åˆ«', 'æ–‡æœ¬åˆ†ç±»', 'æœºå™¨ç¿»è¯‘', 'è¯­éŸ³è¯†åˆ«', 'äººè„¸è¯†åˆ«',
                    'ç›®æ ‡æ£€æµ‹', 'å›¾åƒåˆ†å‰²', 'å§¿æ€ä¼°è®¡', 'YOLO', 'SSD',
                    'å¼ºåŒ–å­¦ä¹ ', 'Qå­¦ä¹ ', 'ç­–ç•¥æ¢¯åº¦', 'è’™ç‰¹å¡æ´›æ ‘æœç´¢', 'Alpha-Betaå‰ªæ',

                    # å‰ç«¯ä¸ç§»åŠ¨å¼€å‘
                    'å‰ç«¯å¼€å‘', 'åç«¯å¼€å‘', 'ç§»åŠ¨åº”ç”¨å¼€å‘', 'React', 'Vue.js',
                    'Angular', 'Node.js', 'Androidå¼€å‘', 'iOSå¼€å‘', 'Flutter',
                    'HTML', 'CSS', 'DOM', 'AJAX', 'JSON',
                    'XML', 'WebSocket', 'REST API', 'GraphQL', 'å¾®æœåŠ¡',
                    'jQuery', 'Bootstrap', 'Webpack', 'Babel', 'ESLint',
                    'React Native', 'Electron', 'PWA', 'SPA', 'SSR',
                    'JAMstack', 'WebAssembly', 'WebGL', 'Canvas', 'SVG',
                    'HTTP/2', 'HTTP/3', 'QUIC', 'Web Components', 'Shadow DOM',
                    'Service Worker', 'Web Workers', 'IndexedDB', 'localStorage', 'sessionStorage',
                    'Swift UI', 'Jetpack Compose', 'Kotlin Multiplatform', 'Xamarin', 'Cordova',

                    # æ•°æ®åº“ä¸å­˜å‚¨
                    'å…³ç³»å‹æ•°æ®åº“', 'NoSQL', 'MySQL', 'MongoDB', 'Redis',
                    'PostgreSQL', 'SQLite', 'æ•°æ®åº“ç´¢å¼•', 'äº‹åŠ¡å¤„ç†', 'ORM',
                    'SQLè¯­è¨€', 'æŸ¥è¯¢ä¼˜åŒ–', 'æ•°æ®åº“èŒƒå¼', 'ACID', 'BASEç†è®º',
                    'åˆ†å¸ƒå¼æ•°æ®åº“', 'æ—¶åºæ•°æ®åº“', 'å›¾æ•°æ®åº“', 'åˆ—å¼æ•°æ®åº“', 'é”®å€¼æ•°æ®åº“',
                    'æ–‡æ¡£æ•°æ®åº“', 'æ•°æ®ä»“åº“', 'æ•°æ®æ¹–', 'ETL', 'OLTP',
                    'OLAP', 'æ•°æ®é›†æˆ', 'æ•°æ®æ²»ç†', 'æ•°æ®è´¨é‡', 'ä¸»æ•°æ®ç®¡ç†',
                    'æ•°æ®å»ºæ¨¡', 'ERæ¨¡å‹', 'æ˜Ÿå‹æ¨¡å¼', 'é›ªèŠ±æ¨¡å¼', 'ç»´åº¦å»ºæ¨¡',
                    'Oracle', 'SQL Server', 'DB2', 'Cassandra', 'HBase',
                    'Neo4j', 'Elasticsearch', 'DynamoDB', 'CouchDB', 'InfluxDB',

                    # ç½‘ç»œä¸äº‘è®¡ç®—
                    'TCP/IPåè®®', 'HTTPåè®®', 'RESTful API', 'ç½‘ç»œå®‰å…¨', 'åˆ†å¸ƒå¼ç³»ç»Ÿ',
                    'äº‘è®¡ç®—', 'è¾¹ç¼˜è®¡ç®—', 'æœåŠ¡å™¨æ¶æ„', 'è´Ÿè½½å‡è¡¡', 'è™šæ‹ŸåŒ–æŠ€æœ¯',
                    'DNS', 'DHCP', 'NAT', 'VPN', 'VLAN',
                    'é˜²ç«å¢™', 'IDS/IPS', 'TLS/SSL', 'SSH', 'FTP',
                    'SMTP', 'IMAP', 'POP3', 'ç½‘ç»œæ‹“æ‰‘', 'è·¯ç”±ç®—æ³•',
                    'ç½‘ç»œå¸¦å®½', 'å»¶è¿Ÿ', 'ååé‡', 'ä¸¢åŒ…ç‡', 'ç½‘ç»œåè®®æ ˆ',
                    'OSIä¸ƒå±‚æ¨¡å‹', 'ç‰©ç†å±‚', 'æ•°æ®é“¾è·¯å±‚', 'ç½‘ç»œå±‚', 'ä¼ è¾“å±‚',
                    'ä¼šè¯å±‚', 'è¡¨ç¤ºå±‚', 'åº”ç”¨å±‚', 'IPv4', 'IPv6',
                    'ICMP', 'ARP', 'UDP', 'TCP', 'HTTP1.1',
                    'HTTP/2', 'HTTP/3', 'QUIC', 'WebSocket', 'gRPC',
                    'Thrift', 'MQTT', 'AMQP', 'Kafka', 'RabbitMQ',
                    'ZeroMQ', 'AWS', 'Azure', 'GCP', 'Alibaba Cloud',

                    # ç³»ç»Ÿä¸åº•å±‚æŠ€æœ¯
                    'æ“ä½œç³»ç»Ÿè®¾è®¡', 'å†…å­˜ç®¡ç†', 'è¿›ç¨‹ä¸çº¿ç¨‹', 'å¹¶å‘ç¼–ç¨‹', 'æ±‡ç¼–è¯­è¨€',
                    'è®¡ç®—æœºä½“ç³»ç»“æ„', 'CPUè®¾è®¡', 'å­˜å‚¨æŠ€æœ¯', 'åµŒå…¥å¼ç³»ç»Ÿ', 'FPGA',
                    'å¤„ç†å™¨æ¶æ„', 'x86', 'ARM', 'RISC-V', 'GPU',
                    'TPU', 'ASIC', 'å†…å­˜å±‚æ¬¡ç»“æ„', 'ç¼“å­˜', 'è™šæ‹Ÿå†…å­˜',
                    'æ–‡ä»¶ç³»ç»Ÿ', 'FAT', 'NTFS', 'ext4', 'ZFS',
                    'è¿›ç¨‹ç®¡ç†', 'çº¿ç¨‹ç®¡ç†', 'è°ƒåº¦ç®—æ³•', 'åŒæ­¥æœºåˆ¶', 'äº’æ–¥é”',
                    'ä¿¡å·é‡', 'æ¡ä»¶å˜é‡', 'æ­»é”', 'é¥¥é¥¿', 'ç«æ€æ¡ä»¶',
                    'ä¸­æ–­å¤„ç†', 'ç³»ç»Ÿè°ƒç”¨', 'é©±åŠ¨ç¨‹åº', 'å†…æ ¸æ¨¡å—', 'å¾®å†…æ ¸',
                    'å®å†…æ ¸', 'æ··åˆå†…æ ¸', 'UEFI', 'BIOS', 'å¼•å¯¼åŠ è½½ç¨‹åº',
                    'Linux', 'Windows', 'macOS', 'Unix', 'FreeBSD',
                    'Androidç³»ç»Ÿ', 'iOSç³»ç»Ÿ', 'å®æ—¶æ“ä½œç³»ç»Ÿ', 'åˆ†å¸ƒå¼æ“ä½œç³»ç»Ÿ', 'å¤šå¤„ç†å™¨ç³»ç»Ÿ',

                    # ç½‘ç»œå®‰å…¨ä¸å¯†ç å­¦
                    'ç½‘ç»œå®‰å…¨', 'ä¿¡æ¯å®‰å…¨', 'å¯†ç å­¦', 'åŠ å¯†ç®—æ³•', 'è®¤è¯åè®®',
                    'é˜²ç«å¢™', 'å…¥ä¾µæ£€æµ‹', 'æ¸—é€æµ‹è¯•', 'å®‰å…¨è¯„ä¼°', 'é£é™©ç®¡ç†',
                    'å¨èƒæƒ…æŠ¥', 'å®‰å…¨è¿è¥', 'åº”æ€¥å“åº”', 'ç¾éš¾æ¢å¤', 'ä¸šåŠ¡è¿ç»­æ€§',
                    'èº«ä»½è®¤è¯', 'è®¿é—®æ§åˆ¶', 'æƒé™ç®¡ç†', 'å•ç‚¹ç™»å½•', 'å¤šå› ç´ è®¤è¯',
                    'ç”Ÿç‰©è®¤è¯', 'æ•°å­—ç­¾å', 'æ•°å­—è¯ä¹¦', 'PKI', 'TLS/SSL',
                    'å¯¹ç§°åŠ å¯†', 'éå¯¹ç§°åŠ å¯†', 'å“ˆå¸Œå‡½æ•°', 'MD5', 'SHA',
                    'AES', 'DES', 'RSA', 'ECC', 'é‡å­åŠ å¯†',
                    'åŒºå—é“¾å®‰å…¨', 'Webå®‰å…¨', 'SQLæ³¨å…¥', 'XSSæ”»å‡»', 'CSRFæ”»å‡»',
                    'æ¶æ„è½¯ä»¶', 'ç—…æ¯’', 'è •è™«', 'æœ¨é©¬', 'å‹’ç´¢è½¯ä»¶',
                    'ç¤¾ä¼šå·¥ç¨‹å­¦', 'é’“é±¼æ”»å‡»', 'ä¸­é—´äººæ”»å‡»', 'DDoSæ”»å‡»', 'é›¶æ—¥æ¼æ´',

                    # æ–°å…´æŠ€æœ¯ä¸è·¨å­¦ç§‘é¢†åŸŸ
                    'åŒºå—é“¾', 'ç‰©è”ç½‘', 'é‡å­è®¡ç®—', '5GæŠ€æœ¯', '6GæŠ€æœ¯',
                    'å¢å¼ºç°å®', 'è™šæ‹Ÿç°å®', 'æ··åˆç°å®', 'è¾¹ç¼˜è®¡ç®—', 'é›¾è®¡ç®—',
                    'ç”Ÿç‰©ä¿¡æ¯å­¦', 'è®¡ç®—ç”Ÿç‰©å­¦', 'åŒ»å­¦ä¿¡æ¯å­¦', 'ç”Ÿç‰©è®¡ç®—', 'ç¥ç»å½¢æ€è®¡ç®—',
                    'ç±»è„‘è®¡ç®—', 'ç»¿è‰²è®¡ç®—', 'å¯æŒç»­è®¡ç®—', 'é«˜æ€§èƒ½è®¡ç®—', 'è¶…çº§è®¡ç®—æœº',
                    'å¹¶è¡Œè®¡ç®—', 'åˆ†å¸ƒå¼è®¡ç®—', 'ç½‘æ ¼è®¡ç®—', 'äº‘è®¡ç®—', 'é‡å­åŠ å¯†',
                    'é‡å­é€šä¿¡', 'é‡å­äº’è”ç½‘', 'æ•°å­—å­ªç”Ÿ', 'æ— äººé©¾é©¶', 'æœºå™¨äººæŠ€æœ¯',
                    'æ™ºèƒ½åˆ¶é€ ', 'å·¥ä¸š4.0', 'æ™ºæ…§åŸå¸‚', 'ç²¾å‡†åŒ»ç–—', 'åŸºå› ç¼–è¾‘',
                    'åˆæˆç”Ÿç‰©å­¦', 'çº³ç±³æŠ€æœ¯', 'å¤ªèµ«å…¹é€šä¿¡', 'å¯ç©¿æˆ´è®¾å¤‡', 'è„‘æœºæ¥å£',
                    'å…ƒå®‡å®™', 'NFT', 'å»ä¸­å¿ƒåŒ–é‡‘è', 'æ™ºèƒ½åˆçº¦', 'äººå·¥é€šç”¨æ™ºèƒ½'
                ]
            }
        }
        
        # é…ç½®é‡è¯•å‚æ•°
        self.max_retries = 3
        self.retry_delay = 5  # ç§’
        
        # è®¾ç½®è¯·æ±‚è¶…æ—¶
        if HAS_AIOHTTP:
            import aiohttp
            self.timeout = aiohttp.ClientTimeout(total=60)
        else:
            self.timeout = ClientTimeout(total=60)
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'Referer': 'https://www.baidu.com/',
            'sec-ch-ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1'
        }

        # è®°å½•ä¸Šæ¬¡æ›´æ–°æ—¶é—´ï¼Œç”¨äºå¢é‡æ›´æ–°
        self.last_update_time = time.time()
        # å­˜å‚¨å·²é‡‡é›†çš„çŸ¥è¯†ç‚¹ï¼Œé¿å…é‡å¤é‡‡é›†
        self.collected_knowledge_points = set()
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ä¸ºNone
        self.db = None
        self.db_mock = True

    async def collect_course_data(self):
        """é‡‡é›†è¯¾ç¨‹æ•°æ®"""
        updates = []
        try:
            print("å¼€å§‹é‡‡é›†è¯¾ç¨‹æ•°æ®ï¼Œå¹¶è¡Œå¤„ç†ä¸­...")
            start_time = time.time()

            # ç”Ÿæˆæ‰€æœ‰è¯¾ç¨‹æºå’Œç±»åˆ«ç»„åˆçš„ä»»åŠ¡åˆ—è¡¨
            tasks = []
            total_tasks = 0

            for source_name, source_info in self.course_sources.items():
                for category in source_info['categories']:
                    if source_name == 'icourse163':
                        url = f"{source_info['url']}/search.htm?search={urllib.parse.quote(category)}&page=1"
                    elif source_name == 'xuetangx':
                        url = f"{source_info['url']}/search?query={urllib.parse.quote(category)}&page=1"
                    elif source_name == 'mooc':
                        url = f"{source_info['url']}/search?keyword={urllib.parse.quote(category)}"
                    else:
                        continue
                    
                    task = asyncio.create_task(self._collect_course_from_source(source_name, url))
                    tasks.append(task)
                    total_tasks += 1

            print(f"åˆ›å»ºäº† {total_tasks} ä¸ªè¯¾ç¨‹æ•°æ®é‡‡é›†ä»»åŠ¡")

            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œä½†é™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…è¢«ç½‘ç«™å°ç¦
            max_concurrent = 5  # æœ€å¤§å¹¶å‘æ•°
            completed = 0

            # å¯¹ä»»åŠ¡è¿›è¡Œåˆ†æ‰¹å¤„ç†
            for i in range(0, len(tasks), max_concurrent):
                batch = tasks[i:i + max_concurrent]
                print(f"å¤„ç†ç¬¬ {i // max_concurrent + 1} æ‰¹ä»»åŠ¡ï¼Œå…± {len(batch)} ä¸ª")
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                batch_results = await asyncio.gather(*batch, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
                for result in batch_results:
                    completed += 1
                    progress = (completed / total_tasks) * 100
                    print(f"è¿›åº¦: {progress:.1f}% ({completed}/{total_tasks})")

                if isinstance(result, Exception):
                    print(f"é‡‡é›†è¯¾ç¨‹æ•°æ®é”™è¯¯: {str(result)}")
                elif result:  # ç¡®ä¿resultä¸æ˜¯None
                    updates.extend(result)
                    print(f"æˆåŠŸé‡‡é›† {len(result)} æ¡æ•°æ®")

                # åœ¨æ‰¹æ¬¡ä¹‹é—´æ·»åŠ å»¶è¿Ÿ
                if i + max_concurrent < len(tasks):
                    delay = random.randint(2, 5)
                    print(f"ç­‰å¾… {delay} ç§’åç»§ç»­ä¸‹ä¸€æ‰¹ä»»åŠ¡...")
                    await asyncio.sleep(delay)

            elapsed = time.time() - start_time
            print(f"è¯¾ç¨‹æ•°æ®é‡‡é›†å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f} ç§’ï¼Œè·å–åˆ° {len(updates)} æ¡æ•°æ®")
                    
        except Exception as e:
            print(f"é‡‡é›†è¯¾ç¨‹æ•°æ®é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()

        return updates

    async def collect_knowledge_data(self):
        """æ”¶é›†çŸ¥è¯†ç‚¹æ•°æ®"""
        print("å¼€å§‹ä»ç½‘ç»œé‡‡é›†çŸ¥è¯†ç‚¹æ•°æ®...")
        start_time = time.time()

        # ä»æ•°æ®åº“ä¸­è·å–å·²æœ‰çŸ¥è¯†ç‚¹
        existing_knowledge = set()
        try:
            # å¦‚æœå¯ä»¥è®¿é—®æ•°æ®åº“ï¼Œä»æ•°æ®åº“ä¸­è·å–å·²æœ‰çŸ¥è¯†ç‚¹
            if HAS_NEO4J and self.db and not self.db_mock:
                # æŸ¥è¯¢å·²æœ‰çš„æ‰€æœ‰çŸ¥è¯†ç‚¹
                query = """
                MATCH (n:KnowledgePoint)
                RETURN n.name as name
                """
                result = await self.db.query(query)
                if result and 'data' in result:
                    for record in result['data']:
                        if 'name' in record and record['name']:
                            existing_knowledge.add(record['name'].lower())

                print(f"æ•°æ®åº“ä¸­å·²å­˜åœ¨ {len(existing_knowledge)} ä¸ªçŸ¥è¯†ç‚¹")
        except Exception as e:
            print(f"è·å–å·²æœ‰çŸ¥è¯†ç‚¹å‡ºé”™: {str(e)}")

        # åˆå¹¶æ”¶é›†çš„æ•°æ®
        all_collected_data = []

        # å¯¹æ¯ä¸ªæ•°æ®æºè¿›è¡Œé‡‡é›†
        for source_name, source_info in self.knowledge_sources.items():
            source_url = source_info.get('url')
            if not source_url:
                continue

            collected_data = await self._collect_knowledge_from_source(source_name, source_url)

            # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„çŸ¥è¯†ç‚¹ï¼Œå®ç°å¢é‡æ›´æ–°
            new_collected_data = []
            for item in collected_data:
                # æ£€æŸ¥çŸ¥è¯†ç‚¹æ˜¯å¦å·²å­˜åœ¨
                if 'name' in item and item['name'] and item['name'].lower() not in existing_knowledge:
                    new_collected_data.append(item)
                    # å°†æ–°çŸ¥è¯†ç‚¹æ·»åŠ åˆ°å·²å­˜åœ¨é›†åˆä¸­ï¼Œé¿å…é‡å¤æ·»åŠ 
                    existing_knowledge.add(item['name'].lower())

            print(f"ä»{source_name}é‡‡é›†åˆ° {len(collected_data)} æ¡çŸ¥è¯†ç‚¹æ•°æ®ï¼Œå…¶ä¸­æ–°çŸ¥è¯†ç‚¹ {len(new_collected_data)} æ¡")
            all_collected_data.extend(new_collected_data)

        end_time = time.time()
        print(f"çŸ¥è¯†ç‚¹æ•°æ®é‡‡é›†å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’ï¼Œè·å–åˆ° {len(all_collected_data)} æ¡æ–°æ•°æ®")

        # å¦‚æœæ²¡æœ‰é‡‡é›†åˆ°æ•°æ®ï¼Œä½¿ç”¨æœ¬åœ°å­˜å‚¨çš„é¢„è®¾åˆ—è¡¨
        if not all_collected_data:
            print("è­¦å‘Šï¼šæœªèƒ½ä»æ•°æ®æºé‡‡é›†åˆ°çŸ¥è¯†ç‚¹æ•°æ®")
            # åˆ›å»ºä¸€äº›åŸºç¡€è®¡ç®—æœºçŸ¥è¯†ç‚¹æ•°æ®
            basic_knowledge = [
                "è®¡ç®—æœºç§‘å­¦", "æ•°æ®ç»“æ„", "ç®—æ³•", "æ“ä½œç³»ç»Ÿ", "è®¡ç®—æœºç½‘ç»œ",
                "æ•°æ®åº“", "è½¯ä»¶å·¥ç¨‹", "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "
            ]
            for name in basic_knowledge:
                if name.lower() not in existing_knowledge:
                    all_collected_data.append({
                        'name': name,
                        'description': f"è®¡ç®—æœºç§‘å­¦ä¸­çš„{name}æ¦‚å¿µ",
                        'source': 'system',
                        'url': f"https://example.com/{urllib.parse.quote(name)}",
                        'type': 'knowledge'
                    })
                    existing_knowledge.add(name.lower())

            print(f"ä½¿ç”¨é¢„è®¾çŸ¥è¯†ç‚¹åˆ—è¡¨ï¼Œæ·»åŠ äº† {len(all_collected_data)} æ¡çŸ¥è¯†ç‚¹æ•°æ®")

        # ä¿å­˜æˆåŠŸé‡‡é›†çš„çŸ¥è¯†ç‚¹åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œç”¨äºä¸‹æ¬¡å¢é‡æ›´æ–°
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs('data', exist_ok=True)

            # ä¿å­˜é‡‡é›†åˆ°çš„çŸ¥è¯†ç‚¹
            knowledge_file = 'data/collected_knowledge.json'

            # é¦–å…ˆè¯»å–å·²æœ‰çš„çŸ¥è¯†ç‚¹
            existing_data = []
            if os.path.exists(knowledge_file):
                try:
                    with open(knowledge_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                except Exception as e:
                    print(f"è¯»å–å·²æœ‰çŸ¥è¯†ç‚¹æ–‡ä»¶å‡ºé”™: {str(e)}")

            # åˆå¹¶æ–°é‡‡é›†çš„çŸ¥è¯†ç‚¹ä¸å·²æœ‰çš„çŸ¥è¯†ç‚¹
            merged_data = existing_data.copy()
            existing_names = {item['name'].lower() for item in existing_data if 'name' in item}

            for item in all_collected_data:
                if 'name' in item and item['name'].lower() not in existing_names:
                    merged_data.append(item)
                    existing_names.add(item['name'].lower())

            # ä¿å­˜åˆå¹¶åçš„çŸ¥è¯†ç‚¹
            with open(knowledge_file, 'w', encoding='utf-8') as f:
                json.dump(merged_data, f, ensure_ascii=False, indent=2)

            print(f"çŸ¥è¯†ç‚¹æ•°æ®ä¿å­˜æˆåŠŸï¼Œå…± {len(merged_data)} æ¡")
        except Exception as e:
            print(f"ä¿å­˜çŸ¥è¯†ç‚¹æ•°æ®å‡ºé”™: {str(e)}")

        return all_collected_data

    async def _collect_course_from_source(self, source_name, source_url):
        """ä»æŒ‡å®šæºé‡‡é›†è¯¾ç¨‹æ•°æ®"""
        for attempt in range(self.max_retries):
            try:
                # å…ˆå°è¯•ç›´æ¥ä½¿ç”¨APIè·å–æ•°æ®
                api_data = await self._fetch_api_data(source_name, source_url)
                if api_data:
                    print(f"æˆåŠŸé€šè¿‡APIè·å–{source_name}çš„æ•°æ®")
                    courses = await self.spider.parse_courses(api_data, source_name)
                    if courses:
                        updates = self._process_course_data(courses, source_name)
                        return updates
                    
                # APIè·å–å¤±è´¥ï¼Œå°è¯•è·å–é¡µé¢å†…å®¹
                # é¦–å…ˆå°è¯•ä½¿ç”¨å¼‚æ­¥æ–¹å¼
                html = await self.spider.fetch_page(source_url, source_name)

                # å¦‚æœå¼‚æ­¥æ–¹å¼å¤±è´¥ï¼Œå°è¯•åŒæ­¥æ–¹å¼
                if not html:
                    print(f"è­¦å‘Šï¼šå¼‚æ­¥è·å–{source_name}çš„é¡µé¢å†…å®¹å¤±è´¥ï¼Œå°è¯•åŒæ­¥æ–¹å¼...")
                    html = self.spider.fetch_page_sync(source_url, source_name)

                if not html:
                    print(f"è­¦å‘Šï¼šæ— æ³•è·å–{source_name}çš„é¡µé¢å†…å®¹ï¼Œå°è¯•é‡è¯•...")
                    await asyncio.sleep(self.retry_delay)
                    continue

                # è§£æè¯¾ç¨‹ä¿¡æ¯
                courses = await self.spider.parse_courses(html, source_name)
                if not courses:
                    print(f"è­¦å‘Šï¼šæ— æ³•ä»{source_name}è§£æè¯¾ç¨‹ä¿¡æ¯ï¼Œå°è¯•é‡è¯•...")
                    await asyncio.sleep(self.retry_delay)
                    continue
                
                updates = self._process_course_data(courses, source_name)
                return updates
            except Exception as e:
                print(f"ä»{source_name}é‡‡é›†è¯¾ç¨‹æ•°æ®é”™è¯¯: {str(e)}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(self.retry_delay)
                    continue
                return []
                
        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é¢„ç½®æ•°æ®
        print(f"æ‰€æœ‰å°è¯•ä»{source_name}è·å–æ•°æ®å‡å¤±è´¥ï¼Œä½¿ç”¨é¢„ç½®æ•°æ®...")
        mock_courses = []
        
        if source_name == 'icourse163':
            mock_courses = [
                {
                    'title': 'æ•°æ®åº“ç³»ç»Ÿæ¦‚è®º',
                    'url': 'https://www.icourse163.org/course/PKU-1001',
                    'instructor': 'ç‹çŠæ•™æˆ',
                    'description': 'æœ¬è¯¾ç¨‹ç³»ç»Ÿä»‹ç»æ•°æ®åº“ç³»ç»Ÿçš„åŸºæœ¬æ¦‚å¿µã€åŸç†å’ŒæŠ€æœ¯ï¼ŒåŒ…æ‹¬æ•°æ®æ¨¡å‹ã€SQLè¯­è¨€ã€æ•°æ®åº“è®¾è®¡ç­‰ã€‚',
                    'topics': ['æ•°æ®åº“', 'è®¡ç®—æœºç§‘å­¦', 'SQL']
                },
                {
                    'title': 'æ“ä½œç³»ç»ŸåŸç†',
                    'url': 'https://www.icourse163.org/course/THU-1002',
                    'instructor': 'é™ˆæ¸æ•™æˆ',
                    'description': 'æœ¬è¯¾ç¨‹ä»‹ç»æ“ä½œç³»ç»Ÿçš„åŸºæœ¬æ¦‚å¿µã€åŸç†å’Œå®ç°æŠ€æœ¯ï¼ŒåŒ…æ‹¬è¿›ç¨‹ç®¡ç†ã€å†…å­˜ç®¡ç†ã€æ–‡ä»¶ç³»ç»Ÿç­‰ã€‚',
                    'topics': ['æ“ä½œç³»ç»Ÿ', 'è®¡ç®—æœºç§‘å­¦', 'ç³»ç»Ÿç¼–ç¨‹']
                },
                {
                    'title': 'è®¡ç®—æœºç½‘ç»œ',
                    'url': 'https://www.icourse163.org/course/HIT-1003',
                    'instructor': 'æå»ºæ˜æ•™æˆ',
                    'description': 'æœ¬è¯¾ç¨‹ä»‹ç»è®¡ç®—æœºç½‘ç»œçš„åŸºæœ¬æ¦‚å¿µã€ä½“ç³»ç»“æ„å’Œåè®®ï¼ŒåŒ…æ‹¬ç‰©ç†å±‚ã€æ•°æ®é“¾è·¯å±‚ã€ç½‘ç»œå±‚ã€ä¼ è¾“å±‚å’Œåº”ç”¨å±‚ã€‚',
                    'topics': ['è®¡ç®—æœºç½‘ç»œ', 'TCP/IP', 'åè®®']
                }
            ]
        elif source_name == 'xuetangx':
            mock_courses = [
                {
                    'title': 'Pythonç¼–ç¨‹åŸºç¡€',
                    'url': 'https://www.xuetangx.com/course/CS001',
                    'instructor': 'å¼ æ•™æˆ',
                    'description': 'æœ¬è¯¾ç¨‹ä»‹ç»Pythonç¼–ç¨‹çš„åŸºç¡€çŸ¥è¯†ï¼ŒåŒ…æ‹¬æ•°æ®ç±»å‹ã€æ§åˆ¶ç»“æ„ã€å‡½æ•°å’Œæ¨¡å—ç­‰å†…å®¹ã€‚',
                    'topics': ['Python', 'ç¼–ç¨‹', 'è®¡ç®—æœºç§‘å­¦']
                },
                {
                    'title': 'æ•°æ®ç»“æ„ä¸ç®—æ³•',
                    'url': 'https://www.xuetangx.com/course/CS002',
                    'instructor': 'ææ•™æˆ',
                    'description': 'æœ¬è¯¾ç¨‹ä»‹ç»åŸºæœ¬æ•°æ®ç»“æ„å’Œç®—æ³•ï¼ŒåŒ…æ‹¬æ•°ç»„ã€é“¾è¡¨ã€é˜Ÿåˆ—ã€æ ˆã€æ ‘ã€å›¾ä»¥åŠå¸¸è§æ’åºå’Œæœç´¢ç®—æ³•ã€‚',
                    'topics': ['æ•°æ®ç»“æ„', 'ç®—æ³•', 'è®¡ç®—æœºç§‘å­¦']
                },
                {
                    'title': 'æœºå™¨å­¦ä¹ å…¥é—¨',
                    'url': 'https://www.xuetangx.com/course/CS003',
                    'instructor': 'ç‹æ•™æˆ',
                    'description': 'æœ¬è¯¾ç¨‹ä»‹ç»æœºå™¨å­¦ä¹ åŸºæœ¬æ¦‚å¿µå’Œå¸¸ç”¨ç®—æ³•ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚',
                    'topics': ['æœºå™¨å­¦ä¹ ', 'äººå·¥æ™ºèƒ½', 'æ•°æ®ç§‘å­¦']
                }
            ]
        
        if mock_courses:
            print(f"ä½¿ç”¨ {len(mock_courses)} ä¸ªé¢„ç½®çš„{source_name}è¯¾ç¨‹æ•°æ®")
            updates = self._process_course_data(mock_courses, source_name)
            return updates
            
        return []

    def _process_course_data(self, courses, source_name):
        """å¤„ç†è¯¾ç¨‹æ•°æ®ï¼Œè½¬æ¢ä¸ºçŸ¥è¯†å›¾è°±éœ€è¦çš„æ ¼å¼"""
        updates = []
        
        for course in courses:
            title = course.get('title', 'æœªçŸ¥è¯¾ç¨‹')
            url = course.get('url', '')
            instructor = course.get('instructor', 'æœªçŸ¥è®²å¸ˆ')
            description = course.get('description', '')
            topics = course.get('topics', [])
            
            # åˆ›å»ºè¯¾ç¨‹å®ä½“
            course_entity = {
                'id': f"course_{hashlib.md5(title.encode()).hexdigest()[:8]}",
                'name': title,
                'type': 'entity',
                'label': 'Course',
                'properties': {
                    'url': url,
                    'description': description,
                    'source': source_name
                },
                'confidence': 0.9
            }
            updates.append(course_entity)
            
            # åˆ›å»ºæ•™å¸ˆå®ä½“
            if instructor and instructor != 'æœªçŸ¥è®²å¸ˆ':
                instructor_entity = {
                    'id': f"instructor_{hashlib.md5(instructor.encode()).hexdigest()[:8]}",
                    'name': instructor,
                    'type': 'entity',
                    'label': 'Instructor',
                    'properties': {
                        'source': source_name
                    },
                    'confidence': 0.85
                }
                updates.append(instructor_entity)
                
                # åˆ›å»ºæ•™å¸ˆ-è¯¾ç¨‹å…³ç³»
                teaches_relation = {
                    'source': instructor_entity['id'],
                    'target': course_entity['id'],
                    'type': 'relation',
                    'label': 'TEACHES',
                    'properties': {
                        'source': source_name
                    },
                    'confidence': 0.85
                }
                updates.append(teaches_relation)
            
            # å¤„ç†ä¸»é¢˜/çŸ¥è¯†ç‚¹
            for topic in topics:
                if not topic or len(topic) < 2:  # è·³è¿‡è¿‡çŸ­çš„ä¸»é¢˜
                    continue
                    
                topic_entity = {
                    'id': f"topic_{hashlib.md5(topic.encode()).hexdigest()[:8]}",
                    'name': topic,
                    'type': 'entity',
                    'label': 'KnowledgePoint',
                    'properties': {
                        'source': source_name
                    },
                    'confidence': 0.8
                }
                updates.append(topic_entity)
                
                # åˆ›å»ºè¯¾ç¨‹-ä¸»é¢˜å…³ç³»
                covers_relation = {
                    'source': course_entity['id'],
                    'target': topic_entity['id'],
                    'type': 'relation',
                    'label': 'COVERS',
                    'properties': {
                        'source': source_name
                    },
                    'confidence': 0.8
                }
                updates.append(covers_relation)
        
        return updates
    
    async def _fetch_api_data(self, source_name, search_url):
        """ä½¿ç”¨APIè·å–è¯¾ç¨‹æ•°æ®"""
        try:
            print(f"å°è¯•ä½¿ç”¨APIæ–¹å¼è·å–{source_name}çš„æ•°æ®...")
            
            # ä»æœç´¢URLä¸­æå–æœç´¢å…³é”®è¯
            search_term = ""
            if source_name == 'xuetangx':
                match = re.search(r'query=([^&]+)', search_url)
                if match:
                    search_term = urllib.parse.unquote(match.group(1))
                    
                # æ”¹ç”¨è¯¾ç¨‹åˆ—è¡¨æ¥å£è€Œéæœç´¢æ¥å£
                api_url = "https://www.xuetangx.com/api/v1/lms/get_product_list/?page=1&page_size=12&category=1"
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'application/json, text/plain, */*',
                    'Accept-Language': 'zh-CN,zh;q=0.9',
                    'Connection': 'keep-alive',
                    'Referer': 'https://www.xuetangx.com/channel/computer',
                    'Origin': 'https://www.xuetangx.com',
                    'Sec-Fetch-Dest': 'empty',
                    'Sec-Fetch-Mode': 'cors',
                    'Sec-Fetch-Site': 'same-origin',
                    'x-client': 'web',
                    'Content-Type': 'application/json'
                }
                
                # ä½¿ç”¨Python 3.7+å¼‚æ­¥HTTPå®¢æˆ·ç«¯
                if HAS_AIOHTTP and self.spider.session:
                    async with self.spider.session.get(api_url, headers=headers) as response:
                        if response.status == 200:
                            data = await response.text()
                            print(f"æˆåŠŸè·å–å­¦å ‚åœ¨çº¿APIå“åº”ï¼Œå†…å®¹é•¿åº¦: {len(data)}")
                            return data
                        else:
                            print(f"å­¦å ‚åœ¨çº¿APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                else:
                    # å›é€€åˆ°åŒæ­¥è¯·æ±‚
                    session = requests.Session()
                    response = session.get(api_url, headers=headers, timeout=30)
                    if response.status_code == 200:
                        print(f"æˆåŠŸè·å–å­¦å ‚åœ¨çº¿APIå“åº”ï¼Œå†…å®¹é•¿åº¦: {len(response.text)}")
                        return response.text
                    else:
                        print(f"å­¦å ‚åœ¨çº¿APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                
                # å¦‚æœAPIè¯·æ±‚å¤±è´¥ï¼Œè¿”å›é¢„ç½®çš„å­¦å ‚åœ¨çº¿è¯¾ç¨‹æ•°æ®
                print("è¿”å›å­¦å ‚åœ¨çº¿æ¨¡æ‹Ÿæ•°æ®...")
                return json.dumps({
                    "data": {
                        "courseList": [
                            {
                                "id": "CS001",
                                "name": "Pythonç¼–ç¨‹åŸºç¡€",
                                "teacherName": "å¼ æ•™æˆ",
                                "description": "æœ¬è¯¾ç¨‹ä»‹ç»Pythonç¼–ç¨‹çš„åŸºç¡€çŸ¥è¯†ï¼ŒåŒ…æ‹¬æ•°æ®ç±»å‹ã€æ§åˆ¶ç»“æ„ã€å‡½æ•°å’Œæ¨¡å—ç­‰å†…å®¹ã€‚",
                                "category": "è®¡ç®—æœºç§‘å­¦"
                            },
                            {
                                "id": "CS002",
                                "name": "æ•°æ®ç»“æ„ä¸ç®—æ³•",
                                "teacherName": "ææ•™æˆ",
                                "description": "æœ¬è¯¾ç¨‹ä»‹ç»åŸºæœ¬æ•°æ®ç»“æ„å’Œç®—æ³•ï¼ŒåŒ…æ‹¬æ•°ç»„ã€é“¾è¡¨ã€é˜Ÿåˆ—ã€æ ˆã€æ ‘ã€å›¾ä»¥åŠå¸¸è§æ’åºå’Œæœç´¢ç®—æ³•ã€‚",
                                "category": "è®¡ç®—æœºç§‘å­¦"
                            },
                            {
                                "id": "CS003",
                                "name": "æœºå™¨å­¦ä¹ å…¥é—¨",
                                "teacherName": "ç‹æ•™æˆ",
                                "description": "æœ¬è¯¾ç¨‹ä»‹ç»æœºå™¨å­¦ä¹ åŸºæœ¬æ¦‚å¿µå’Œå¸¸ç”¨ç®—æ³•ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚",
                                "category": "äººå·¥æ™ºèƒ½"
                            }
                        ]
                    }
                })
            
            elif source_name == 'icourse163':
                match = re.search(r'search=([^&]+)', search_url)
                if match:
                    search_term = urllib.parse.unquote(match.group(1))
                
                # æ”¹ç”¨ç›´æ¥è·å–è¯¾ç¨‹åˆ—è¡¨çš„æ–¹å¼ï¼Œä¸ä½¿ç”¨æœç´¢API
                # ä¸­å›½å¤§å­¦MOOCçš„è¯¾ç¨‹åˆ—è¡¨API
                api_url = "https://www.icourse163.org/web/j/courseBean.getCoursePanelListByFrontParams.rpc"
                
                # ä½¿ç”¨æ›´å®Œæ•´çš„è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'application/json, text/javascript, */*; q=0.01',
                    'Accept-Language': 'zh-CN,zh;q=0.9',
                    'Connection': 'keep-alive',
                    'Content-Type': 'application/x-www-form-urlencoded',
                    'Referer': 'https://www.icourse163.org/channel/3002.htm',
                    'Origin': 'https://www.icourse163.org',
                    'Sec-Fetch-Dest': 'empty',
                    'Sec-Fetch-Mode': 'cors',
                    'Sec-Fetch-Site': 'same-origin',
                    'X-Requested-With': 'XMLHttpRequest',
                    'Cookie': 'NTESSTUDYSI=1; EDUWEBDEVICE=62adaea9d8204fb09e1a26b82647d72e'
                }
                
                # æ„é€ POSTè¯·æ±‚å‚æ•° - è®¡ç®—æœºä¸“ä¸šç›¸å…³è¯¾ç¨‹
                post_data = {
                    'p': '1',  # é¡µç 
                    'psize': '20',  # æ¯é¡µæ•°é‡
                    'type': '1',  # è¯¾ç¨‹ç±»å‹ï¼š1è¡¨ç¤ºæ™®é€šè¯¾ç¨‹
                    'categoryId': '2001',  # å¤§ç±»ï¼š2001è¡¨ç¤ºè®¡ç®—æœº
                    'orderBy': '0',  # æ’åºæ–¹å¼ï¼š0ä¸ºé»˜è®¤æ’åº
                    'stat0': '-1',  # è¯¾ç¨‹çŠ¶æ€ï¼šå…¨éƒ¨
                }
                
                # ä½¿ç”¨Python 3.7+å¼‚æ­¥HTTPå®¢æˆ·ç«¯
                if HAS_AIOHTTP and self.spider.session:
                    async with self.spider.session.post(api_url, headers=headers, data=post_data) as response:
                        if response.status == 200:
                            data = await response.text()
                            print(f"æˆåŠŸè·å–ä¸­å›½å¤§å­¦MOOC APIå“åº”ï¼Œå†…å®¹é•¿åº¦: {len(data)}")
                            return data
                        else:
                            print(f"ä¸­å›½å¤§å­¦MOOC APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                else:
                    # å›é€€åˆ°åŒæ­¥è¯·æ±‚
                    session = requests.Session()
                    response = session.post(api_url, headers=headers, data=post_data, timeout=30)
                    if response.status_code == 200:
                        print(f"æˆåŠŸè·å–ä¸­å›½å¤§å­¦MOOC APIå“åº”ï¼Œå†…å®¹é•¿åº¦: {len(response.text)}")
                        return response.text
                    else:
                        print(f"ä¸­å›½å¤§å­¦MOOC APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                
                # å¦‚æœAPIè¯·æ±‚å¤±è´¥ï¼Œè¿”å›é¢„ç½®çš„ä¸­å›½å¤§å­¦MOOCè¯¾ç¨‹æ•°æ®
                print("è¿”å›ä¸­å›½å¤§å­¦MOOCæ¨¡æ‹Ÿæ•°æ®...")
                return json.dumps({
                    "result": {
                        "list": [
                            {
                                "id": "1001",
                                "name": "æ•°æ®åº“ç³»ç»Ÿæ¦‚è®º",
                                "schoolName": "åŒ—äº¬å¤§å­¦",
                                "teacherName": "ç‹çŠæ•™æˆ",
                                "description": "æœ¬è¯¾ç¨‹ç³»ç»Ÿä»‹ç»æ•°æ®åº“ç³»ç»Ÿçš„åŸºæœ¬æ¦‚å¿µã€åŸç†å’ŒæŠ€æœ¯ï¼ŒåŒ…æ‹¬æ•°æ®æ¨¡å‹ã€SQLè¯­è¨€ã€æ•°æ®åº“è®¾è®¡ç­‰ã€‚",
                                "categoryName": "è®¡ç®—æœº"
                            },
                            {
                                "id": "1002",
                                "name": "æ“ä½œç³»ç»ŸåŸç†",
                                "schoolName": "æ¸…åå¤§å­¦",
                                "teacherName": "é™ˆæ¸æ•™æˆ",
                                "description": "æœ¬è¯¾ç¨‹ä»‹ç»æ“ä½œç³»ç»Ÿçš„åŸºæœ¬æ¦‚å¿µã€åŸç†å’Œå®ç°æŠ€æœ¯ï¼ŒåŒ…æ‹¬è¿›ç¨‹ç®¡ç†ã€å†…å­˜ç®¡ç†ã€æ–‡ä»¶ç³»ç»Ÿç­‰ã€‚",
                                "categoryName": "è®¡ç®—æœº"
                            },
                            {
                                "id": "1003",
                                "name": "è®¡ç®—æœºç½‘ç»œ",
                                "schoolName": "å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦",
                                "teacherName": "æå»ºæ˜æ•™æˆ",
                                "description": "æœ¬è¯¾ç¨‹ä»‹ç»è®¡ç®—æœºç½‘ç»œçš„åŸºæœ¬æ¦‚å¿µã€ä½“ç³»ç»“æ„å’Œåè®®ï¼ŒåŒ…æ‹¬ç‰©ç†å±‚ã€æ•°æ®é“¾è·¯å±‚ã€ç½‘ç»œå±‚ã€ä¼ è¾“å±‚å’Œåº”ç”¨å±‚ã€‚",
                                "categoryName": "è®¡ç®—æœº"
                            }
                        ]
                    }
                })
            
            return None
        except Exception as e:
            print(f"APIæ•°æ®è·å–é”™è¯¯: {str(e)}")
            # å‘ç”Ÿé”™è¯¯æ—¶ä¹Ÿè¿”å›æ¨¡æ‹Ÿæ•°æ®
            if source_name == 'xuetangx':
                return json.dumps({
                    "data": {
                        "courseList": [
                            {
                                "id": "CS001",
                                "name": "Pythonç¼–ç¨‹åŸºç¡€",
                                "teacherName": "å¼ æ•™æˆ",
                                "description": "æœ¬è¯¾ç¨‹ä»‹ç»Pythonç¼–ç¨‹çš„åŸºç¡€çŸ¥è¯†ï¼ŒåŒ…æ‹¬æ•°æ®ç±»å‹ã€æ§åˆ¶ç»“æ„ã€å‡½æ•°å’Œæ¨¡å—ç­‰å†…å®¹ã€‚",
                                "category": "è®¡ç®—æœºç§‘å­¦"
                            }
                        ]
                    }
                })
            elif source_name == 'icourse163':
                return json.dumps({
                    "result": {
                        "list": [
                            {
                                "id": "1001",
                                "name": "æ•°æ®åº“ç³»ç»Ÿæ¦‚è®º",
                                "schoolName": "åŒ—äº¬å¤§å­¦",
                                "teacherName": "ç‹çŠæ•™æˆ",
                                "description": "æœ¬è¯¾ç¨‹ç³»ç»Ÿä»‹ç»æ•°æ®åº“ç³»ç»Ÿçš„åŸºæœ¬æ¦‚å¿µã€åŸç†å’ŒæŠ€æœ¯ï¼ŒåŒ…æ‹¬æ•°æ®æ¨¡å‹ã€SQLè¯­è¨€ã€æ•°æ®åº“è®¾è®¡ç­‰ã€‚",
                                "categoryName": "è®¡ç®—æœº"
                            }
                        ]
                    }
                })
            return None

    async def _collect_knowledge_from_source(self, source_name, source_url):
        """ä»æ•°æ®æºé‡‡é›†çŸ¥è¯†ç‚¹æ•°æ®"""
        print(f"\nå¼€å§‹ä»{source_name}é‡‡é›†çŸ¥è¯†ç‚¹æ•°æ®...")
        collected_data = []

        try:
            # æ„å»ºçŸ¥è¯†ç‚¹URLåˆ—è¡¨
            knowledge_urls = []

            if source_name == 'baidu':
                # è·å–è¯¥æ¥æºçš„çŸ¥è¯†ç‚¹åˆ—è¡¨
                topics = self.knowledge_sources[source_name]['subtopics']
                total_topics = len(topics)
                print(f"å‡†å¤‡é‡‡é›† {total_topics} ä¸ªçŸ¥è¯†ç‚¹")

                # å¢åŠ æ‰¹æ¬¡å¤§å°ï¼Œæé«˜æ•ˆç‡
                batch_size = 10  # ä»1ä¸ªå¢åŠ åˆ°10ä¸ªï¼Œæ˜¾è‘—æé«˜å¹¶å‘æ•ˆç‡
                total_batches = (total_topics + batch_size - 1) // batch_size

                start_time = time.time()
                print(f"å…±è®¡ {total_batches} ä¸ªæ‰¹æ¬¡, é¢„è®¡å®Œæˆæ—¶é—´: {total_batches * 30} ç§’")

                for batch_idx in range(total_batches):
                    start_idx = batch_idx * batch_size
                    end_idx = min(start_idx + batch_size, total_topics)
                    batch_topics = topics[start_idx:end_idx]

                    batch_start_time = time.time()
                    print(f"\nğŸ“¦ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ä»»åŠ¡ï¼Œå…± {len(batch_topics)} ä¸ªçŸ¥è¯†ç‚¹")
                    print(f"   è¿›åº¦: {(batch_idx + 1) / total_batches * 100:.1f}% ({batch_idx + 1}/{total_batches})")

                    batch_urls = []
                    for topic in batch_topics:
                        # æ„å»ºURL
                        url = f"{source_url}/{urllib.parse.quote(topic)}"
                        batch_urls.append((url, topic))

                    # ä¸ºæ¯ä¸ªä¸»é¢˜åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
                    tasks = []
                    for url, topic in batch_urls:
                        task = asyncio.create_task(self._fetch_knowledge_data(source_name, url, topic))
                        tasks.append(task)

                    # å¹¶è¡Œæ‰§è¡Œæœ¬æ‰¹æ¬¡çš„æ‰€æœ‰ä»»åŠ¡
                    batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                    # å¤„ç†ç»“æœ
                    batch_success = 0
                    for result in batch_results:
                        if isinstance(result, Exception):
                            print(f"âŒ è·å–çŸ¥è¯†ç‚¹æ•°æ®æ—¶å‡ºé”™: {str(result)}")
                            continue
                        
                        if result and len(result) > 0:
                            collected_data.extend(result)
                            batch_success += 1

                    batch_elapsed = time.time() - batch_start_time
                    print(f"   æ‰¹æ¬¡è€—æ—¶: {batch_elapsed:.2f} ç§’ï¼ŒæˆåŠŸ: {batch_success}/{len(batch_topics)}")

                    # æ„å»ºéƒ¨åˆ†çŸ¥è¯†å›¾è°±ï¼Œä»¥ä¾¿æŸ¥çœ‹æ•ˆæœ
                    if batch_idx > 0 and (batch_idx % 2 == 0 or batch_idx == total_batches - 1):
                        print(f"\nğŸ”„ å·²å®Œæˆ {batch_idx + 1}/{total_batches} æ‰¹æ¬¡ï¼Œå¼€å§‹æ„å»ºéƒ¨åˆ†çŸ¥è¯†å›¾è°±...")
                        # åˆ›å»ºä¸´æ—¶æ•°æ®ï¼Œè¿›è¡Œéƒ¨åˆ†æ„å»º
                        temp_data = collected_data.copy()

                        # åˆ›å»ºä¸€ä¸ªäº‹ä»¶ï¼Œä»¥ä¾¿åœ¨åå°è¿›è¡Œå¤„ç†
                        build_event = asyncio.create_task(self._build_partial_graph(temp_data, batch_idx + 1))
                        # ä¸ç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œç»§ç»­é‡‡é›†æ•°æ®

                    # åœ¨æ‰¹æ¬¡ä¹‹é—´æ·»åŠ åˆç†çš„å»¶è¿Ÿï¼Œé¿å…è§¦å‘åçˆ¬ä½†åˆä¸è¿‡æ…¢
                    if batch_idx < total_batches - 1:
                        delay = 10 + random.random() * 5  # å‡å°‘å»¶è¿Ÿåˆ°10-15ç§’
                        print(f"   ç­‰å¾… {delay:.2f} ç§’åå¤„ç†ä¸‹ä¸€æ‰¹æ¬¡...")
                        await asyncio.sleep(delay)
                    else:
                        print(f"âœ… æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼")

                total_elapsed = time.time() - start_time
                print(f"\næ€»é‡‡é›†è€—æ—¶: {total_elapsed:.2f} ç§’ï¼Œå¹³å‡æ¯ä¸ªçŸ¥è¯†ç‚¹ {total_elapsed / total_topics:.2f} ç§’")

            # è¿”å›æ”¶é›†åˆ°çš„æ•°æ®
            print(f"ä»{source_name}æˆåŠŸé‡‡é›† {len(collected_data)} æ¡çŸ¥è¯†ç‚¹æ•°æ®")
            return collected_data

        except Exception as e:
            print(f"âŒ é‡‡é›†çŸ¥è¯†ç‚¹æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            return []

    async def _build_partial_graph(self, data, batch_num):
        """æ„å»ºéƒ¨åˆ†çŸ¥è¯†å›¾è°±ï¼Œç”¨äºé¢„è§ˆæ•ˆæœ"""
        try:
            print(f"å¼€å§‹æ„å»ºç¬¬ {batch_num} æ‰¹æ¬¡çš„ä¸´æ—¶çŸ¥è¯†å›¾è°±...")
            # åˆ›å»ºå®ä½“
            entities = [item for item in data if item.get('type') == 'entity']
            # å¤„ç†çŸ¥è¯†ç‚¹ï¼Œåˆ›å»ºå…³ç³»
            relations = []

                                # ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹åˆ›å»ºå…³ç³»
            for entity in entities:
                if entity.get('label') == 'KnowledgePoint':
                    # è·å–ç›¸å…³ä¸»é¢˜
                    related_topics = entity.get('properties', {}).get('related_topics', [])
                    for topic in related_topics:
                        # æ ¹æ®çŸ¥è¯†ç‚¹åç§°åˆ¤æ–­å…³ç³»ç±»å‹
                        # å¦‚æœknowledgeä¸»é¢˜å’ŒtopicåŒ…å«ç›¸åŒçš„è¯ï¼Œå¯èƒ½æ˜¯åŒ…å«å…³ç³»
                        knowledge_name = entity.get('name', '').lower()
                        topic_lower = topic.lower()
                        
                        # ç¡®å®šå…³ç³»ç±»å‹ - é»˜è®¤ä¸ºCONTAINSï¼Œå‡å°‘RELATED_TOçš„ä½¿ç”¨
                        relation_type = 'CONTAINS'
                        # åªæœ‰åœ¨ä¸»é¢˜åè¶…è¿‡çŸ¥è¯†ç‚¹åç§°é•¿åº¦çš„æƒ…å†µä¸‹æ‰è€ƒè™‘ä½¿ç”¨RELATED_TO
                        if len(topic) < len(knowledge_name) and topic_lower not in knowledge_name:
                            relation_type = 'RELATED_TO'
                            
                        relations.append({
                            'type': 'relation',
                            'source': entity.get('name'),
                            'target': topic,
                            'relation_type': relation_type,
                            'source_type': 'derived',
                            'confidence': 0.8,
                            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                        })

            # åˆå¹¶å®ä½“å’Œå…³ç³»
            processed_data = entities + relations

            # åº”ç”¨åˆ°æ¨¡æ‹Ÿæ•°æ®åº“
            if hasattr(self, 'db') and self.db and not self.db_mock:
                # å¦‚æœæœ‰çœŸå®æ•°æ®åº“è¿æ¥ï¼Œå†™å…¥æ•°æ®åº“
                # æ­¤å¤„éœ€è¦æ ¹æ®å®é™…æƒ…å†µå®ç°
                pass
            else:
                # æ›´æ–°æ¨¡æ‹Ÿæ•°æ®åº“
                self._update_mock_db(processed_data)

            print(f"âœ… ç¬¬ {batch_num} æ‰¹æ¬¡ä¸´æ—¶çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(entities)} ä¸ªå®ä½“å’Œ {len(relations)} ä¸ªå…³ç³»")
            return True
        except Exception as e:
            print(f"âŒ æ„å»ºéƒ¨åˆ†çŸ¥è¯†å›¾è°±å¤±è´¥: {str(e)}")
            return False

    def _update_mock_db(self, data):
        """æ›´æ–°æ¨¡æ‹Ÿæ•°æ®åº“"""
        # åˆå§‹åŒ–æ¨¡æ‹Ÿæ•°æ®åº“ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
        if not hasattr(self, 'mock_db'):
            self.mock_db = {
                'entities': {},
                'relations': []
            }

        # æ›´æ–°å®ä½“
        entities_added = 0
        for item in data:
            if item.get('type') == 'entity':
                name = item.get('name')
                if name and name not in self.mock_db['entities']:
                    self.mock_db['entities'][name] = item
                    entities_added += 1

        # æ›´æ–°å…³ç³»
        relations_added = 0
        for item in data:
            if item.get('type') == 'relation':
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°å…³ç³»
                source = item.get('source')
                target = item.get('target')
                relation_type = item.get('relation_type')

                # æ£€æŸ¥å…³ç³»æ˜¯å¦å·²å­˜åœ¨
                is_new = True
                for rel in self.mock_db['relations']:
                    if (rel.get('source') == source and
                            rel.get('target') == target and
                            rel.get('relation_type') == relation_type):
                        is_new = False
                        break

                if is_new:
                    self.mock_db['relations'].append(item)
                    relations_added += 1

        print(f"æ¨¡æ‹Ÿæ•°æ®åº“æ›´æ–°ï¼šå¢åŠ äº† {entities_added} ä¸ªå®ä½“å’Œ {relations_added} ä¸ªå…³ç³»")

    async def _fetch_knowledge_data(self, source_name, url, topic):
        """è·å–å•ä¸ªçŸ¥è¯†ç‚¹çš„æ•°æ®"""
        print(f"ğŸ“„ è·å–çŸ¥è¯†ç‚¹æ•°æ®: {topic}")
        try:
            # è°ƒç”¨spiderè·å–é¡µé¢å†…å®¹
            content = await self.spider.fetch_page(url, source_name)

            if not content:
                print(f"âŒ æ— æ³•è·å–çŸ¥è¯†ç‚¹é¡µé¢å†…å®¹: {topic}")
                return []

            content_length = len(content)
            print(f"âœ… æˆåŠŸè·å–çŸ¥è¯†ç‚¹ '{topic}' çš„é¡µé¢å†…å®¹, å¤§å°: {content_length} å­—èŠ‚")

            # è§£æçŸ¥è¯†ç‚¹æ•°æ®
            if source_name == 'baidu':
                result = self._parse_baidu_knowledge(content, topic)
                print(f"   è§£æå¾—åˆ° {len(result)} æ¡è®°å½•")
                return result

            return []
        except Exception as e:
            print(f"âŒ è·å–çŸ¥è¯†ç‚¹ {topic} æ—¶å‡ºé”™: {str(e)}")
            return []

    def _parse_baidu_knowledge(self, html, topic):
        """è§£æç™¾åº¦ç™¾ç§‘çŸ¥è¯†ç‚¹æ•°æ®"""
        try:
            results = []

            # å¦‚æœæ²¡æœ‰BeautifulSoupï¼Œåˆ™ä½¿ç”¨ç®€å•è§£æ
            if not HAS_BS4:
                print("   ä½¿ç”¨ç®€å•æ­£åˆ™è¡¨è¾¾å¼è§£æHTML")
                # ç®€å•æå–æ ‡é¢˜å’Œç®€ä»‹
                title_match = re.search(r'<h1[^>]*>(.*?)</h1>', html)
                title = title_match.group(1) if title_match else topic

                # å°è¯•æå–ç®€ä»‹
                summary_match = re.search(r'<div class="lemma-summary[^>]*>(.*?)</div>', html, re.DOTALL)
                summary = summary_match.group(1) if summary_match else f"å…³äº{topic}çš„çŸ¥è¯†ç‚¹"

                # ç§»é™¤HTMLæ ‡ç­¾
                summary = re.sub(r'<[^>]+>', '', summary)
                summary = re.sub(r'\s+', ' ', summary).strip()

                # å°è¯•æå–ç›¸å…³ä¸»é¢˜
                related_topics = []
                link_pattern = re.compile(r'<a[^>]*href="[^"]*"[^>]*>(.*?)</a>', re.DOTALL)
                for match in link_pattern.finditer(html):
                    link_text = match.group(1).strip()
                    if link_text and len(link_text) > 1 and link_text != title:
                        related_topics.append(link_text)

                # å°è¯•æå–åˆ†ç±»ä¿¡æ¯
                category_match = re.search(r'<dt>åˆ†ç±»</dt>\s*<dd>(.*?)</dd>', html, re.DOTALL)
                category = category_match.group(1).strip() if category_match else "è®¡ç®—æœºç§‘å­¦"

                # é™åˆ¶ç›¸å…³ä¸»é¢˜æ•°é‡
                related_topics = list(set(related_topics))[:5]

                    # åˆ›å»ºçŸ¥è¯†ç‚¹å®ä½“
                entity = {
                        'type': 'entity',
                    'name': title,
                        'label': 'KnowledgePoint',
                    'source': 'baidu',
                        'confidence': 0.9,
                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                        'properties': {
                        'description': summary[:300] + '...' if len(summary) > 300 else summary,
                        'category': category,
                        'url': f"https://baike.baidu.com/item/{urllib.parse.quote(topic)}",
                        'related_topics': related_topics
                    }
                }

                # å°è¯•æå–é¢å¤–å±æ€§
                extra_attrs = {}
                attr_pattern = re.compile(r'<dt>(.*?)</dt>\s*<dd>(.*?)</dd>', re.DOTALL)
                for match in attr_pattern.finditer(html):
                    key = match.group(1).strip()
                    value = match.group(2).strip()
                    if key and value and key not in ['ä¸­æ–‡å', 'é¢†åŸŸ', 'åˆ†ç±»', 'ç›¸å…³ä¸»é¢˜']:
                        # ç§»é™¤HTMLæ ‡ç­¾
                        value = re.sub(r'<[^>]+>', '', value).strip()
                        extra_attrs[key] = value

                # æ·»åŠ é¢å¤–å±æ€§
                if extra_attrs:
                    for key, value in extra_attrs.items():
                        entity['properties'][key] = value

                results.append(entity)
                print(f"   æˆåŠŸè§£æçŸ¥è¯†ç‚¹: {title}ï¼Œæ‰¾åˆ° {len(related_topics)} ä¸ªç›¸å…³ä¸»é¢˜å’Œ {len(extra_attrs)} ä¸ªé¢å¤–å±æ€§")
                return results

            # ä½¿ç”¨BeautifulSoupè§£æ
            print("   ä½¿ç”¨BeautifulSoupè§£æHTML")
            soup = BeautifulSoup(html, 'html.parser')

            # æå–æ ‡é¢˜
            title_elem = soup.find('h1')
            title = title_elem.text.strip() if title_elem else topic

            # æå–ç®€ä»‹
            summary_elem = soup.select_one('.lemma-summary')
            if summary_elem:
                summary = summary_elem.text.strip()
            else:
                summary = f"å…³äº{topic}çš„çŸ¥è¯†ç‚¹"

            # æå–åˆ†ç±»ä¿¡æ¯
            category = "è®¡ç®—æœºç§‘å­¦"
            category_elem = soup.select('.basic-info dt:contains("åˆ†ç±»") + dd')
            if category_elem:
                category = category_elem[0].text.strip()
            else:
                # å°è¯•ä»å…¶ä»–ä½ç½®è·å–åˆ†ç±»
                catalog_elems = soup.select('.catalog-list a')
                if catalog_elems:
                    category_texts = [elem.text.strip() for elem in catalog_elems]
                    if category_texts:
                        category = category_texts[0]

            # æå–ç›¸å…³ä¸»é¢˜
            related_topics = []

            # ç›´æ¥ä»ç›¸å…³ä¸»é¢˜å­—æ®µæå–
            rel_topics_elem = soup.select('.basic-info dt:contains("ç›¸å…³ä¸»é¢˜") + dd')
            if rel_topics_elem:
                topics_text = rel_topics_elem[0].text.strip()
                if topics_text:
                    # æ‹†åˆ†é€—å·åˆ†éš”çš„ä¸»é¢˜
                    related_topics = [t.strip() for t in topics_text.split(',')]

            # å¦‚æœä¸Šé¢æ–¹æ³•æ²¡æ‰¾åˆ°ç›¸å…³ä¸»é¢˜ï¼Œå°è¯•ä»é“¾æ¥ä¸­æå–
            if not related_topics:
                # æ–¹æ³•1ï¼šä»æ­£æ–‡ä¸­çš„é“¾æ¥æå–
                summary_links = soup.select('.lemma-summary a')
                for link in summary_links:
                    link_text = link.text.strip()
                    if link_text and len(link_text) > 1 and link_text != title:
                        related_topics.append(link_text)

                # æ–¹æ³•2ï¼šä»é¡µé¢å…¶ä»–éƒ¨åˆ†æå–
                if len(related_topics) < 3:
                    other_links = soup.select('a')
                    for link in other_links:
                        link_text = link.text.strip()
                        if (link_text and len(link_text) > 1 and link_text != title
                                and not any(c in link_text for c in '.,;:(){}[]<>/\\\'"`')
                                and len(link_text) < 20  # é¿å…å¤ªé•¿çš„æ–‡æœ¬
                                and link_text not in related_topics):
                            related_topics.append(link_text)
                            if len(related_topics) >= 5:
                                break

            # ç¡®ä¿ç›¸å…³ä¸»é¢˜ä¸é‡å¤ï¼Œé™åˆ¶æ•°é‡
            related_topics = list(set(related_topics))[:5]

            # æå–é¢å¤–å±æ€§
            extra_properties = {}

            # è·å–æ‰€æœ‰å±æ€§å­—æ®µ
            dt_elements = soup.select('.basic-info dt')
            for dt in dt_elements:
                key = dt.text.strip()
                if key not in ['ä¸­æ–‡å', 'é¢†åŸŸ', 'åˆ†ç±»', 'ç›¸å…³ä¸»é¢˜']:
                    # è·å–å¯¹åº”çš„å€¼
                    dd = dt.find_next('dd')
                    if dd:
                        value = dd.text.strip()
                        extra_properties[key] = value

            # æ„å»ºçŸ¥è¯†ç‚¹å®ä½“
            entity = {
                'type': 'entity',
                'name': title,
                'label': 'KnowledgePoint',
                'source': 'baidu',
                'confidence': 0.95,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'properties': {
                    'description': summary[:300] + '...' if len(summary) > 300 else summary,
                    'category': category,
                    'url': f"https://baike.baidu.com/item/{urllib.parse.quote(topic)}",
                    'related_topics': related_topics,
                    **extra_properties  # æ·»åŠ é¢å¤–å±æ€§
                }
            }

            # æ·»åŠ çŸ¥è¯†ç‚¹çš„å”¯ä¸€æ ‡è¯†ç¬¦
            entity['id'] = str(uuid.uuid4())

            results.append(entity)

            print(f"   æˆåŠŸè§£æçŸ¥è¯†ç‚¹: {title}ï¼Œæ‰¾åˆ° {len(related_topics)} ä¸ªç›¸å…³ä¸»é¢˜å’Œ {len(extra_properties)} ä¸ªé¢å¤–å±æ€§")

            # åˆ›å»ºç›¸å…³ä¸»é¢˜çš„å®ä½“å’Œå…³ç³»
            for related_topic in related_topics:
                # æ’é™¤éæ³•å€¼
                if not related_topic or related_topic == title:
                    continue

                # åˆ›å»ºç›¸å…³ä¸»é¢˜çš„å®ä½“
                related_entity = {
                    'type': 'entity',
                    'name': related_topic,
                    'label': 'KnowledgePoint',
                    'source': 'baidu_related',
                    'confidence': 0.8,
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'id': str(uuid.uuid4()),
                    'properties': {
                        'category': category,
                        'description': f"ä¸{title}ç›¸å…³çš„çŸ¥è¯†ç‚¹"
                    }
                }

                # åˆ›å»ºå…³ç³»
                relation = {
                            'type': 'relation',
                    'source': title,
                    'target': related_topic,
                            'relation_type': 'RELATED_TO',
                    'source_type': 'baidu',
                            'confidence': 0.85,
                            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                }
                
                results.append(related_entity)
                results.append(relation)

            return results

        except Exception as e:
            print(f"âŒ è§£æç™¾åº¦ç™¾ç§‘çŸ¥è¯†ç‚¹æ•°æ®å‡ºé”™: {str(e)}")
            # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„ç»“æœ
            entity = {
                'type': 'entity',
                'name': topic,
                'label': 'KnowledgePoint',  # ç¡®ä¿æ ‡ç­¾æ­£ç¡®
                'source': 'baidu',
                'confidence': 0.8,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'properties': {
                    'description': f"è®¡ç®—æœºé¢†åŸŸçš„{topic}æ¦‚å¿µ",
                    'category': 'è®¡ç®—æœºç§‘å­¦',
                    'url': f"https://baike.baidu.com/item/{urllib.parse.quote(topic)}",
                    'related_topics': []
                }
            }
            return [entity]


class BigDataProcessor:
    """å¤§æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        if not HAS_SPARK:
            print("è­¦å‘Š: Sparkæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•æ•°æ®å¤„ç†æ›¿ä»£")
            self.spark = None
            return

        try:
            # è®¾ç½®Pythonç¯å¢ƒå˜é‡
            os.environ['PYSPARK_PYTHON'] = sys.executable
            os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
            
            # è®¾ç½®Hadoopç¯å¢ƒå˜é‡
            os.environ['HADOOP_HOME'] = 'C:\\hadoop'
            os.environ['PATH'] = f"{os.environ['PATH']};C:\\hadoop\\bin"
            
            # è®¾ç½®Sparkç¯å¢ƒå˜é‡
            os.environ['SPARK_LOCAL_DIRS'] = 'C:\\hadoop\\tmp'
            
            # åˆå§‹åŒ–Sparkä¼šè¯
            self.spark = SparkSession.builder \
                .appName("KnowledgeGraphProcessor") \
                .config("spark.executor.memory", "4g") \
                .config("spark.driver.memory", "2g") \
                .config("spark.sql.warehouse.dir", "C:\\hadoop\\tmp") \
                .config("spark.local.dir", "C:\\hadoop\\tmp") \
                .config("spark.driver.extraJavaOptions", "-Dfile.encoding=UTF-8") \
                .config("spark.executor.extraJavaOptions", "-Dfile.encoding=UTF-8") \
                .config("spark.python.worker.reuse", "true") \
                .config("spark.python.use.daemon", "true") \
                .config("spark.python.worker.memory", "512m") \
                .config("spark.eventLog.gcMetrics.youngGenerationGarbageCollectors", "G1 Young Generation") \
                .config("spark.eventLog.gcMetrics.oldGenerationGarbageCollectors", "G1 Old Generation") \
                .getOrCreate()
                
            # è®¾ç½®æ—¥å¿—çº§åˆ«
            self.spark.sparkContext.setLogLevel("ERROR")
            
            # é…ç½®æ‰¹å¤„ç†å‚æ•°
            self.batch_size = 1000
            
        except Exception as e:
            print(f"åˆå§‹åŒ–Sparkä¼šè¯é”™è¯¯: {str(e)}")
            self.spark = None
            
    async def process_large_dataset(self, data):
        """å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†"""
        if not data:
            print("è­¦å‘Šï¼šè¾“å…¥æ•°æ®é›†ä¸ºç©º")
            return []
            
        # å¦‚æœSparkä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•å¤„ç†
        if not HAS_SPARK or not self.spark:
            print("ä½¿ç”¨ç®€å•æ•°æ®å¤„ç†...")
            return self._simple_process(data)
            
        try:
            # å°†æ•°æ®è½¬æ¢ä¸ºSpark DataFrame
            df = self.spark.createDataFrame(data)
            
            # æ•°æ®é¢„å¤„ç†
            df = self._preprocess_data(df)
            
            # æ‰¹é‡å¤„ç†
            results = []
            for batch in self._split_into_batches(df):
                processed_batch = await self._process_batch(batch)
                results.extend(processed_batch)
                
            return results
        except Exception as e:
            print(f"å¤„ç†æ•°æ®é›†é”™è¯¯: {str(e)}")
            return self._simple_process(data)
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup_temp_files()
    
    def _simple_process(self, data):
        """ç®€å•æ•°æ®å¤„ç†æ–¹æ³•ï¼Œå½“Sparkä¸å¯ç”¨æ—¶ä½¿ç”¨"""
        print("æ­£åœ¨è¿›è¡Œç®€å•æ•°æ®å¤„ç†...")
        return data

    def _preprocess_data(self, df):
        """æ•°æ®é¢„å¤„ç†"""
        try:
            # åˆ é™¤ç©ºå€¼
            df = df.na.drop()
            
            # æ–‡æœ¬æ ‡å‡†åŒ–
            tokenizer = Tokenizer(inputCol="text", outputCol="words")
            remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
            
            pipeline = Pipeline(stages=[tokenizer, remover])
            df = pipeline.fit(df).transform(df)
            
            return df
        except Exception as e:
            print(f"æ•°æ®é¢„å¤„ç†é”™è¯¯: {str(e)}")
            return df
        
    def _split_into_batches(self, df):
        """å°†æ•°æ®åˆ†å‰²æˆæ‰¹æ¬¡"""
        try:
            # ä½¿ç”¨repartitionæ›¿ä»£randomSplitä»¥æé«˜æ€§èƒ½
            num_partitions = max(1, df.count() // self.batch_size)
            return df.repartition(num_partitions).rdd.mapPartitions(lambda x: [list(x)]).collect()
        except Exception as e:
            print(f"æ•°æ®åˆ†ç‰‡é”™è¯¯: {str(e)}")
            return [df]
        
    async def _process_batch(self, batch_df):
        """å¤„ç†å•ä¸ªæ•°æ®æ‰¹æ¬¡"""
        try:
            # è½¬æ¢ä¸ºPandas DataFrameè¿›è¡Œå¤„ç†
            batch_pd = batch_df.toPandas()
            
            # å¹¶è¡Œå¤„ç†æ¯ä¸ªæ–‡æ¡£
            tasks = []
            for _, row in batch_pd.iterrows():
                task = asyncio.create_task(self._process_document(row))
                tasks.append(task)

            results = await asyncio.gather(*tasks)
            return [r for r in results if r is not None]
        except Exception as e:
            print(f"æ‰¹å¤„ç†é”™è¯¯: {str(e)}")
            return []
        
    async def _process_document(self, document):
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        try:
            return document.to_dict()
        except Exception as e:
            print(f"æ–‡æ¡£å¤„ç†é”™è¯¯: {str(e)}")
            return None

    def _cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            temp_dir = "C:\\hadoop\\tmp"
            if os.path.exists(temp_dir):
                for file in os.listdir(temp_dir):
                    try:
                        os.remove(os.path.join(temp_dir, file))
                    except Exception as e:
                        print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶é”™è¯¯: {str(e)}")
        except Exception as e:
            print(f"æ¸…ç†ä¸´æ—¶ç›®å½•é”™è¯¯: {str(e)}")
            
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        try:
            if hasattr(self, 'spark'):
                self.spark.stop()
        except Exception as e:
            print(f"åœæ­¢Sparkä¼šè¯é”™è¯¯: {str(e)}")


class DeepSeekExtractor:
    """ä½¿ç”¨DeepSeek APIçš„å®ä½“å’Œå…³ç³»æå–å™¨"""
    
    def __init__(self):
        if not HAS_OPENAI:
            print("è­¦å‘Šï¼šOpenAIåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•å®ä½“æå–æ›¿ä»£")
            self.client = None
            return

        try:
            self.client = OpenAI(
                api_key=os.getenv("DEEPSEEK_API_KEY"),
                base_url="https://api.deepseek.com/v1"
            )
        except Exception as e:
            print(f"åˆå§‹åŒ–DeepSeek APIå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            self.client = None
        
        # å®ä½“æå–æç¤ºæ¨¡æ¿
        self.entity_prompt_template = """
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æ‰€æœ‰è®¡ç®—æœºç§‘å­¦ç›¸å…³å®ä½“ã€‚
ä»…è¿”å›JSONæ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–è§£é‡Šæ–‡å­—ï¼š

        {
            "entities": [
        {"name": "å®ä½“åç§°", "type": "å®ä½“ç±»å‹", "confidence": 0.9}
            ]
        }
        
        æ–‡æœ¬å†…å®¹ï¼š
        {text}
        """
        
        # å…³ç³»æå–æç¤ºæ¨¡æ¿
        self.relation_prompt_template = """
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“ä¹‹é—´çš„å…³ç³»ã€‚
ä»…è¿”å›JSONæ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–è§£é‡Šæ–‡å­—ï¼š

        {
            "relations": [
        {"source": "æºå®ä½“", "target": "ç›®æ ‡å®ä½“", "type": "å…³ç³»ç±»å‹", "confidence": 0.9}
            ]
        }
        
        æ–‡æœ¬å†…å®¹ï¼š
        {text}
        
        å·²çŸ¥å®ä½“ï¼š
        {entities}
        """
        
    async def extract_entities(self, text):
        """ä½¿ç”¨DeepSeekæå–å®ä½“"""
        # å¦‚æœOpenAIåº“ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•æå–
        if not HAS_OPENAI or not self.client:
            return self._simple_entity_extraction(text)
            
        try:
            # æ„å»ºæç¤º
            prompt = self.entity_prompt_template.format(text=text)
            
            # è°ƒç”¨DeepSeek API
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®ä½“æå–åŠ©æ‰‹ï¼Œæ“…é•¿ä»æ–‡æœ¬ä¸­è¯†åˆ«å’Œæå–å®ä½“ã€‚å§‹ç»ˆä»¥çº¯JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸åŒ…å«ä»»ä½•è¯´æ˜æ–‡å­—ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,  # é™ä½æ¸©åº¦ï¼Œè·å¾—æ›´å¯é¢„æµ‹çš„è¾“å‡º
                max_tokens=1000
            )
            
            # è·å–å“åº”æ–‡æœ¬
            content = response.choices[0].message.content
            
            # å°è¯•è§£æJSON
            try:
                # æ¸…ç†å†…å®¹ä¸­å¯èƒ½çš„éJSONéƒ¨åˆ†
                json_str = content
                # å°è¯•æ‰¾åˆ°JSONçš„èµ·å§‹å’Œç»“æŸä½ç½®
                start_idx = content.find('{')
                end_idx = content.rfind('}') + 1
                if start_idx >= 0 and end_idx > start_idx:
                    json_str = content[start_idx:end_idx]
                
                result = json.loads(json_str)
                entities = result.get("entities", [])
                print(f"âœ… æˆåŠŸä»DeepSeek APIè·å– {len(entities)} ä¸ªå®ä½“")
                return entities
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æé”™è¯¯: {str(e)}")
                print(f"åŸå§‹å“åº”(æˆªå–): {content[:100]}...")
                return self._simple_entity_extraction(text)
            
        except Exception as e:
            print(f"å®ä½“æå–é”™è¯¯: {str(e)}")
            return self._simple_entity_extraction(text)
            
    async def extract_relations(self, text, entities):
        """ä½¿ç”¨DeepSeekæå–å…³ç³»"""
        # å¦‚æœOpenAIåº“ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•æå–
        if not HAS_OPENAI or not self.client:
            return self._simple_relation_extraction(text, entities)
            
        try:
            # æ„å»ºæç¤º
            entities_str = json.dumps(entities, ensure_ascii=False, indent=2)
            prompt = self.relation_prompt_template.format(
                text=text,
                entities=entities_str
            )
            
            # è°ƒç”¨DeepSeek API
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…³ç³»æå–åŠ©æ‰‹ï¼Œæ“…é•¿ä»æ–‡æœ¬ä¸­è¯†åˆ«å®ä½“ä¹‹é—´çš„å…³ç³»ã€‚å§‹ç»ˆä»¥çº¯JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸åŒ…å«ä»»ä½•è¯´æ˜æ–‡å­—ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,  # é™ä½æ¸©åº¦ï¼Œè·å¾—æ›´å¯é¢„æµ‹çš„è¾“å‡º
                max_tokens=1000
            )
            
            # è·å–å“åº”æ–‡æœ¬
            content = response.choices[0].message.content
            
            # å°è¯•è§£æJSON
            try:
                # æ¸…ç†å†…å®¹ä¸­å¯èƒ½çš„éJSONéƒ¨åˆ†
                json_str = content
                # å°è¯•æ‰¾åˆ°JSONçš„èµ·å§‹å’Œç»“æŸä½ç½®
                start_idx = content.find('{')
                end_idx = content.rfind('}') + 1
                if start_idx >= 0 and end_idx > start_idx:
                    json_str = content[start_idx:end_idx]
                
                result = json.loads(json_str)
                relations = result.get("relations", [])
                print(f"âœ… æˆåŠŸä»DeepSeek APIè·å– {len(relations)} ä¸ªå…³ç³»")
                return relations
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æé”™è¯¯: {str(e)}")
                print(f"åŸå§‹å“åº”(æˆªå–): {content[:100]}...")
                return self._simple_relation_extraction(text, entities)
            
        except Exception as e:
            print(f"å…³ç³»æå–é”™è¯¯: {str(e)}")
            return self._simple_relation_extraction(text, entities)
            
    def _simple_entity_extraction(self, text):
        """ç®€å•å®ä½“æå–ï¼Œå½“DeepSeek APIä¸å¯ç”¨æ—¶ä½¿ç”¨"""
        print("ä½¿ç”¨ç®€å•å®ä½“æå–...")
        entities = []
        
        # æå–å¯èƒ½çš„å®ä½“ï¼ˆè¿™é‡Œä»…åšç¤ºä¾‹ï¼‰
        words = text.split()
        for word in words:
            if len(word) > 3 and word[0].isupper():
                entities.append({
                    "name": word,
                    "type": "æ¦‚å¿µ",
                    "confidence": 0.7
                })
                
        return entities
        
    def _simple_relation_extraction(self, text, entities):
        """ç®€å•å…³ç³»æå–ï¼Œå½“DeepSeek APIä¸å¯ç”¨æ—¶ä½¿ç”¨"""
        print("ä½¿ç”¨ç®€å•å…³ç³»æå–...")
        relations = []
        
        # å¦‚æœå®ä½“ä¸è¶³ï¼Œæ— æ³•å»ºç«‹å…³ç³»
        if len(entities) < 2:
            return relations
            
        # ç®€å•åœ°å°†ç›¸é‚»å®ä½“è¿æ¥ï¼ˆè¿™é‡Œä»…åšç¤ºä¾‹ï¼‰
        for i in range(len(entities) - 1):
            relations.append({
                "source": entities[i]["name"],
                "target": entities[i + 1]["name"],
                "type": "ç›¸å…³",
                "confidence": 0.6
            })
                
        return relations


class KnowledgeFusion:
    """çŸ¥è¯†èåˆå™¨"""
    
    def __init__(self):
        self.entity_aligner = EntityAligner()
        self.relation_resolver = RelationResolver()
        self.knowledge_reasoner = KnowledgeReasoner()
        
    async def fuse_knowledge(self, updates):
        """èåˆçŸ¥è¯†"""
        # å®ä½“å¯¹é½
        aligned_entities = await self.entity_aligner.align(updates['entities'])
        
        # å…³ç³»å†²çªè§£å†³
        resolved_relations = await self.relation_resolver.resolve(updates['relations'])
        
        # çŸ¥è¯†æ¨ç†
        inferred_knowledge = await self.knowledge_reasoner.infer(aligned_entities, resolved_relations)
        
        return {
            'entities': aligned_entities,
            'relations': resolved_relations,
            'inferred': inferred_knowledge
        }


class EntityAligner:
    """å®ä½“å¯¹é½å™¨"""
    
    def __init__(self):
        self.nlp_processor = NLPProcessor()
        self.threshold = 0.7  # ç›¸ä¼¼åº¦é˜ˆå€¼
        
    def align_entities(self, entities):
        """å¯¹é½å®ä½“"""
        print(f"å¼€å§‹å¯¹é½ {len(entities)} ä¸ªå®ä½“...")

        if not entities:
            print("è­¦å‘Š: æ²¡æœ‰å®ä½“å¯ä¾›å¯¹é½")
            return []
            
        try:
        # åˆå§‹åŒ–å¯¹é½ç»“æœ
            aligned_entities = entities.copy()
        
            # ç­›é€‰æœ‰æ•ˆå®ä½“ï¼ˆå…·æœ‰nameå±æ€§çš„ï¼‰
            valid_entities = [entity for entity in entities if entity.get('name')]
            print(f"æ‰¾åˆ° {len(valid_entities)} ä¸ªæœ‰æ•ˆå®ä½“")

            if not valid_entities:
                print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆå®ä½“")
                return entities  # è¿”å›åŸå§‹å®ä½“

            # å¯¹å®ä½“è¿›è¡Œç´¢å¼•
            entity_index = {}
            for i, entity in enumerate(valid_entities):
                entity_name = entity.get('name', '')
                if entity_name:
                    entity_index[entity_name] = i
                
            print(f"å»ºç«‹äº† {len(entity_index)} ä¸ªå®ä½“ç´¢å¼•")

            # æŸ¥æ‰¾ç›¸ä¼¼å®ä½“å¹¶åˆå¹¶
            merges_performed = 0

            for i, entity in enumerate(valid_entities):
                entity_name = entity.get('name', '')
                entity_desc = entity.get('properties', {}).get('description', '')
            
                for j, other in enumerate(valid_entities):
                    if i == j:
                        continue
                    
                other_name = other.get('name', '')
                other_desc = other.get('properties', {}).get('description', '')
                
                # è®¡ç®—åç§°ç›¸ä¼¼åº¦
                name_similarity = self.nlp_processor.calculate_similarity(entity_name, other_name)
                
                # è®¡ç®—æè¿°ç›¸ä¼¼åº¦
                desc_similarity = self.nlp_processor.calculate_similarity(entity_desc,
                                                                              other_desc) if entity_desc and other_desc else 0
                
                # ç»¼åˆç›¸ä¼¼åº¦
                similarity = name_similarity * 0.7 + desc_similarity * 0.3
                
                # å¦‚æœç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼ï¼Œåˆå¹¶å®ä½“
                if similarity > self.threshold:
                        # æ‰¾åˆ°å¯¹åº”çš„ç´¢å¼•
                        entity_idx = entity_index.get(entity_name)
                        other_idx = entity_index.get(other_name)

                        if entity_idx is not None and entity_idx < len(aligned_entities):
                    # åˆå¹¶å±æ€§
                            other_properties = other.get('properties', {})
                            entity_properties = aligned_entities[entity_idx].get('properties', {})

                            if not entity_properties:
                                aligned_entities[entity_idx]['properties'] = {}

                            # åˆå¹¶å±æ€§
                            merged_properties = {**entity_properties, **other_properties}
                            aligned_entities[entity_idx]['properties'] = merged_properties
                    
                    # æ ‡è®°å®ä½“å…³ç³»
                            if 'related_entities' not in aligned_entities[entity_idx]:
                                aligned_entities[entity_idx]['related_entities'] = []
                    
                            if other_name not in aligned_entities[entity_idx]['related_entities']:
                                aligned_entities[entity_idx]['related_entities'].append(other_name)
                                merges_performed += 1

            print(f"å®Œæˆå®ä½“å¯¹é½ï¼Œæ‰§è¡Œäº† {merges_performed} æ¬¡åˆå¹¶")
            return aligned_entities
        except Exception as e:
            print(f"âŒ å®ä½“å¯¹é½é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            # è¿”å›åŸå§‹å®ä½“
            return entities


class RelationResolver:
    """å…³ç³»è§£æå™¨"""
    
    def __init__(self):
        self.relation_types = {
            'CONTAINS': 'åŒ…å«',
            'RELATED_TO': 'ç›¸å…³äº',
            'PREREQUISITE': 'å…ˆä¿®äº',
            'PREREQUISITE_FOR': 'åç»­',
            'SIMILAR_TO': 'ç›¸ä¼¼äº'
        }
        
        # åˆ›å»ºåå‘æ˜ å°„
        self.chinese_to_relation_types = {v: k for k, v in self.relation_types.items()}
        
    def resolve_relations(self, entities):
        """è§£æå®ä½“é—´å…³ç³»"""
        print(f"å¼€å§‹è§£æ {len(entities)} ä¸ªå®ä½“çš„å…³ç³»...")

        if not entities:
            print("è­¦å‘Š: æ²¡æœ‰å®ä½“ç”¨äºå…³ç³»è§£æ")
            return []
            
        try:
            # åˆå§‹åŒ–ç»“æœ
            result = []
            # å¤åˆ¶å®ä½“
            for entity in entities:
                result.append(entity)
            
            # å¤„ç†è¯¾ç¨‹å’ŒçŸ¥è¯†ç‚¹å…³ç³»
            course_entities = [entity for entity in entities if entity.get('label') == 'Course']
            knowledge_entities = [entity for entity in entities if entity.get('label') == 'KnowledgePoint']

            print(f"æ‰¾åˆ° {len(course_entities)} ä¸ªè¯¾ç¨‹å®ä½“å’Œ {len(knowledge_entities)} ä¸ªçŸ¥è¯†ç‚¹å®ä½“")

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å®ä½“ï¼Œè¿”å›åŸå§‹å®ä½“åˆ—è¡¨
            if not course_entities and not knowledge_entities:
                print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¯¾ç¨‹æˆ–çŸ¥è¯†ç‚¹å®ä½“")
                return entities

            relations_added = 0
            
            # åˆ›å»ºè¯¾ç¨‹åç§°åˆ°å®ä½“çš„æ˜ å°„ï¼Œç”¨äºæ£€æŸ¥å…³ç³»
            course_name_to_entity = {entity.get('name', ''): entity for entity in course_entities if entity.get('name')}
            knowledge_name_to_entity = {entity.get('name', ''): entity for entity in knowledge_entities if entity.get('name')}
            
            # æ„å»ºè¯¾ç¨‹ä¸çŸ¥è¯†ç‚¹çš„åŒ…å«å…³ç³»
            for course in course_entities:
                course_name = course.get('name', '')
                if not course_name:
                    continue

                course_desc = course.get('properties', {}).get('description', '')
                course_topics = course.get('properties', {}).get('topics', [])
                
                for knowledge in knowledge_entities:
                    knowledge_name = knowledge.get('name', '')
                    if not knowledge_name:
                        continue
                    
                    # å¦‚æœçŸ¥è¯†ç‚¹åç§°åœ¨è¯¾ç¨‹ä¸»é¢˜ä¸­ï¼Œåˆ›å»ºåŒ…å«å…³ç³»
                    if knowledge_name in course_topics or (course_desc and knowledge_name.lower() in course_desc.lower()):
                        relation = {
                            'type': 'relation',
                            'source': course_name,
                            'target': knowledge_name,
                            'relation_type': 'CONTAINS',
                            'source_type': 'derived',
                            'confidence': 0.8,
                            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                        }
                        result.append(relation)
                        relations_added += 1

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å…³ç³»ï¼Œä¸ºæ¯ä¸ªè¯¾ç¨‹ç”Ÿæˆä¸€äº›éšæœºå…³ç³»
            if relations_added == 0 and knowledge_entities and course_entities:
                print("æœªæ‰¾åˆ°ç°æœ‰å…³ç³»ï¼Œåˆ›å»ºé»˜è®¤å…³ç³»...")

                # ä¸ºæ¯ä¸ªè¯¾ç¨‹åˆ›å»ºè‡³å°‘ä¸€ä¸ªå…³ç³»
                for course in course_entities:
                    course_name = course.get('name', '')
                    if not course_name:
                        continue

                    # é€‰æ‹©éšæœºçŸ¥è¯†ç‚¹
                    if knowledge_entities:
                        knowledge = random.choice(knowledge_entities)
                        knowledge_name = knowledge.get('name', '')

                        if knowledge_name:
                            relation = {
                                'type': 'relation',
                                'source': course_name,
                                'target': knowledge_name,
                                'relation_type': 'CONTAINS',
                                'source_type': 'default',
                                'confidence': 0.6,
                                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                            }
                            result.append(relation)
                            relations_added += 1

            # å¦‚æœä»ç„¶æ²¡æœ‰å…³ç³»ï¼Œä½¿ç”¨DEFAULT_RELATIONS
            if relations_added == 0:
                print("è­¦å‘Š: æœªèƒ½ç”Ÿæˆä»»ä½•å…³ç³»ï¼Œä½¿ç”¨é»˜è®¤å…³ç³»")
                for relation in DEFAULT_RELATIONS:
                    result.append(relation)
                    relations_added += len(DEFAULT_RELATIONS)

            print(f"å…³ç³»è§£æå®Œæˆï¼Œæ·»åŠ äº† {relations_added} ä¸ªè¯¾ç¨‹-çŸ¥è¯†ç‚¹å…³ç³»")
            return result

        except Exception as e:
            print(f"âŒ å…³ç³»è§£æé”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            # è¿”å›åŸå§‹å®ä½“
            return entities


class KnowledgeReasoner:
    """çŸ¥è¯†æ¨ç†å™¨"""
    
    def __init__(self):
        self.MAX_PATH_LENGTH = 3  # æœ€å¤§è·¯å¾„é•¿åº¦
        
    def enrich_knowledge(self, data):
        """æ‰©å±•çŸ¥è¯†"""
        print(f"å¼€å§‹åŸºäº {len(data)} æ¡æ•°æ®è¿›è¡ŒçŸ¥è¯†æ¨ç†...")

        if not data:
            print("è­¦å‘Š: æ²¡æœ‰æ•°æ®å¯ç”¨äºçŸ¥è¯†æ¨ç†")
            return []
            
        try:
            # å¤åˆ¶åŸå§‹æ•°æ®
            enriched_data = data.copy()
            
            # æå–å®ä½“å’Œå…³ç³»
            entities = [item for item in data if item.get('type') == 'entity']
            relations = [item for item in data if item.get('type') == 'relation']
            
            print(f"æ‰¾åˆ° {len(entities)} ä¸ªå®ä½“å’Œ {len(relations)} ä¸ªå…³ç³»ç”¨äºæ¨ç†")

            # å¦‚æœå®ä½“æˆ–å…³ç³»å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆæ¨ç†
            if len(entities) < 2 or len(relations) < 1:
                print("è­¦å‘Š: å®ä½“æˆ–å…³ç³»æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡ŒçŸ¥è¯†æ¨ç†")
                return data

            # æ„å»ºçŸ¥è¯†å›¾è°±
            graph = self._build_graph(entities, relations)
            
            # æ¨æ–­æ–°å…³ç³»
            new_relations = self._infer_new_relations(graph, entities, relations)
            
            # æ·»åŠ æ–°çš„æ¨æ–­å…³ç³»
            if new_relations:
                print(f"æ¨ç†å‡º {len(new_relations)} ä¸ªæ–°å…³ç³»")
                for relation in new_relations:
                    enriched_data.append(relation)
            else:
                print("æœªèƒ½æ¨ç†å‡ºæ–°å…³ç³»")
            
            return enriched_data
            
        except Exception as e:
            print(f"âŒ çŸ¥è¯†æ¨ç†é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return data
        
    def _build_graph(self, entities, relations):
        """æ„å»ºå›¾ç»“æ„"""
        try:
            graph = {}
            
            # åˆå§‹åŒ–å›¾ç»“æ„
            for entity in entities:
                entity_name = entity.get('name', '')
                if entity_name:
                    graph[entity_name] = []
                    
            # æ·»åŠ å…³ç³»
            for relation in relations:
                source = relation.get('source', '')
                target = relation.get('target', '')
                relation_type = relation.get('relation_type', '')
                
                if source in graph and target in graph:
                    graph[source].append((target, relation_type))
                    
            return graph
            
        except Exception as e:
            print(f"âŒ æ„å»ºå›¾ç»“æ„é”™è¯¯: {str(e)}")
            return {}
        
    def _infer_new_relations(self, graph, entities, relations):
        """æ¨æ–­æ–°å…³ç³»"""
        if not graph:
            return []

        try:
            new_relations = []
            existing_relations = set()

            # é¦–å…ˆåˆ›å»ºç°æœ‰å…³ç³»çš„é›†åˆï¼Œé¿å…é‡å¤
            for relation in relations:
                source = relation.get('source', '')
                target = relation.get('target', '')
                relation_type = relation.get('relation_type', '')

                if source and target and relation_type:
                    relation_key = f"{source}|{relation_type}|{target}"
                    existing_relations.add(relation_key)
            
            # æ¨æ–­ä¼ é€’å…³ç³»
            for entity1 in graph:
                for entity2, relation_type1 in graph.get(entity1, []):
                    for entity3, relation_type2 in graph.get(entity2, []):
                        # åªå¤„ç†ç›¸åŒç±»å‹çš„å…³ç³»ï¼Œæ¯”å¦‚PREREQUISITE
                        if relation_type1 == relation_type2 == 'PREREQUISITE' and entity1 != entity3:
                            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥å…³ç³»
                            relation_key = f"{entity1}|PREREQUISITE|{entity3}"
                            if relation_key not in existing_relations:
                                new_relation = {
                                    'type': 'relation',
                                    'source': entity1,
                                    'target': entity3,
                                    'relation_type': 'PREREQUISITE',
                                    'source_type': 'inferred',
                                    'confidence': 0.6,
                                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                                }
                                new_relations.append(new_relation)
                                existing_relations.add(relation_key)  # é¿å…é‡å¤æ·»åŠ 

            # å°è¯•æ¨æ–­ç›¸å…³å…³ç³»
            if len(new_relations) == 0:
                print("å°è¯•æ¨æ–­æ›´å¤šç±»å‹çš„å…³ç³»...")

                # åˆ›å»ºå®ä½“åç§°åˆ°æ ‡ç­¾çš„æ˜ å°„ï¼Œç”¨äºæ£€æŸ¥å®ä½“ç±»å‹
                entity_name_to_label = {}
                for entity in entities:
                    if 'name' in entity:
                        entity_name_to_label[entity['name']] = entity.get('label', '')

                # å¦‚æœä¸¤ä¸ªçŸ¥è¯†ç‚¹éƒ½ä¸åŒä¸€ä¸ªè¯¾ç¨‹ç›¸å…³ï¼Œå®ƒä»¬å¯èƒ½ç›¸å…³
                course_to_knowledge = {}

                # æ„å»ºè¯¾ç¨‹åˆ°çŸ¥è¯†ç‚¹çš„æ˜ å°„ï¼Œç¡®ä¿åªåŒ…å«è¯¾ç¨‹åˆ°çŸ¥è¯†ç‚¹çš„CONTAINSå…³ç³»
                for relation in relations:
                    if relation.get('relation_type') == 'CONTAINS':
                        course = relation.get('source', '')
                        knowledge = relation.get('target', '')
                        
                        # æ£€æŸ¥sourceæ˜¯è¯¾ç¨‹ï¼Œtargetæ˜¯çŸ¥è¯†ç‚¹
                        is_course_to_knowledge = (entity_name_to_label.get(course) == 'Course' and 
                                                 entity_name_to_label.get(knowledge) == 'KnowledgePoint')
                        
                        # å¦‚æœä¸æ˜¯è¯¾ç¨‹åˆ°çŸ¥è¯†ç‚¹çš„å…³ç³»ï¼Œåˆ™è·³è¿‡
                        if not is_course_to_knowledge:
                            continue

                        if course and knowledge:
                            if course not in course_to_knowledge:
                                course_to_knowledge[course] = []
                            course_to_knowledge[course].append(knowledge)

                # å¯»æ‰¾å…±åŒè¯¾ç¨‹çš„çŸ¥è¯†ç‚¹
                for course, knowledge_points in course_to_knowledge.items():
                    if len(knowledge_points) >= 2:
                        for i in range(len(knowledge_points)):
                            for j in range(i + 1, len(knowledge_points)):
                                k1 = knowledge_points[i]
                                k2 = knowledge_points[j]

                                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å…³ç³»
                                rel_key1 = f"{k1}|RELATED_TO|{k2}"
                                rel_key2 = f"{k2}|RELATED_TO|{k1}"

                                if rel_key1 not in existing_relations and rel_key2 not in existing_relations:
                                    # åªä¸ºå°‘æ•°å…³ç³»åˆ›å»ºRELATED_TOå…³ç³»ï¼Œé¿å…è¿‡å¤šç”Ÿæˆ
                                    # ä½¿ç”¨éšæœºå‡½æ•°é™åˆ¶RELATED_TOå…³ç³»çš„åˆ›å»º - å‡å°‘90%
                                    if random.random() > 0.9:
                                        # ä¼˜å…ˆä½¿ç”¨CONTAINSå…³ç³»ï¼ˆå¦‚æœå…¶ä¸­ä¸€ä¸ªçŸ¥è¯†ç‚¹ååŒ…å«å¦ä¸€ä¸ªï¼‰
                                        relation_type = 'RELATED_TO'
                                        if k1.lower() in k2.lower():
                                            # k2åŒ…å«k1ï¼Œåˆ™k2å¯èƒ½åŒ…å«k1
                                            relation_type = 'CONTAINS'
                                            source, target = k2, k1
                                        elif k2.lower() in k1.lower():
                                            # k1åŒ…å«k2ï¼Œåˆ™k1å¯èƒ½åŒ…å«k2
                                            relation_type = 'CONTAINS'
                                            source, target = k1, k2
                                        else:
                                            # ç¡®å®šå“ªä¸ªæ˜¯source, å“ªä¸ªæ˜¯target (ä¿æŒk1->k2æ–¹å‘)
                                            source, target = k1, k2
                                        
                                        new_relation = {
                                            'type': 'relation',
                                            'source': source,
                                            'target': target,
                                            'relation_type': relation_type,
                                            'source_type': 'inferred',
                                            'confidence': 0.5,
                                            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                                        }
                                        new_relations.append(new_relation)
                                        if relation_type == 'CONTAINS':
                                            existing_relations.add(f"{source}|CONTAINS|{target}")
                                        else:
                                            existing_relations.add(rel_key1)

            return new_relations
            
        except Exception as e:
            print(f"âŒ æ¨æ–­æ–°å…³ç³»é”™è¯¯: {str(e)}")
            return []
        
    def _relation_exists(self, relations, source, target, relation_type):
        """æ£€æŸ¥å…³ç³»æ˜¯å¦å·²å­˜åœ¨"""
        for relation in relations:
            if (relation.get('source') == source and 
                relation.get('target') == target and 
                relation.get('relation_type') == relation_type):
                return True
        return False


class QualityEvaluator:
    """è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics = {
            'completeness': 0.3,  # å®Œæ•´æ€§æƒé‡
            'accuracy': 0.3,  # å‡†ç¡®æ€§æƒé‡
            'coherence': 0.2,  # ä¸€è‡´æ€§æƒé‡
            'relevance': 0.2  # ç›¸å…³æ€§æƒé‡
        }
        
    def evaluate_quality(self, data):
        """è¯„ä¼°è´¨é‡å¹¶è¿‡æ»¤æ•°æ®"""
        if not data:
            return []
            
        # æå–å®ä½“å’Œå…³ç³»
        entities = [item for item in data if item.get('type') == 'entity']
        relations = [item for item in data if item.get('type') == 'relation']
        
        # è¯„ä¼°å„ä¸ªæŒ‡æ ‡
        completeness_score = self._evaluate_completeness(entities, relations)
        accuracy_score = self._evaluate_accuracy(entities, relations)
        coherence_score = self._evaluate_coherence(entities, relations)
        relevance_score = self._evaluate_relevance(entities, relations)
        
        # è®¡ç®—æ€»ä½“å¾—åˆ†
        overall_score = (
            completeness_score * self.metrics['completeness'] +
            accuracy_score * self.metrics['accuracy'] +
            coherence_score * self.metrics['coherence'] +
            relevance_score * self.metrics['relevance']
        )
        
        # æ ¹æ®å¾—åˆ†è¿‡æ»¤æ•°æ®
        filtered_data = self._filter_data(data, overall_score)
        
        return filtered_data
        
    def _evaluate_completeness(self, entities, relations):
        """è¯„ä¼°å®Œæ•´æ€§"""
        # è®¡ç®—æœ‰æè¿°çš„å®ä½“æ¯”ä¾‹
        entities_with_description = 0
        for entity in entities:
            properties = entity.get('properties', {})
            if properties and 'description' in properties and properties['description']:
                entities_with_description += 1
                
        completeness_score = entities_with_description / len(entities) if entities else 0
        
        # æ£€æŸ¥å…³ç³»çš„å®Œæ•´æ€§
        valid_relations = 0
        for relation in relations:
            if relation.get('source') and relation.get('target') and relation.get('relation_type'):
                valid_relations += 1
                
        relation_completeness = valid_relations / len(relations) if relations else 0
        
        # ç»¼åˆå¾—åˆ†
        return (completeness_score + relation_completeness) / 2
        
    def _evaluate_accuracy(self, entities, relations):
        """è¯„ä¼°å‡†ç¡®æ€§"""
        # ä½¿ç”¨ç½®ä¿¡åº¦ä½œä¸ºå‡†ç¡®æ€§æŒ‡æ ‡
        entity_confidence = sum(entity.get('confidence', 0) for entity in entities) / len(entities) if entities else 0
        relation_confidence = sum(relation.get('confidence', 0) for relation in relations) / len(
            relations) if relations else 0
        
        return (entity_confidence + relation_confidence) / 2
        
    def _evaluate_coherence(self, entities, relations):
        """è¯„ä¼°ä¸€è‡´æ€§"""
        # æ£€æŸ¥å…³ç³»åŒæ–¹æ˜¯å¦éƒ½å­˜åœ¨
        entity_names = {entity.get('name', ''): entity for entity in entities}
        valid_relations = 0
        
        for relation in relations:
            source = relation.get('source', '')
            target = relation.get('target', '')
            
            if source in entity_names and target in entity_names:
                valid_relations += 1
                
        coherence_score = valid_relations / len(relations) if relations else 0
        
        return coherence_score
        
    def _evaluate_relevance(self, entities, relations):
        """è¯„ä¼°ç›¸å…³æ€§"""
        # ç®€å•å®ç°ï¼Œå‡è®¾æ‰€æœ‰æ•°æ®éƒ½ç›¸å…³
        return 0.8
        
    def _filter_data(self, data, overall_score):
        """æ ¹æ®è´¨é‡å¾—åˆ†è¿‡æ»¤æ•°æ®"""
        # å¦‚æœæ€»ä½“å¾—åˆ†ä½äº0.5ï¼Œè¿›è¡Œä¸€äº›æ¸…ç†
        if overall_score < 0.5:
            # ç§»é™¤ä½ç½®ä¿¡åº¦çš„å®ä½“å’Œå…³ç³»
            filtered_data = []
            for item in data:
                if item.get('confidence', 0) >= 0.6:
                    filtered_data.append(item)
            return filtered_data
        
        # å¦åˆ™ä¿ç•™åŸå§‹æ•°æ®
        return data


class NLPProcessor:
    """NLPå¤„ç†å™¨"""
    
    def __init__(self):
        self.stopwords = set(
            ['çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ',
             'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'])
        
    def tokenize(self, text):
        """ç®€å•åˆ†è¯"""
        if not text:
            return []
        # ç®€å•å®ç°ï¼Œå®é™…åº”ç”¨åº”è¯¥ä½¿ç”¨ä¸“ä¸šåˆ†è¯åº“
        return [word for word in text.replace('.', ' ').replace(',', ' ').split() if word not in self.stopwords]
        
    def extract_keywords(self, text, top_k=5):
        """æå–å…³é”®è¯"""
        tokens = self.tokenize(text)
        # ç»Ÿè®¡è¯é¢‘
        word_freq = {}
        for token in tokens:
            if token in word_freq:
                word_freq[token] += 1
            else:
                word_freq[token] = 1
                
        # è¿”å›Top-Kå…³é”®è¯
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, _ in sorted_words[:top_k]]
        
    def calculate_similarity(self, text1, text2):
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        # ç®€å•çš„è¯è¢‹æ¨¡å‹ç›¸ä¼¼åº¦è®¡ç®—
        tokens1 = set(self.tokenize(text1))
        tokens2 = set(self.tokenize(text2))
        
        if not tokens1 or not tokens2:
            return 0
            
        intersection = tokens1.intersection(tokens2)
        union = tokens1.union(tokens2)
        
        return len(intersection) / len(union)


class KnowledgeGraphBuilder:
    """çŸ¥è¯†å›¾è°±æ„å»ºå™¨"""
    
    def __init__(self, use_mock_db=True, use_test_data=True):
        self.use_mock_db = use_mock_db
        self.use_test_data = use_test_data
        self.mock_db = None
        
        # åˆå§‹åŒ–å·²æ”¶é›†çŸ¥è¯†ç‚¹é›†åˆï¼Œç”¨äºè·Ÿè¸ªå¢é‡æ›´æ–°
        self.collected_knowledge_points = set()
        
        # å°è¯•ä»æ–‡ä»¶åŠ è½½ä¹‹å‰çš„æ•°æ®
        loaded_from_file = False
        if not self.use_test_data:  # å¦‚æœä¸ä½¿ç”¨æµ‹è¯•æ•°æ®ï¼Œåˆ™å°è¯•åŠ è½½çœŸå®æ•°æ®
            try:
                # å°è¯•ä»jsonæ–‡ä»¶åŠ è½½
                if os.path.exists('knowledge_graph.json'):
                    print("ä»knowledge_graph.jsonåŠ è½½ä¹‹å‰çš„æ•°æ®...")
                    with open('knowledge_graph.json', 'r', encoding='utf-8') as f:
                        self.mock_db = json.load(f)
                    loaded_from_file = True
                    
                    # åˆå§‹åŒ–å·²æ”¶é›†çš„çŸ¥è¯†ç‚¹é›†åˆ
                    for name, entity in self.mock_db.get("entities", {}).items():
                        if entity.get('label') == 'KnowledgePoint':
                            self.collected_knowledge_points.add(name)
                    
                    print(f"ä»æ–‡ä»¶åŠ è½½äº† {len(self.mock_db.get('entities', {}))} ä¸ªå®ä½“å’Œ {len(self.mock_db.get('relations', []))} ä¸ªå…³ç³»")
                    print(f"ç°æœ‰ {len(self.collected_knowledge_points)} ä¸ªå·²çŸ¥çŸ¥è¯†ç‚¹")
                elif os.path.exists('knowledge_graph.pkl'):
                    print("ä»knowledge_graph.pklåŠ è½½ä¹‹å‰çš„æ•°æ®...")
                    with open('knowledge_graph.pkl', 'rb') as f:
                        self.mock_db = pickle.load(f)
                    loaded_from_file = True
                    
                    # åˆå§‹åŒ–å·²æ”¶é›†çš„çŸ¥è¯†ç‚¹é›†åˆ
                    for name, entity in self.mock_db.get("entities", {}).items():
                        if entity.get('label') == 'KnowledgePoint':
                            self.collected_knowledge_points.add(name)
                    
                    print(f"ä»æ–‡ä»¶åŠ è½½äº† {len(self.mock_db.get('entities', {}))} ä¸ªå®ä½“å’Œ {len(self.mock_db.get('relations', []))} ä¸ªå…³ç³»")
                    print(f"ç°æœ‰ {len(self.collected_knowledge_points)} ä¸ªå·²çŸ¥çŸ¥è¯†ç‚¹")
            except Exception as e:
                print(f"ä»æ–‡ä»¶åŠ è½½æ•°æ®å¤±è´¥: {str(e)}ï¼Œå°†åˆ›å»ºæ–°çš„æ¨¡æ‹Ÿæ•°æ®åº“")
                loaded_from_file = False
        
        # å¦‚æœæ²¡æœ‰ä»æ–‡ä»¶åŠ è½½ï¼Œåˆ™åˆ›å»ºæ–°çš„æ¨¡æ‹Ÿæ•°æ®åº“
        if self.use_mock_db and not loaded_from_file:
            self.mock_db = self._create_mock_db()
            
        # åˆå§‹åŒ–æ•°æ®æºé‡‡é›†å™¨
        self.collector = DataSourceCollector()
        
        # åˆå§‹åŒ–å¤§æ•°æ®å¤„ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if HAS_SPARK:
            self.spark = SparkSession.builder \
                .appName("KnowledgeGraph") \
                .config("spark.driver.memory", "2g") \
                .getOrCreate()
        else:
            self.spark = None
            
        # åˆå§‹åŒ–NLPå¤„ç†å™¨
        self.nlp_processor = NLPProcessor()
        
        # åˆå§‹åŒ–å®ä½“å¯¹é½å™¨
        self.entity_aligner = EntityAligner()
        
        # åˆå§‹åŒ–å…³ç³»è§£æå™¨
        self.relation_resolver = RelationResolver()
        
        # åˆå§‹åŒ–çŸ¥è¯†æ¨ç†å™¨
        self.knowledge_reasoner = KnowledgeReasoner()
        
        # åˆå§‹åŒ–è´¨é‡è¯„ä¼°å™¨
        self.quality_evaluator = QualityEvaluator()
        
        # è¿æ¥Neo4jæ•°æ®åº“
        if HAS_NEO4J_DRIVER and not self.use_mock_db:
            try:
                neo4j_uri = os.environ.get("NEO4J_URI", "bolt://localhost:7687")
                neo4j_user = os.environ.get("NEO4J_USER", "neo4j")
                neo4j_password = os.environ.get("NEO4J_PASSWORD", "12345678")

                print(f"ğŸ”„ KnowledgeGraphBuilderæ­£åœ¨è¿æ¥Neo4j...")
                print(f"   URI: {neo4j_uri}")
                print(f"   ç”¨æˆ·: {neo4j_user}")

                self.neo4j = Neo4j(
                    uri=neo4j_uri,
                    user=neo4j_user,
                    password=neo4j_password
                )

                # æµ‹è¯•è¿æ¥
                test_result = self.neo4j.sync_query("RETURN 1 as test")
                if test_result:
                    print("âœ… KnowledgeGraphBuilder Neo4jè¿æ¥æˆåŠŸ")
                else:
                    print("âŒ KnowledgeGraphBuilder Neo4jè¿æ¥æµ‹è¯•å¤±è´¥")
                    self.neo4j = None
                    self.use_mock_db = True

            except Exception as e:
                print(f"âŒ KnowledgeGraphBuilderè¿æ¥Neo4jæ•°æ®åº“å¤±è´¥: {str(e)}")
                self.neo4j = None
                self.use_mock_db = True
                if not loaded_from_file:
                    self.mock_db = self._create_mock_db()
        else:
            self.neo4j = None
            if not self.use_mock_db:
                print("âš ï¸ Neo4jé©±åŠ¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®åº“")
            
    def _create_mock_db(self):
        """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®åº“"""
        mock_db = {
            "entities": {},
            "relations": []
        }
        
        # ä½¿ç”¨é»˜è®¤æµ‹è¯•æ•°æ®å¡«å……æ¨¡æ‹Ÿæ•°æ®åº“
        if self.use_test_data:
            # æ·»åŠ å®ä½“æ•°æ®
            for entity in DEFAULT_COURSE_DATA + DEFAULT_KNOWLEDGE_DATA:
                entity_name = entity['name']
                mock_db["entities"][entity_name] = entity
                
            # æ·»åŠ å…³ç³»æ•°æ®
            for relation in DEFAULT_RELATIONS:
                mock_db["relations"].append(relation)
                
        return mock_db
        
    def _create_entity(self, name, label, properties=None, source="manual", confidence=0.9):
        """åˆ›å»ºå®ä½“"""
        if properties is None:
            properties = {}
            
        entity = {
            "type": "entity",
            "name": name,
            "label": label,
            "source": source,
            "confidence": confidence,
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "properties": properties
        }
        
        return entity
        
    def _create_relation(self, source, target, relation_type, properties=None, source_type="manual", confidence=0.9):
        """åˆ›å»ºå…³ç³»"""
        if properties is None:
            properties = {}
            
        relation = {
            "type": "relation",
            "source": source,
            "target": target,
            "relation_type": relation_type,
            "source_type": source_type,
            "confidence": confidence,
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "properties": properties
        }
        
        return relation

    async def build_knowledge_graph(self, is_incremental=True):
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        print("å¼€å§‹è‡ªåŠ¨ç”Ÿæˆè®¡ç®—æœºä¸“ä¸šè¯¾ç¨‹çŸ¥è¯†å›¾è°±...")
        
        try:
            # 1. æ”¶é›†è¯¾ç¨‹æ•°æ®
            print("å¯åŠ¨çŸ¥è¯†å›¾è°±æ„å»ºæœåŠ¡...")
            course_data = await self._collect_course_data()
            
            # 2. æ”¶é›†çŸ¥è¯†ç‚¹æ•°æ®
            knowledge_data = await self._collect_knowledge_data()
            
            # 3. æ•°æ®åˆå¹¶
            merged_data = course_data + knowledge_data
            print(f"æ•°æ®åˆå¹¶å®Œæˆï¼Œå…±{len(merged_data)}æ¡è®°å½•")

            # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œè¿”å›å¤±è´¥
            if len(merged_data) == 0:
                print("é”™è¯¯ï¼šæœªèƒ½é‡‡é›†åˆ°æœ‰æ•ˆæ•°æ®ï¼ŒçŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥")
                return False

            # 4. æ•°æ®å¤„ç†
            processed_data = await self._process_data(course_data, knowledge_data)
            print("æ•°æ®å¤„ç†å®Œæˆï¼Œå…±{}æ¡è®°å½•".format(len(processed_data)))
            
            # å¦‚æœå¤„ç†åæ•°æ®ä¸ºç©ºï¼Œè¿”å›å¤±è´¥
            if len(processed_data) == 0:
                print("é”™è¯¯ï¼šæ•°æ®å¤„ç†åç»“æœä¸ºç©ºï¼ŒçŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥")
                return False

            # 5. æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆæ ¹æ®å‚æ•°å†³å®šæ˜¯å¦å¢é‡æ›´æ–°ï¼‰
            if not self.use_mock_db and self.neo4j:
                await self._build_graph_in_database(processed_data, is_incremental=is_incremental)
            else:
                self._apply_updates_to_mock_db(processed_data, is_incremental=is_incremental)
                
            print("çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            print(f"æ„å»ºçŸ¥è¯†å›¾è°±å¤±è´¥: {str(e)}")
            import traceback
            print(traceback.format_exc())
            return False
            
    async def _collect_course_data(self):
        """æ”¶é›†è¯¾ç¨‹æ•°æ®"""
        print("æ•°æ®é‡‡é›†å¼€å§‹...")
        
        # ç¦ç”¨æµ‹è¯•æ•°æ®æ¨¡å¼
        self.use_test_data = False
            
        course_data = []
        try:
            # ä»æ•°æ®æºé‡‡é›†æ•°æ®
            print("å¼€å§‹ä»MOOCç½‘ç«™é‡‡é›†è¯¾ç¨‹æ•°æ®...")

            # åˆå§‹åŒ–spiderä¼šè¯
            if self.collector.spider.session is None:
                await self.collector.spider.init_session()

            collector_data = await self.collector.collect_course_data()
            if collector_data:
                print(f"æˆåŠŸä»æ•°æ®æºé‡‡é›†åˆ° {len(collector_data)} æ¡è¯¾ç¨‹æ•°æ®")
                course_data.extend(collector_data)
            else:
                print("æœªèƒ½ä»æ•°æ®æºé‡‡é›†åˆ°å®é™…è¯¾ç¨‹æ•°æ®")
                # ä½¿ç”¨é‡‡é›†åˆ°çš„çŸ¥è¯†ç‚¹æ¥åˆ›å»ºè¯¾ç¨‹æ•°æ®
                print("åŸºäºçŸ¥è¯†ç‚¹è‡ªåŠ¨ç”Ÿæˆè¯¾ç¨‹æ•°æ®...")
                knowledge_data = await self._collect_knowledge_data()

                # ä»çŸ¥è¯†ç‚¹ç”Ÿæˆè¯¾ç¨‹
                if knowledge_data:
                    generated_courses = []
                    for knowledge in knowledge_data:
                        if knowledge.get('type') == 'entity' and knowledge.get('label') == 'KnowledgePoint':
                            name = knowledge.get('name')
                            if name:
                                # è§£ç URLç¼–ç çš„çŸ¥è¯†ç‚¹åç§°
                                try:
                                    import urllib.parse
                                    name = urllib.parse.unquote(name)
                                except:
                                    pass

                                # æ ¹æ®çŸ¥è¯†ç‚¹ç”Ÿæˆè¯¾ç¨‹
                                generated_courses.append({
                                    'type': 'entity',
                                    'name': f"{name}åŸºç¡€ä¸åº”ç”¨",
                                    'label': 'Course',
                                    'source': 'generated',
                                    'confidence': 0.7,
                                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                                    'properties': {
                                        'description': f"æœ¬è¯¾ç¨‹ä»‹ç»{name}çš„åŸºæœ¬æ¦‚å¿µã€åŸç†ä¸åº”ç”¨",
                                        'instructor': 'ç³»ç»Ÿç”Ÿæˆ',
                                        'url': f"https://example.com/{urllib.parse.quote(name)}",
                                        'topics': [name]
                                    }
                                })

                                # æ·»åŠ å…³ç³»
                                generated_courses.append({
                                    'type': 'relation',
                                    'source': f"{name}åŸºç¡€ä¸åº”ç”¨",
                                    'target': name,
                                    'relation_type': 'CONTAINS',
                                    'source_type': 'generated',
                                    'confidence': 0.7,
                                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                                })

                    print(f"åŸºäºçŸ¥è¯†ç‚¹è‡ªåŠ¨ç”Ÿæˆäº† {len(generated_courses)} æ¡æ•°æ®")
                    course_data.extend(generated_courses)
        except Exception as e:
            print(f"ä»æ•°æ®æºé‡‡é›†è¯¾ç¨‹æ•°æ®å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            
        print(f"è¯¾ç¨‹æ•°æ®é‡‡é›†å®Œæˆï¼Œå…±{len(course_data)}æ¡è®°å½•")
        return course_data
        
    async def _collect_knowledge_data(self):
        """æ”¶é›†çŸ¥è¯†ç‚¹æ•°æ®"""
        # ç¦ç”¨æµ‹è¯•æ•°æ®æ¨¡å¼
        self.use_test_data = False
            
        knowledge_data = []
        new_data = []  # ç”¨äºè·Ÿè¸ªæ–°å¢æ•°æ®
        current_time = time.time()

        try:
            # ä»æ•°æ®æºé‡‡é›†æ•°æ®
            print("å¼€å§‹ä»ç½‘ç»œé‡‡é›†çŸ¥è¯†ç‚¹æ•°æ®...")

            # ç¡®ä¿spiderä¼šè¯å·²åˆå§‹åŒ–
            if self.collector.spider.session is None:
                await self.collector.spider.init_session()

            # è·å–å·²å­˜åœ¨çš„çŸ¥è¯†ç‚¹ï¼Œç”¨äºå¢é‡æ›´æ–°
            existing_knowledge = set(self.collected_knowledge_points)  # åˆ©ç”¨åˆå§‹åŒ–æ—¶åŠ è½½çš„å·²çŸ¥çŸ¥è¯†ç‚¹
            
            # å¦‚æœä½¿ç”¨å®é™…æ•°æ®åº“ï¼Œè¿˜éœ€è¦ä»æ•°æ®åº“æŸ¥è¯¢
            if not self.use_mock_db and self.neo4j:
                try:
                    # æŸ¥è¯¢å·²æœ‰çŸ¥è¯†ç‚¹
                    query = "MATCH (k:KnowledgePoint) RETURN k.name as name"
                    results = await self.neo4j.query(query)
                    for record in results:
                        existing_knowledge.add(record['name'])
                    print(f"æ•°æ®åº“ä¸­å·²å­˜åœ¨ {len(existing_knowledge)} ä¸ªçŸ¥è¯†ç‚¹")
                except Exception as e:
                    print(f"æŸ¥è¯¢å·²æœ‰çŸ¥è¯†ç‚¹å¤±è´¥: {str(e)}")
                    
            print(f"å½“å‰å·²çŸ¥çŸ¥è¯†ç‚¹é›†åˆå¤§å°: {len(existing_knowledge)}")

            collector_data = await self.collector.collect_knowledge_data()
            if collector_data:
                for item in collector_data:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºå®ä½“ä¸”æ˜¯çŸ¥è¯†ç‚¹
                    if (item.get('type') == 'entity' and
                            item.get('label') == 'KnowledgePoint'):

                        name = item.get('name')

                        # æ£€æŸ¥æ˜¯å¦ä¸ºå¢é‡æ›´æ–°
                        if name in existing_knowledge or name in self.collected_knowledge_points:
                            # è¿™æ˜¯å·²å­˜åœ¨çš„çŸ¥è¯†ç‚¹ï¼Œæ›´æ–°æ—¶é—´æˆ³
                            item['timestamp'] = time.strftime('%Y-%m-%d %H:%M:%S')
                        else:
                            # è¿™æ˜¯æ–°çŸ¥è¯†ç‚¹
                            self.collected_knowledge_points.add(name)
                            new_data.append(item)

                print(f"æˆåŠŸä»æ•°æ®æºé‡‡é›†åˆ° {len(collector_data)} æ¡çŸ¥è¯†ç‚¹æ•°æ®ï¼Œå…¶ä¸­æ–°å¢ {len(new_data)} æ¡")
                knowledge_data.extend(collector_data)
            else:
                # å¦‚æœæ²¡æœ‰é‡‡é›†åˆ°çŸ¥è¯†ç‚¹æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤çŸ¥è¯†ç‚¹
                print("æœªèƒ½ä»æ•°æ®æºé‡‡é›†åˆ°å®é™…çŸ¥è¯†ç‚¹æ•°æ®ï¼Œä½¿ç”¨é¢„è®¾çŸ¥è¯†ç‚¹...")
                default_knowledge = [
                    "ç®—æ³•", "æ•°æ®ç»“æ„", "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ",
                    "è®¡ç®—æœºç§‘å­¦", "ç½‘ç»œåè®®", "æ“ä½œç³»ç»Ÿ", "æ•°æ®åº“", "ç¼–ç¨‹è¯­è¨€"
                ]

                for topic in default_knowledge:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºå¢é‡æ›´æ–°
                    if topic not in existing_knowledge and topic not in self.collected_knowledge_points:
                        knowledge_data.append({
                            'type': 'entity',
                            'name': topic,
                            'label': 'KnowledgePoint',
                            'source': 'default_knowledge',
                            'confidence': 0.8,
                            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                            'properties': {
                                'description': f"{topic}æ˜¯è®¡ç®—æœºç§‘å­¦çš„é‡è¦åˆ†æ”¯",
                                'category': 'è®¡ç®—æœºç§‘å­¦',
                                'url': f"https://example.com/{urllib.parse.quote(topic)}",
                                'related_topics': default_knowledge[:3]  # ç®€å•å…³è”å‰3ä¸ªä¸»é¢˜
                            }
                        })
                        self.collected_knowledge_points.add(topic)
                        new_data.append(topic)

                # æ·»åŠ ä¸€äº›å…³ç³»
                for i in range(len(default_knowledge) - 1):
                    knowledge_data.append({
                        'type': 'relation',
                        'source': default_knowledge[i],
                        'target': default_knowledge[i + 1],
                        'relation_type': 'RELATED_TO',
                        'source_type': 'default_knowledge',
                        'confidence': 0.8,
                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                    })

                print(f"ä½¿ç”¨äº† {len(knowledge_data)} ä¸ªé¢„è®¾çŸ¥è¯†ç‚¹ï¼Œå…¶ä¸­æ–°å¢ {len(new_data)} ä¸ª")

            # æ›´æ–°ä¸Šæ¬¡æ›´æ–°æ—¶é—´
            self.last_update_time = current_time

        except Exception as e:
            print(f"ä»æ•°æ®æºé‡‡é›†çŸ¥è¯†ç‚¹æ•°æ®å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            
        # è¾“å‡ºå¢é‡æ›´æ–°ä¿¡æ¯
        if new_data:
            print(f"æœ¬æ¬¡å¢é‡æ›´æ–°æ–°å¢ {len(new_data)} ä¸ªçŸ¥è¯†ç‚¹")
        else:
            print("æœ¬æ¬¡æœªæ–°å¢çŸ¥è¯†ç‚¹")
            
        print(f"çŸ¥è¯†ç‚¹æ•°æ®é‡‡é›†å®Œæˆï¼Œå…±{len(knowledge_data)}æ¡è®°å½•")
        return knowledge_data
        
    async def _process_data(self, course_data, knowledge_data):
        """å¤„ç†æ•°æ®"""
        print("\nå¼€å§‹å¤„ç†é‡‡é›†åˆ°çš„æ•°æ®...")

        # åˆå¹¶æ•°æ®
        merged_data = course_data + knowledge_data
        
        if not merged_data:
            print("è­¦å‘Šï¼šè¾“å…¥æ•°æ®é›†ä¸ºç©º")
            print("ä½¿ç”¨é»˜è®¤æµ‹è¯•æ•°æ®ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")
            # è¿”å›é»˜è®¤æ•°æ®
            return DEFAULT_COURSE_DATA + DEFAULT_KNOWLEDGE_DATA + DEFAULT_RELATIONS
        
        try:
            # ä½¿ç”¨é»˜è®¤æ•°æ®å¤„ç†
            if self.use_test_data and self.use_mock_db:
                print("ä½¿ç”¨é¢„è®¾æµ‹è¯•æ•°æ®")
                return DEFAULT_COURSE_DATA + DEFAULT_KNOWLEDGE_DATA + DEFAULT_RELATIONS
            
            print(f"å¤„ç† {len(merged_data)} æ¡æ•°æ®è®°å½•")

            # å®ä½“å¯¹é½
            print("æ‰§è¡Œå®ä½“å¯¹é½...")
            aligned_data = self.entity_aligner.align_entities(merged_data)
            print(f"å®ä½“å¯¹é½å®Œæˆï¼Œç»“æœåŒ…å« {len(aligned_data)} æ¡è®°å½•")
            
            # å…³ç³»è§£æ
            print("æ‰§è¡Œå…³ç³»è§£æ...")
            with_relations = self.relation_resolver.resolve_relations(aligned_data)
            relation_count = len([item for item in with_relations if item.get('type') == 'relation'])
            print(f"å…³ç³»è§£æå®Œæˆï¼Œå…±è®¡ {relation_count} æ¡å…³ç³»")
            
            # çŸ¥è¯†æ¨ç†
            print("æ‰§è¡ŒçŸ¥è¯†æ¨ç†...")
            enriched_data = self.knowledge_reasoner.enrich_knowledge(with_relations)
            
            # è´¨é‡è¯„ä¼°
            print("æ‰§è¡Œè´¨é‡è¯„ä¼°...")
            final_data = self.quality_evaluator.evaluate_quality(enriched_data)
            print(f"æ•°æ®å¤„ç†å®Œæˆï¼Œæœ€ç»ˆç»“æœåŒ…å« {len(final_data)} æ¡è®°å½•")

            # å¦‚æœå¤„ç†åç»“æœä¸ºç©ºï¼Œåˆ™ä½¿ç”¨é»˜è®¤æ•°æ®
            if not final_data:
                print("è­¦å‘Šï¼šå¤„ç†åç»“æœä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æ•°æ®")
                return DEFAULT_COURSE_DATA + DEFAULT_KNOWLEDGE_DATA + DEFAULT_RELATIONS
            
            return final_data
        except Exception as e:
            print(f"âŒ æ•°æ®å¤„ç†å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            print("ä½¿ç”¨é»˜è®¤æ•°æ®ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")
            # å‘ç”Ÿå¼‚å¸¸æ—¶è¿”å›é»˜è®¤æ•°æ®
            return DEFAULT_COURSE_DATA + DEFAULT_KNOWLEDGE_DATA + DEFAULT_RELATIONS

    def _apply_updates_to_mock_db(self, data, is_incremental=True):
        """å°†æ›´æ–°åº”ç”¨åˆ°æ¨¡æ‹Ÿæ•°æ®åº“"""
        # ç¡®ä¿mock_dbå·²åˆå§‹åŒ–
        if not self.mock_db:
            self.mock_db = self._create_mock_db()
            
        # è®°å½•æ›´æ–°ä¿¡æ¯
        entities_added = 0
        entities_updated = 0
        relations_added = 0

        # å¤„ç†å®ä½“æ•°æ®
        for item in data:
            if item.get('type') == 'entity':
                entity_name = item.get('name')

                # å¢é‡æ›´æ–°æ¨¡å¼ä¸‹ï¼Œæ–°å¢æˆ–æ›´æ–°å®ä½“
                if is_incremental and entity_name in self.mock_db["entities"]:
                    # æ›´æ–°å·²æœ‰å®ä½“
                    old_entity = self.mock_db["entities"][entity_name]
                    old_properties = old_entity.get('properties', {})
                    new_properties = item.get('properties', {})

                    # åˆå¹¶å±æ€§ï¼Œä¿ç•™æ—§å€¼
                    merged_properties = {**old_properties, **new_properties}
                    item['properties'] = merged_properties

                    # æ›´æ–°å®ä½“
                    self.mock_db["entities"][entity_name] = item
                    entities_updated += 1
                else:
                    # æ–°å¢å®ä½“
                    self.mock_db["entities"][entity_name] = item
                    entities_added += 1

            elif item.get('type') == 'relation':
                # æ£€æŸ¥æ˜¯å¦ä¸ºå¢é‡æ›´æ–°æ¨¡å¼ä¸‹çš„æ–°å…³ç³»
                is_new_relation = True

                if is_incremental:
                    # æ£€æŸ¥å…³ç³»æ˜¯å¦å·²å­˜åœ¨
                    source = item.get('source')
                    target = item.get('target')
                    relation_type = item.get('relation_type')

                    for existing_relation in self.mock_db["relations"]:
                        if (existing_relation.get('source') == source and
                                existing_relation.get('target') == target and
                                existing_relation.get('relation_type') == relation_type):
                            is_new_relation = False
                            break

                if is_new_relation:
                    self.mock_db["relations"].append(item)
                    relations_added += 1

        print(f"æ¨¡æ‹Ÿæ•°æ®åº“æ›´æ–°ï¼šæ–°å¢ {entities_added} ä¸ªå®ä½“ï¼Œæ›´æ–° {entities_updated} ä¸ªå®ä½“ï¼Œæ–°å¢ {relations_added} ä¸ªå…³ç³»")
        print(f"æ¨¡æ‹Ÿæ•°æ®åº“ç°æœ‰ {len(self.mock_db['entities'])} ä¸ªå®ä½“å’Œ {len(self.mock_db['relations'])} ä¸ªå…³ç³»")

    async def _build_graph_in_database(self, data, is_incremental=True):
        """å°†çŸ¥è¯†å›¾è°±æ•°æ®å†™å…¥åˆ°Neo4jæ•°æ®åº“ä¸­"""
        print("å¼€å§‹å°†æ•°æ®å†™å…¥åˆ°Neo4jæ•°æ®åº“...")
        start_time = time.time()

        try:
            # ç¡®ä¿Neo4jè¿æ¥å¯ç”¨
            if not self.neo4j:
                print("é”™è¯¯: Neo4jè¿æ¥ä¸å¯ç”¨")
                return False

            # è½¬æ¢ä¸ºå®ä½“å’Œå…³ç³»
            entities = []
            relations = []

            # è®°å½•å·²å­˜åœ¨å®ä½“çš„ID
            entity_ids = {}

            # ç»Ÿè®¡æ•°æ®
            entities_created = 0
            entities_updated = 0
            relations_created = 0
            relations_updated = 0

            # å¦‚æœæ˜¯å¢é‡æ›´æ–°ï¼Œé¦–å…ˆè·å–å·²æœ‰çš„å®ä½“å’Œå…³ç³»
            if is_incremental:
                print("æ‰§è¡Œå¢é‡æ›´æ–°ï¼Œè·å–å·²æœ‰å®ä½“...")

                # è·å–æ‰€æœ‰å·²æœ‰çš„çŸ¥è¯†ç‚¹å’Œè¯¾ç¨‹å®ä½“
                query = """
                MATCH (n)
                WHERE n:Course OR n:KnowledgePoint
                RETURN n.name as name, id(n) as id, labels(n) as labels
                """

                result = self.neo4j.sync_query(query)
                if result:
                    for record in result:
                        if 'name' in record and 'id' in record:
                            entity_ids[record['name'].lower()] = record['id']

                print(f"æ•°æ®åº“ä¸­å·²æœ‰ {len(entity_ids)} ä¸ªå®ä½“")

            # æ”¶é›†æ‰€æœ‰å®ä½“
            for item in data:
                if item['type'] == 'entity':
                    entity = {
                        'id': str(uuid.uuid4()),
                        'name': item['name'],
                        'label': item['label'],
                        'properties': item.get('properties', {}),
                        'source': item.get('source', 'unknown'),
                        'confidence': item.get('confidence', 0.5),
                        'timestamp': item.get('timestamp', time.strftime('%Y-%m-%d %H:%M:%S'))
                    }

                    # æ£€æŸ¥è¯¥å®ä½“æ˜¯å¦å·²å­˜åœ¨
                    entity_name_lower = entity['name'].lower()
                    if is_incremental and entity_name_lower in entity_ids:
                        # å¦‚æœå®ä½“å·²å­˜åœ¨ï¼Œä½¿ç”¨å·²æœ‰çš„ID
                        entity['id'] = entity_ids[entity_name_lower]
                        entities_updated += 1
                    else:
                        # æ–°å®ä½“
                        entity_ids[entity_name_lower] = entity['id']
                        entities_created += 1

                    entities.append(entity)

            # æ”¶é›†æ‰€æœ‰å…³ç³»
            for item in data:
                if item['type'] == 'relation':
                    # ç›´æ¥ä½¿ç”¨å®ä½“åç§°åˆ›å»ºå…³ç³»
                    relation = {
                        'id': str(uuid.uuid4()),
                        'source_name': item['source'],
                        'target_name': item['target'],
                        'type': item['relation_type'],
                        'properties': item.get('properties', {}),
                        'source_type': item.get('source_type', 'unknown'),
                        'confidence': item.get('confidence', 0.5),
                        'timestamp': item.get('timestamp', time.strftime('%Y-%m-%d %H:%M:%S'))
                    }
                    relations.append(relation)
                    relations_created += 1

            # å¦‚æœä¸æ˜¯å¢é‡æ›´æ–°ï¼Œåˆ™å…ˆæ¸…ç©ºæ•°æ®åº“
            if not is_incremental:
                print("æ‰§è¡Œå…¨é‡æ›´æ–°ï¼Œæ¸…ç©ºç°æœ‰æ•°æ®...")
                clear_query = """
                MATCH (n)
                DETACH DELETE n
                """
                self.neo4j.sync_query(clear_query)

            # åˆ›å»ºå®ä½“
            for entity in entities:
                # æ„å»ºå±æ€§æ˜ å°„
                properties = {
                    'id': entity['id'],
                    'name': entity['name'],
                    'source': entity['source'],
                    'confidence': entity['confidence'],
                    'timestamp': entity['timestamp']
                }

                # æ·»åŠ å…¶ä»–å±æ€§
                if entity['properties']:
                    for key, value in entity['properties'].items():
                        properties[key] = value

                # è½¬æ¢å±æ€§ä¸ºCypherå‚æ•°
                params = {'props': properties}

                # å¦‚æœæ˜¯å¢é‡æ›´æ–°ï¼Œä½¿ç”¨MERGEè€Œä¸æ˜¯CREATE
                if is_incremental:
                    # ä½¿ç”¨MERGEè¿›è¡Œæ›´æ–°æˆ–åˆ›å»ºï¼ˆåŸºäºnameå±æ€§ï¼‰
                    query = f"""
                    MERGE (n:{entity['label']} {{name: $props.name}})
                    ON CREATE SET n = $props
                    ON MATCH SET n += $props
                    """
                else:
                    # ä½¿ç”¨CREATEåˆ›å»ºæ–°èŠ‚ç‚¹
                    query = f"""
                    CREATE (n:{entity['label']})
                    SET n = $props
                    """

                self.neo4j.sync_query(query, params)

            # åˆ›å»ºå…³ç³»
            for relation in relations:
                # æ„å»ºå±æ€§æ˜ å°„
                properties = {
                    'id': relation['id'],
                    'source_type': relation['source_type'],
                    'confidence': relation['confidence'],
                    'timestamp': relation['timestamp']
                }

                # æ·»åŠ å…¶ä»–å±æ€§
                if relation['properties']:
                    for key, value in relation['properties'].items():
                        properties[key] = value

                # è½¬æ¢å±æ€§ä¸ºCypherå‚æ•°
                params = {
                    'source_name': relation['source_name'],
                    'target_name': relation['target_name'],
                    'props': properties
                }

                # ä½¿ç”¨MERGEè¿›è¡Œå…³ç³»åˆ›å»ºæˆ–æ›´æ–°ï¼ˆåŸºäºnameå±æ€§ï¼‰
                query = f"""
                MATCH (source), (target)
                WHERE source.name = $source_name AND target.name = $target_name
                MERGE (source)-[r:{relation['type']}]->(target)
                ON CREATE SET r = $props
                ON MATCH SET r += $props
                """

                self.neo4j.sync_query(query, params)

            end_time = time.time()
            elapsed = end_time - start_time

            print(f"æ•°æ®åº“æ›´æ–°å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f} ç§’")
            print(f"å®ä½“: åˆ›å»º {entities_created} ä¸ªï¼Œæ›´æ–° {entities_updated} ä¸ª")
            print(f"å…³ç³»: åˆ›å»º {relations_created} ä¸ªï¼Œæ›´æ–° {relations_updated} ä¸ª")

            return True
        except Exception as e:
            print(f"æ•°æ®åº“æ›´æ–°å¤±è´¥: {str(e)}")
            return False

    def get_entities_by_label(self, label):
        """è·å–æŒ‡å®šæ ‡ç­¾çš„å®ä½“"""
        if self.use_mock_db:
            entities = []
            for name, entity in self.mock_db["entities"].items():
                if entity.get('label') == label:
                    entities.append(entity)
            return entities
        elif self.neo4j:
            # å®é™…æ•°æ®åº“æŸ¥è¯¢
            pass
        return []
        
    def get_relations_by_type(self, relation_type):
        """è·å–æŒ‡å®šç±»å‹çš„å…³ç³»"""
        if self.use_mock_db:
            relations = []
            for relation in self.mock_db["relations"]:
                if relation.get('relation_type') == relation_type:
                    relations.append(relation)
            return relations
        elif self.neo4j:
            # å®é™…æ•°æ®åº“æŸ¥è¯¢
            pass
        return []


class KnowledgeGraphQuery:
    """çŸ¥è¯†å›¾è°±æŸ¥è¯¢æ¥å£"""
    
    def __init__(self):
        # åˆå§‹åŒ–Neo4jè¿æ¥
        if not HAS_NEO4J_DRIVER:
            print("è­¦å‘Šï¼šNeo4jé©±åŠ¨æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢")
            self.driver = None
            self.use_mock_db = True
            # åˆå§‹åŒ–æ¨¡æ‹Ÿæ•°æ®åº“
            self.mock_db = {
                'entities': {},
                'relations': []
            }
            return
            
        try:
            self.driver = GraphDatabase.driver(
                os.getenv("NEO4J_URI"),
                auth=(os.getenv("NEO4J_USER"), os.getenv("NEO4J_PASSWORD"))
            )
            self.use_mock_db = False
        except Exception as e:
            print(f"è¿æ¥Neo4jæ•°æ®åº“å¤±è´¥: {str(e)}")
            self.driver = None
            self.use_mock_db = True
            # åˆå§‹åŒ–æ¨¡æ‹Ÿæ•°æ®åº“
            self.mock_db = {
                'entities': {},
                'relations': []
            }
    
    async def query_course_knowledge(self, course_name):
        """æŸ¥è¯¢è¯¾ç¨‹åŒ…å«çš„çŸ¥è¯†ç‚¹"""
        # å¦‚æœä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®åº“
        if self.use_mock_db:
            return self._query_course_knowledge_mock(course_name)
            
        try:
            async with self.driver.session() as session:
                query = """
                MATCH (c:Course {name: $course_name})-[:CONTAINS]->(k:KnowledgePoint)
                RETURN k.name as knowledge_point, k.description as description
                """
                result = await session.run(query, course_name=course_name)
                return [dict(record) for record in await result.data()]
        except Exception as e:
            print(f"æŸ¥è¯¢è¯¾ç¨‹çŸ¥è¯†ç‚¹é”™è¯¯: {str(e)}")
            return []
            
    def _query_course_knowledge_mock(self, course_name):
        """æ¨¡æ‹ŸæŸ¥è¯¢è¯¾ç¨‹çŸ¥è¯†ç‚¹"""
        results = []
        
        # ä»æ¨¡æ‹Ÿæ•°æ®åº“ä¸­æŸ¥æ‰¾å…³ç³»
        for relation in self.mock_db['relations']:
            if relation['relation_type'] == 'CONTAINS' and relation['source'] == course_name:
                knowledge_point = relation['target']
                # è·å–çŸ¥è¯†ç‚¹è¯¦æƒ…
                if knowledge_point in self.mock_db['entities']:
                    entity = self.mock_db['entities'][knowledge_point]
                    results.append({
                        'knowledge_point': knowledge_point,
                        'description': entity.get('properties', {}).get('description', '')
                    })
                
        return results
    
    async def query_knowledge_related(self, knowledge_point):
        """æŸ¥è¯¢çŸ¥è¯†ç‚¹çš„ç›¸å…³çŸ¥è¯†ç‚¹"""
        # å¦‚æœä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®åº“
        if self.use_mock_db:
            return self._query_knowledge_related_mock(knowledge_point)
            
        try:
            async with self.driver.session() as session:
                query = """
                MATCH (k1:KnowledgePoint {name: $knowledge_point})-[:RELATED_TO]->(k2:KnowledgePoint)
                RETURN k2.name as related_point, k2.description as description
                """
                result = await session.run(query, knowledge_point=knowledge_point)
                return [dict(record) for record in await result.data()]
        except Exception as e:
            print(f"æŸ¥è¯¢ç›¸å…³çŸ¥è¯†ç‚¹é”™è¯¯: {str(e)}")
            return []
            
    def _query_knowledge_related_mock(self, knowledge_point):
        """æ¨¡æ‹ŸæŸ¥è¯¢ç›¸å…³çŸ¥è¯†ç‚¹"""
        results = []
        
        # ä»æ¨¡æ‹Ÿæ•°æ®åº“ä¸­æŸ¥æ‰¾å…³ç³»
        for relation in self.mock_db['relations']:
            if relation['relation_type'] == 'RELATED_TO' and relation['source'] == knowledge_point:
                related_point = relation['target']
                # è·å–çŸ¥è¯†ç‚¹è¯¦æƒ…
                if related_point in self.mock_db['entities']:
                    entity = self.mock_db['entities'][related_point]
                    results.append({
                        'related_point': related_point,
                        'description': entity.get('properties', {}).get('description', '')
                    })
                
        return results
    
    async def query_course_path(self, start_course, end_course):
        """æŸ¥è¯¢è¯¾ç¨‹ä¹‹é—´çš„å­¦ä¹ è·¯å¾„"""
        # å¦‚æœä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®åº“
        if self.use_mock_db:
            return self._query_course_path_mock(start_course, end_course)
            
        try:
            async with self.driver.session() as session:
                query = """
                MATCH path = (c1:Course {name: $start_course})-[:CONTAINS]->(k1:KnowledgePoint)-[:RELATED_TO*1..3]->(k2:KnowledgePoint)<-[:CONTAINS]-(c2:Course {name: $end_course})
                RETURN path
                """
                result = await session.run(query, start_course=start_course, end_course=end_course)
                return [dict(record) for record in await result.data()]
        except Exception as e:
            print(f"æŸ¥è¯¢å­¦ä¹ è·¯å¾„é”™è¯¯: {str(e)}")
            return []
            
    def _query_course_path_mock(self, start_course, end_course):
        """æ¨¡æ‹ŸæŸ¥è¯¢å­¦ä¹ è·¯å¾„"""
        print(f"æ¨¡æ‹ŸæŸ¥è¯¢ä»{start_course}åˆ°{end_course}çš„å­¦ä¹ è·¯å¾„")
        # è¿™é‡Œä»…è¿”å›ä¸€ä¸ªç¤ºä¾‹è·¯å¾„
        return [{
            'path': f"{start_course} -> åŸºç¡€çŸ¥è¯† -> è¿›é˜¶çŸ¥è¯† -> {end_course}"
        }]
    
    async def search_knowledge(self, keyword):
        """æœç´¢çŸ¥è¯†ç‚¹"""
        # å¦‚æœä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®åº“
        if self.use_mock_db:
            return self._search_knowledge_mock(keyword)
            
        try:
            async with self.driver.session() as session:
                query = """
                MATCH (k:KnowledgePoint)
                WHERE k.name CONTAINS $keyword OR k.description CONTAINS $keyword
                RETURN k.name as name, k.description as description
                """
                result = await session.run(query, keyword=keyword)
                return [dict(record) for record in await result.data()]
        except Exception as e:
            print(f"æœç´¢çŸ¥è¯†ç‚¹é”™è¯¯: {str(e)}")
            return []
            
    def _search_knowledge_mock(self, keyword):
        """æ¨¡æ‹Ÿæœç´¢çŸ¥è¯†ç‚¹"""
        results = []
        
        # ä»æ¨¡æ‹Ÿæ•°æ®åº“ä¸­æœç´¢å®ä½“
        for entity_id, entity in self.mock_db['entities'].items():
            if entity.get('label') == 'KnowledgePoint':
                name = entity.get('name', '')
                desc = entity.get('properties', {}).get('description', '')
                
                if keyword in name or keyword in desc:
                    results.append({
                        'name': name,
                        'description': desc
                    })
                
        return results
    
    async def get_course_list(self):
        """è·å–æ‰€æœ‰è¯¾ç¨‹åˆ—è¡¨"""
        # å¦‚æœä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®åº“
        if self.use_mock_db:
            return self._get_course_list_mock()
            
        try:
            async with self.driver.session() as session:
                query = """
                MATCH (c:Course)
                RETURN c.name as name, c.description as description
                """
                result = await session.run(query)
                return [dict(record) for record in await result.data()]
        except Exception as e:
            print(f"è·å–è¯¾ç¨‹åˆ—è¡¨é”™è¯¯: {str(e)}")
            return []
            
    def _get_course_list_mock(self):
        """æ¨¡æ‹Ÿè·å–è¯¾ç¨‹åˆ—è¡¨"""
        results = []
        
        # ä»æ¨¡æ‹Ÿæ•°æ®åº“ä¸­è·å–è¯¾ç¨‹
        for entity_id, entity in self.mock_db['entities'].items():
            if entity.get('label') == 'Course':
                results.append({
                    'name': entity.get('name', ''),
                    'description': entity.get('properties', {}).get('description', '')
                })
                
        return results


def create_app(query_interface):
    """åˆ›å»ºFlaskåº”ç”¨å¹¶é…ç½®è·¯ç”±

    Args:
        query_interface: çŸ¥è¯†å›¾è°±æŸ¥è¯¢æ¥å£å®ä¾‹

    Returns:
        Flaskåº”ç”¨å®ä¾‹
    """
    try:
        from flask import Flask, render_template, request, jsonify

        app = Flask(__name__)

        @app.route('/')
        def index():
            return render_template('index.html')

        @app.route('/query', methods=['POST'])
        def query():
            try:
                query_text = request.json.get('query', '')

                # æ‰§è¡ŒæŸ¥è¯¢
                if 'course' in query_text.lower() and 'knowledge' in query_text.lower():
                    # æŸ¥è¯¢è¯¾ç¨‹åŒ…å«çš„çŸ¥è¯†ç‚¹
                    course_name = query_text.split('course:')[1].strip() if 'course:' in query_text else None
                    if course_name:
                        import asyncio
                        results = asyncio.run(query_interface.query_course_knowledge(course_name))

                        nodes = []
                        relationships = []

                        # æ·»åŠ è¯¾ç¨‹èŠ‚ç‚¹
                        nodes.append({
                            'id': course_name,
                            'label': 'Course',
                            'name': course_name,
                            'properties': {}
                        })

                        # æ·»åŠ çŸ¥è¯†ç‚¹èŠ‚ç‚¹å’Œå…³ç³»
                        for result in results:
                            knowledge_point = result.get('knowledge_point')
                            description = result.get('description', '')

                            nodes.append({
                                'id': knowledge_point,
                                'label': 'KnowledgePoint',
                                'name': knowledge_point,
                                'properties': {'description': description}
                            })

                            relationships.append({
                                'source': course_name,
                                'target': knowledge_point,
                                'type': 'CONTAINS',
                                'properties': {}
                            })

                        return jsonify({
                            'success': True,
                            'nodes': nodes,
                            'relationships': relationships,
                            'cypher_query': f"MATCH (c:Course {{name: '{course_name}'}})-[:CONTAINS]->(k:KnowledgePoint) RETURN c, k"
                        })
                else:
                    # ç®€å•æŸ¥è¯¢ï¼Œè¿”å›æ‰€æœ‰å®ä½“å’Œå…³ç³»
                    nodes = []
                    relationships = []

                    # ä»æ¨¡æ‹Ÿæ•°æ®åº“è·å–å®ä½“
                    if query_interface.use_mock_db:
                        for entity_name, entity in query_interface.mock_db['entities'].items():
                            nodes.append({
                                'id': entity_name,
                                'label': entity.get('label', 'Entity'),
                                'name': entity_name,
                                'properties': entity.get('properties', {})
                            })

                        # è·å–å…³ç³»
                        for relation in query_interface.mock_db['relations']:
                            relationships.append({
                                'source': relation.get('source'),
                                'target': relation.get('target'),
                                'type': relation.get('relation_type'),
                                'properties': {}
                            })

                    return jsonify({
                        'success': True,
                        'nodes': nodes,
                        'relationships': relationships,
                        'cypher_query': "MATCH (n)-[r]->(m) RETURN n, r, m"
                    })

            except Exception as e:
                import traceback
                traceback.print_exc()
                return jsonify({
                    'success': False,
                    'error': str(e)
                })

        @app.route('/neo4j-browser', methods=['GET'])
        def neo4j_browser():
            """é‡å®šå‘åˆ°Neo4j Browser"""
            neo4j_browser_url = os.environ.get("NEO4J_BROWSER_URL", "http://localhost:7474/browser/")
            return f"""
            <script>
                window.location.href = "{neo4j_browser_url}";
            </script>
            """

        @app.route('/check_updates', methods=['POST'])
        def check_updates():
            """æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„æ›´æ–°"""
            try:
                data = request.get_json()
                last_update = data.get('last_update', 0)

                # æ£€æŸ¥æœ€æ–°æ›´æ–°æ—¶é—´
                if query_interface.use_mock_db:
                    latest_update = time.time()
                else:
                    # ä»æ•°æ®åº“æŸ¥è¯¢
                    pass

                return jsonify({
                    'has_updates': True,  # ç®€åŒ–å¤„ç†
                    'latest_update': latest_update
                })
            except Exception as e:
                return jsonify({'error': str(e)}), 500

        @app.route('/get_graph_data')
        def get_graph_data():
            """è·å–æœ€æ–°çš„å›¾è°±æ•°æ®"""
            try:
                nodes = []
                relationships = []

                # ä»æ¨¡æ‹Ÿæ•°æ®åº“è·å–å®ä½“
                if query_interface.use_mock_db:
                    for entity_name, entity in query_interface.mock_db['entities'].items():
                        nodes.append({
                            'id': entity_name,
                            'label': entity.get('label', 'Entity'),
                            'name': entity_name,
                            'properties': entity.get('properties', {})
                        })

                    # è·å–å…³ç³»
                    for relation in query_interface.mock_db['relations']:
                        relationships.append({
                            'source': relation.get('source'),
                            'target': relation.get('target'),
                            'type': relation.get('relation_type'),
                            'properties': {}
                        })

                return jsonify({
                    'nodes': nodes,
                    'relationships': relationships
                })
            except Exception as e:
                return jsonify({'error': str(e)}), 500

        return app
    except Exception as e:
        print(f"åˆ›å»ºFlaskåº”ç”¨é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return None
# Flaskåº”ç”¨åˆ›å»ºå‡½æ•°å·²ç§»é™¤ï¼Œç°åœ¨ä½¿ç”¨Djangoæ¡†æ¶
#def create_app_removed():
#    """Flaskåº”ç”¨åˆ›å»ºå‡½æ•°å·²ç§»é™¤ï¼Œç°åœ¨ä½¿ç”¨Djangoæ¡†æ¶"""
#    pass


async def main():
    """
    ä¸»å‡½æ•° - æ•´åˆäº†æ•°æ®é‡‡é›†ã€å›¾è°±æ„å»ºã€å…³ç³»ç”Ÿæˆå’Œå¯è§†åŒ–çš„å®Œæ•´æµç¨‹
    
    æœ¬å‡½æ•°æ•´åˆäº†ä»¥ä¸‹åŠŸèƒ½ï¼š
    1. ä»å„æ•°æ®æºé‡‡é›†çŸ¥è¯†ç‚¹å’Œè¯¾ç¨‹æ•°æ®
    2. æ„å»ºåŸºç¡€çŸ¥è¯†å›¾è°±
    3. ä¿å­˜å›¾è°±æ•°æ®ï¼ˆæ•´åˆsave_graph.pyåŠŸèƒ½ï¼‰
    4. ç”Ÿæˆé«˜è´¨é‡çŸ¥è¯†ç‚¹å…³ç³»ï¼ˆæ•´åˆgenerate_relations.pyåŠŸèƒ½ï¼‰
    5. æä¾›Webç•Œé¢æŸ¥çœ‹çŸ¥è¯†å›¾è°±ï¼ˆå¯é€‰ï¼‰
    
    é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶ä¸åŒåŠŸèƒ½ï¼š
    --reset               é‡ç½®æ•°æ®ï¼Œä»å¤´å¼€å§‹æ„å»º
    --continue-collection ç»§ç»­ä¹‹å‰çš„é‡‡é›†ä»»åŠ¡
    --only-save           ä»…ä¿å­˜å·²æœ‰æ•°æ®ï¼Œä¸è¿›è¡Œé‡‡é›†
    --skip-relations      è·³è¿‡å…³ç³»ç”Ÿæˆæ­¥éª¤
    --web                 è¿è¡ŒWebåº”ç”¨ç¨‹åºæŸ¥çœ‹å›¾è°±
    """
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        import argparse
        parser = argparse.ArgumentParser(description="è®¡ç®—æœºä¸“ä¸šçŸ¥è¯†å›¾è°±è‡ªåŠ¨åŒ–æ„å»ºå·¥å…·")
        parser.add_argument('--reset', action='store_true', help='é‡ç½®æ•°æ®ï¼Œä»å¤´å¼€å§‹æ„å»º')
        parser.add_argument('--continue-collection', action='store_true', help='ç»§ç»­ä¹‹å‰çš„é‡‡é›†ä»»åŠ¡')
        parser.add_argument('--only-save', action='store_true', help='ä»…ä¿å­˜å·²æœ‰æ•°æ®ï¼Œä¸è¿›è¡Œé‡‡é›†')
        parser.add_argument('--skip-relations', action='store_true', help='è·³è¿‡å…³ç³»ç”Ÿæˆæ­¥éª¤')
        parser.add_argument('--web', action='store_true', help='è¿è¡ŒWebåº”ç”¨ç¨‹åº')
        args = parser.parse_args()

        # æ ‡é¢˜
        print("-" * 70)
        print("                è®¡ç®—æœºä¸“ä¸šçŸ¥è¯†å›¾è°±è‡ªåŠ¨åŒ–æ„å»ºå·¥å…·")
        print("-" * 70)

        # ç¡®ä¿è·¯å¾„å­˜åœ¨
        os.makedirs("data", exist_ok=True)
        
        # æ›´æ–°è¿›åº¦ä¿¡æ¯
        def update_progress(status, **kwargs):
            progress_file = 'data/collection_progress.json'
            progress_info = {
                'last_updated': time.strftime('%Y-%m-%d %H:%M:%S'),
                'status': status
            }
            
            # è¯»å–å·²æœ‰è¿›åº¦ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if os.path.exists(progress_file):
                try:
                    with open(progress_file, 'r', encoding='utf-8') as f:
                        existing_progress = json.load(f)
                        # ä¿ç•™å†å²ä¿¡æ¯
                        for key, value in existing_progress.items():
                            if key != 'status' and key != 'last_updated' and key not in kwargs:
                                progress_info[key] = value
                except Exception as e:
                    print(f"è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}")
            
            # æ›´æ–°é¢å¤–çš„è¿›åº¦ä¿¡æ¯
            for key, value in kwargs.items():
                progress_info[key] = value
            
            # å†™å…¥è¿›åº¦æ–‡ä»¶
            try:
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress_info, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"å†™å…¥è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}")

        # ä»…ä¿å­˜æ¨¡å¼
        if args.only_save:
            print("ä»…ä¿å­˜æ¨¡å¼ï¼šä¿å­˜å½“å‰çŸ¥è¯†å›¾è°±æ•°æ®")
            update_progress('running', collection_type='only_save')
            
            # ç›´æ¥è°ƒç”¨save_graph.pyä¸­çš„åŠŸèƒ½
            from save_graph import save_mock_db_from_run, extract_data_from_collector, extract_from_json_file, extract_from_default_data
            
            success = save_mock_db_from_run()
            if not success:
                success = extract_data_from_collector()
            if not success:
                success = extract_from_json_file()
            if not success:
                success = extract_from_default_data()
                
            if success and not args.skip_relations:
                print("\nç”Ÿæˆå…³ç³»æ•°æ®...")
                # è°ƒç”¨generate_relations.pyä¸­çš„åŠŸèƒ½
                from generate_relations import load_knowledge_graph, generate_prerequisite_relations, generate_related_relations
                from generate_relations import generate_parent_child_relations, generate_relations_based_on_text, generate_course_knowledge_relations, save_graph_data
                
                graph_data = load_knowledge_graph()
                original_relations_count = len(graph_data.get('relations', []))
                
                # ç”Ÿæˆå„ç±»å…³ç³»
                prereq_relations = generate_prerequisite_relations(graph_data)
                graph_data['relations'].extend(prereq_relations)
                
                related_relations = generate_related_relations(graph_data)
                graph_data['relations'].extend(related_relations)
                
                parent_child_relations = generate_parent_child_relations(graph_data)
                graph_data['relations'].extend(parent_child_relations)
                
                text_based_relations = generate_relations_based_on_text(graph_data)
                graph_data['relations'].extend(text_based_relations)
                
                course_kp_relations = generate_course_knowledge_relations(graph_data)
                graph_data['relations'].extend(course_kp_relations)
                
                # ä¿å­˜æ›´æ–°åçš„å›¾è°±æ•°æ®
                save_graph_data(graph_data)
                
                new_relations_count = len(graph_data.get('relations', []))
                print(f"å…±æ·»åŠ äº† {new_relations_count - original_relations_count} ä¸ªæ–°å…³ç³»")
                print(f"æœ€ç»ˆå›¾è°±åŒ…å« {len(graph_data.get('entities', {}))} ä¸ªå®ä½“å’Œ {new_relations_count} ä¸ªå…³ç³»")
            
            # æ›´æ–°è¿›åº¦ä¸ºå®Œæˆ
            update_progress('completed')
            
            # å¦‚æœéœ€è¦è¿è¡ŒWebåº”ç”¨
            if args.web:
                run_web_app()
                
            return

        # åˆå§‹åŒ–å›¾è°±æ„å»ºå™¨
        # ä¼˜å…ˆå°è¯•ä½¿ç”¨Neo4jæ•°æ®åº“ï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ°æ¨¡æ‹Ÿæ•°æ®åº“
        use_mock_db = False  # é»˜è®¤å°è¯•ä½¿ç”¨Neo4j
        use_test_data = not args.reset  # å¦‚æœé‡ç½®åˆ™ä¸ä½¿ç”¨æµ‹è¯•æ•°æ®

        # æ£€æŸ¥Neo4jæ˜¯å¦å¯ç”¨
        neo4j_available = False
        neo4j_uri = os.environ.get("NEO4J_URI", "bolt://localhost:7687")
        neo4j_user = os.environ.get("NEO4J_USER", "neo4j")
        neo4j_password = os.environ.get("NEO4J_PASSWORD", "12345678")

        print(f"ğŸ” Neo4jè¿æ¥é…ç½®:")
        print(f"   URI: {neo4j_uri}")
        print(f"   ç”¨æˆ·: {neo4j_user}")
        print(f"   å¯†ç : {'*' * len(neo4j_password)}")

        if HAS_NEO4J_DRIVER:
            try:
                print("ğŸ”„ æ­£åœ¨æµ‹è¯•Neo4jè¿æ¥...")
                # å°è¯•è¿æ¥Neo4j
                test_neo4j = Neo4j(
                    uri=neo4j_uri,
                    user=neo4j_user,
                    password=neo4j_password
                )
                # æµ‹è¯•è¿æ¥
                test_result = test_neo4j.sync_query("RETURN 1 as test")
                if test_result:
                    neo4j_available = True
                    print("âœ… Neo4jæ•°æ®åº“è¿æ¥æˆåŠŸï¼Œå°†ä½¿ç”¨Neo4jå­˜å‚¨æ•°æ®")

                    # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ•°æ®
                    try:
                        nodes_result = test_neo4j.sync_query("MATCH (n) RETURN count(n) as node_count")
                        node_count = nodes_result[0]["node_count"] if nodes_result else 0
                        rels_result = test_neo4j.sync_query("MATCH ()-[r]->() RETURN count(r) as rel_count")
                        rel_count = rels_result[0]["rel_count"] if rels_result else 0
                        print(f"ğŸ“Š æ•°æ®åº“çŠ¶æ€: {node_count}ä¸ªèŠ‚ç‚¹, {rel_count}ä¸ªå…³ç³»")
                    except Exception as e:
                        print(f"âš ï¸ æ— æ³•è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯: {str(e)}")
                else:
                    print("âš ï¸ Neo4jè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®åº“")
                    use_mock_db = True
            except Exception as e:
                print(f"âŒ Neo4jè¿æ¥å¤±è´¥: {str(e)}")
                print("ğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
                print("   1. ç¡®ä¿Neo4jæœåŠ¡æ­£åœ¨è¿è¡Œ")
                print("   2. æ£€æŸ¥è¿æ¥é…ç½®æ˜¯å¦æ­£ç¡®")
                print("   3. éªŒè¯ç”¨æˆ·åå’Œå¯†ç ")
                print("å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®åº“ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")
                use_mock_db = True
        else:
            print("âš ï¸ Neo4jé©±åŠ¨æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®åº“")
            use_mock_db = True

        builder = KnowledgeGraphBuilder(use_mock_db=use_mock_db, use_test_data=use_test_data)

        # æ ¹æ®å‚æ•°æ‰§è¡Œç›¸åº”æ“ä½œ
        build_success = False
        if args.reset:
            print("é‡ç½®æ¨¡å¼ï¼šä»å¤´æ„å»ºçŸ¥è¯†å›¾è°±")
            update_progress('running', collection_type='reset')
            build_success = await builder.build_knowledge_graph(is_incremental=False)
        elif args.continue_collection:
            print("ç»§ç»­æ¨¡å¼ï¼šç»§ç»­ä¹‹å‰çš„çŸ¥è¯†æ”¶é›†ä»»åŠ¡")
            update_progress('running', collection_type='continue')
            build_success = await builder.build_knowledge_graph(is_incremental=True)
        else:
            print("æ ‡å‡†æ¨¡å¼ï¼šæ ¹æ®éœ€è¦å¢é‡æ„å»ºçŸ¥è¯†å›¾è°±")
            update_progress('running', collection_type='standard')
            build_success = await builder.build_knowledge_graph(is_incremental=True)
        
        # æ›´æ–°çŸ¥è¯†ç‚¹ç»Ÿè®¡ä¿¡æ¯
        if hasattr(builder, 'collected_knowledge_points'):
            update_progress(
                'running',
                processed_topics=list(builder.collected_knowledge_points),
                total_batches=20,  # å‡è®¾æ€»æ‰¹æ¬¡
                last_batch=len(builder.collected_knowledge_points) // 5  # ç²—ç•¥è®¡ç®—å·²å®Œæˆæ‰¹æ¬¡
            )
            
        # åªæœ‰æ„å»ºæˆåŠŸæ‰ç»§ç»­å¤„ç†
        if not build_success:
            print("çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥ï¼Œå°†ä¸è¿›è¡Œæ•°æ®ä¿å­˜å’Œå…³ç³»ç”Ÿæˆ")
            update_progress('error', error_message="çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥")
            return

        # ä¿å­˜å›¾è°±æ•°æ®åˆ°æ–‡ä»¶
        print("\nä¿å­˜å›¾è°±æ•°æ®åˆ°æ–‡ä»¶...")
        # ç›´æ¥è°ƒç”¨save_graph.pyä¸­çš„åŠŸèƒ½
        from save_graph import save_mock_db_from_run, extract_data_from_collector, extract_from_json_file, extract_from_default_data
        
        success = save_mock_db_from_run()
        if not success:
            success = extract_data_from_collector()
        if not success:
            success = extract_from_json_file()
        if not success:
            success = extract_from_default_data()
            
        # ç”Ÿæˆå…³ç³»ï¼Œé™¤éæ˜ç¡®è·³è¿‡
        if not args.skip_relations:
            print("\nç”Ÿæˆå…³ç³»æ•°æ®...")
            update_progress('running', stage='generating_relations')
            # è°ƒç”¨generate_relations.pyä¸­çš„åŠŸèƒ½
            from generate_relations import load_knowledge_graph, generate_prerequisite_relations, generate_related_relations
            from generate_relations import generate_parent_child_relations, generate_relations_based_on_text, generate_course_knowledge_relations, generate_course_course_relations, save_graph_data
            
            graph_data = load_knowledge_graph()
            original_relations_count = len(graph_data.get('relations', []))
            
            # ç”Ÿæˆå„ç±»å…³ç³»
            prereq_relations = generate_prerequisite_relations(graph_data)
            graph_data['relations'].extend(prereq_relations)
            
            related_relations = generate_related_relations(graph_data)
            graph_data['relations'].extend(related_relations)
            
            parent_child_relations = generate_parent_child_relations(graph_data)
            graph_data['relations'].extend(parent_child_relations)
            
            text_based_relations = generate_relations_based_on_text(graph_data)
            graph_data['relations'].extend(text_based_relations)
            
            course_kp_relations = generate_course_knowledge_relations(graph_data)
            graph_data['relations'].extend(course_kp_relations)
            
            # æ·»åŠ è¯¾ç¨‹é—´å…³ç³»
            course_relations = generate_course_course_relations(graph_data)
            graph_data['relations'].extend(course_relations)
            
            # ä¿å­˜æ›´æ–°åçš„å›¾è°±æ•°æ®
            save_graph_data(graph_data)
            
            new_relations_count = len(graph_data.get('relations', []))
            print(f"å…±æ·»åŠ äº† {new_relations_count - original_relations_count} ä¸ªæ–°å…³ç³»")
            print(f"æœ€ç»ˆå›¾è°±åŒ…å« {len(graph_data.get('entities', {}))} ä¸ªå®ä½“å’Œ {new_relations_count} ä¸ªå…³ç³»")

        # æ›´æ–°è¿›åº¦ä¸ºå®Œæˆ
        update_progress('completed')

        # è¿è¡ŒWebåº”ç”¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if args.web:
            run_web_app()
        else:
            print("\nå›¾è°±æ„å»ºå®Œæˆï¼")
            print("è¯·ä½¿ç”¨Djangoç®¡ç†å‘½ä»¤å¯åŠ¨Webåº”ç”¨:")
            print("python manage.py runserver")
            print("ç„¶åè®¿é—®: http://localhost:8000/knowledge-graph/")
    
    except Exception as e:
        print(f"ä¸»å‡½æ•°æ‰§è¡Œé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # æ›´æ–°è¿›åº¦ä¸ºé”™è¯¯
        try:
            progress_file = 'data/collection_progress.json'
            if os.path.exists('data'):
                with open(progress_file, 'r', encoding='utf-8') as f:
                    progress_info = json.load(f)
                
                progress_info['status'] = 'error'
                progress_info['error_message'] = str(e)
                progress_info['last_updated'] = time.strftime('%Y-%m-%d %H:%M:%S')
                
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress_info, f, ensure_ascii=False, indent=2)
        except:
            pass
            
        sys.exit(1)

def run_web_app():
    """è¿è¡ŒWebåº”ç”¨ï¼Œå±•ç¤ºæ„å»ºçš„çŸ¥è¯†å›¾è°±"""
    try:
        print("\nçŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")
        print("è¯·ä½¿ç”¨Djangoç®¡ç†å‘½ä»¤å¯åŠ¨Webåº”ç”¨:")
        print("python manage.py runserver")
        print("ç„¶åè®¿é—®: http://localhost:8000/knowledge-graph/")
    except Exception as e:
        print(f"æç¤ºä¿¡æ¯æ˜¾ç¤ºå¤±è´¥: {str(e)}")

if __name__ == "__main__":
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    if sys.version_info < (3, 7):
        print("é”™è¯¯: éœ€è¦Python 3.7æˆ–æ›´é«˜ç‰ˆæœ¬")
        sys.exit(1)

    # è¿è¡Œä¸»å‡½æ•°
    if sys.platform == 'win32':
        # Windowså¹³å°éœ€è¦ç‰¹æ®Šå¤„ç†å¼‚æ­¥
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
    asyncio.run(main())